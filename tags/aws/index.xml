<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Aws on Damon Cortesi</title><link>https://dacort.xyz/tags/aws/</link><description>Recent content in Aws on Damon Cortesi</description><generator>Hugo -- 0.153.2</generator><language>en-us</language><lastBuildDate>Fri, 04 Feb 2022 10:30:00 -0800</lastBuildDate><atom:link href="https://dacort.xyz/tags/aws/index.xml" rel="self" type="application/rss+xml"/><item><title>"Serverless" Analytics of Twitter Data with MSK Connect and Athena</title><link>https://dacort.xyz/posts/serverless-analytics-of-twitter-data/</link><pubDate>Fri, 04 Feb 2022 10:30:00 -0800</pubDate><guid>https://dacort.xyz/posts/serverless-analytics-of-twitter-data/</guid><description>&lt;p&gt;Like many, &lt;a href="https://twitter.com/dacort/status/1466940658271199233?s=20&amp;amp;t=cEoyLkoJSazoAHW9XRhjvw"&gt;I was recently&lt;/a&gt; drawn in to a simple word game by the name of &amp;ldquo;Wordle&amp;rdquo;. Also, like many I wanted to dive into the analytics of all the yellow, green, and white-or-black-depending-on-your-dark-mode blocks. While you can easily &lt;a href="https://www.tweetstats.com/"&gt;query tweet volume&lt;/a&gt; using the Twitter API, I wanted to dig deeper. And the tweets were growing&amp;hellip;&lt;/p&gt;
&lt;p&gt;&lt;img loading="lazy" src="https://dacort.xyz/posts/serverless-analytics-of-twitter-data/wordle_counts_2021-01-13.png"&gt;&lt;/p&gt;
&lt;p&gt;Given the recent announcement of &lt;a href="https://aws.amazon.com/blogs/aws/introducing-amazon-msk-connect-stream-data-to-and-from-your-apache-kafka-clusters-using-managed-connectors/"&gt;MSK Connect&lt;/a&gt;, I wanted to see if I could easily consume the Twitter Stream into S3 and query the data with Athena. So I looked around a bit and found this &lt;a href="https://github.com/jcustenborder/kafka-connect-twitter"&gt;kafka-connect-twitter&lt;/a&gt; GitHub repo and this &lt;a href="https://www.confluent.io/blog/using-ksql-to-analyse-query-and-transform-data-in-kafka/"&gt;great blog post from Confluent&lt;/a&gt; on analyzing Twitter data using that connector. So&amp;hellip;off to the AWS console!&lt;/p&gt;</description><content:encoded><![CDATA[<p>Like many, <a href="https://twitter.com/dacort/status/1466940658271199233?s=20&amp;t=cEoyLkoJSazoAHW9XRhjvw">I was recently</a> drawn in to a simple word game by the name of &ldquo;Wordle&rdquo;. Also, like many I wanted to dive into the analytics of all the yellow, green, and white-or-black-depending-on-your-dark-mode blocks. While you can easily <a href="https://www.tweetstats.com/">query tweet volume</a> using the Twitter API, I wanted to dig deeper. And the tweets were growing&hellip;</p>
<p><img loading="lazy" src="/posts/serverless-analytics-of-twitter-data/wordle_counts_2021-01-13.png"></p>
<p>Given the recent announcement of <a href="https://aws.amazon.com/blogs/aws/introducing-amazon-msk-connect-stream-data-to-and-from-your-apache-kafka-clusters-using-managed-connectors/">MSK Connect</a>, I wanted to see if I could easily consume the Twitter Stream into S3 and query the data with Athena. So I looked around a bit and found this <a href="https://github.com/jcustenborder/kafka-connect-twitter">kafka-connect-twitter</a> GitHub repo and this <a href="https://www.confluent.io/blog/using-ksql-to-analyse-query-and-transform-data-in-kafka/">great blog post from Confluent</a> on analyzing Twitter data using that connector. So&hellip;off to the AWS console!</p>
<p>I&rsquo;ll walk through the steps of creating everything below, but I did create a CloudFormation template in my <a href="https://github.com/dacort/serverless-twitter-analytics.git">serverless-twitter-analytics</a> repository that creates everything you need except for building the connector artifacts and creating the connectors in MSK.</p>
<h2 id="setting-up-amazon-msk">Setting up Amazon MSK</h2>
<p>First we need an MSK cluster. (<em>I was hoping to use the MSK Serverless preview, but alas MSK Connect isn&rsquo;t supported with it.</em>) This is pretty easy to do in the <a href="https://console.aws.amazon.com/msk/home#/clusters">MSK Console</a> - just click the &ldquo;Create cluster&rdquo; button and use the &ldquo;Quick create&rdquo; option.</p>
<p>‚ÑπÔ∏è <strong>One very important thing to note here</strong> ‚Äì The cluster <em>must</em> be created with private subnets and the VPC should have a public subnet with a public NAT Gateway. You may need to use the &ldquo;Custom create&rdquo; option here if the default subnets aren&rsquo;t private.</p>
<p>I used the default Kafka version of 2.6.2 and left all other defaults.</p>
<h2 id="setting-up-amazon-msk-connect">Setting up Amazon MSK Connect</h2>
<p>We&rsquo;re going to use two connectors:</p>
<ul>
<li>One producer to stream data from the Twitter API into Kafka</li>
<li>One consumer to read data from Kafka and write it out into partitioned JSON files in S3</li>
</ul>
<p>This was probably the trickiest part, for a couple reasons solely related to my environment and lack of experience with Kafka:</p>
<ul>
<li>The <a href="https://github.com/jcustenborder/kafka-connect-twitter">kafka-connect-twitter</a> connector didn&rsquo;t want to compile locally</li>
<li>I had to figure out the proper connector configuration to write the data to S3 in my desired JSON format</li>
</ul>
<h3 id="build-the-twitter-connector">Build the Twitter connector</h3>
<p>To address #1, I used a <a href="https://github.com/dacort/serverless-twitter-analytics/blob/main/Dockerfile">Dockerfile</a> to download/build the Twitter connector.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-docker" data-lang="docker"><span style="display:flex;"><span><span style="color:#66d9ef">FROM</span> <span style="color:#e6db74">openjdk:8</span> <span style="color:#66d9ef">AS</span> <span style="color:#e6db74">build-stage</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">RUN</span> apt-get update <span style="color:#f92672">&amp;&amp;</span> apt-get install -y <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    maven <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    zip<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">WORKDIR</span> <span style="color:#e6db74">/usr/src/ktwit</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">ADD</span> https://github.com/jcustenborder/kafka-connect-twitter/archive/refs/tags/0.3.34.tar.gz /usr/src/ktwit/<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">RUN</span> tar xzf 0.3.34.tar.gz <span style="color:#f92672">&amp;&amp;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    cd kafka-connect-twitter-0.3.34 <span style="color:#f92672">&amp;&amp;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    mvn clean package<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">RUN</span> tar xzf kafka-connect-twitter-0.3.34/target/kafka-connect-twitter-0.3-SNAPSHOT.tar.gz <span style="color:#f92672">&amp;&amp;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    zip -rj kafka-connect-twitter-0.3.34.zip usr/share/kafka-connect/kafka-connect-twitter/<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">FROM</span> <span style="color:#e6db74">scratch</span> <span style="color:#66d9ef">AS</span> <span style="color:#e6db74">export-stage</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">COPY</span> --from<span style="color:#f92672">=</span>build-stage /usr/src/ktwit/*.zip /<span style="color:#960050;background-color:#1e0010">
</span></span></span></code></pre></div><p>MSK plugins must be be a zip file, so the last steps in the Dockerfile also decompress the <code>.tar.gz</code> with the built jar and its dependencies and repackage them into a zip file. I use the following command to build and copy the zip file to my local filesystem.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>docker build --output . .
</span></span></code></pre></div><p>You should have a <code>kafka-connect-twitter-0.3.34.zip</code> locally - go ahead and upload that to S3.</p>
<p>For the S3 connector, you can download a pre-built zip file from the <a href="https://www.confluent.io/hub/confluentinc/kafka-connect-s3">Confluent Amazon S3 Sink Connector</a>. Download that and upload it to S3 as well.</p>
<h3 id="create-the-msk-connect-connectors">Create the MSK Connect connectors</h3>
<h4 id="twitter-connector">Twitter connector</h4>
<p>First you create a custom plugin. Go to the &ldquo;Custom plugins&rdquo; section under &ldquo;MSK Connect&rdquo; and then just click the &ldquo;Create custom plugin&rdquo; button. Provide the path to your <code>kafka-connect-twitter-0.3.34.zip</code> file in S3 and you&rsquo;re done.</p>
<p>Next, in the Connectors section click &ldquo;Create connector&rdquo;. Select &ldquo;Use existing custom plugin&rdquo; and select the plugin you just created. In the next section, give it a name (I chose <code>twitter-search-wordle</code>) and select the MSK cluster you created previously. For the Connector configuration, here&rsquo;s what I used below ‚Äì you&rsquo;ll need to replace the <code>twitter.oauth</code> variables with your own credentials and set <code>filter.keywords</code> to the keywords you want to track.</p>
<p>This configuration will automatically create a <code>twitter_json</code> topic in your Kafka cluster.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-ini" data-lang="ini"><span style="display:flex;"><span><span style="color:#a6e22e">connector.class</span><span style="color:#f92672">=</span><span style="color:#e6db74">com.github.jcustenborder.kafka.connect.twitter.TwitterSourceConnector</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">tasks.max</span><span style="color:#f92672">=</span><span style="color:#e6db74">1</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">twitter.http.connectionTimeout</span><span style="color:#f92672">=</span><span style="color:#e6db74">120000</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">twitter.oauth.consumerKey</span><span style="color:#f92672">=</span><span style="color:#e6db74">&lt;TWITTER_CONSUMER_KEY&gt;</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">twitter.oauth.consumerSecret</span><span style="color:#f92672">=</span><span style="color:#e6db74">&lt;TWITTER_CONSUMER_SECRET&gt;</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">twitter.oauth.accessToken</span><span style="color:#f92672">=</span><span style="color:#e6db74">&lt;TWITTER_ACCESS_TOKEN&gt;</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">twitter.oauth.accessTokenSecret</span><span style="color:#f92672">=</span><span style="color:#e6db74">&lt;TWITTER_ACCESS_TOKEN_SECRET&gt;</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">twitter.debug</span><span style="color:#f92672">=</span><span style="color:#e6db74">true</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">process.deletes</span><span style="color:#f92672">=</span><span style="color:#e6db74">true</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">filter.keywords</span><span style="color:#f92672">=</span><span style="color:#e6db74">wordle</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">kafka.status.topic</span><span style="color:#f92672">=</span><span style="color:#e6db74">twitter_json</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">kafka.delete.topic</span><span style="color:#f92672">=</span><span style="color:#e6db74">twitter_deletes_json_02</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">twitter.http.prettyDebug</span><span style="color:#f92672">=</span><span style="color:#e6db74">true</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">topic.creation.enable</span><span style="color:#f92672">=</span><span style="color:#e6db74">true</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">topic.creation.default.partitions</span><span style="color:#f92672">=</span><span style="color:#e6db74">4</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">topic.creation.default.replication.factor</span><span style="color:#f92672">=</span><span style="color:#e6db74">2</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">key.converter</span><span style="color:#f92672">=</span><span style="color:#e6db74">org.apache.kafka.connect.json.JsonConverter</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">key.converter.schemas.enable</span><span style="color:#f92672">=</span><span style="color:#e6db74">false</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">value.converter</span><span style="color:#f92672">=</span><span style="color:#e6db74">org.apache.kafka.connect.json.JsonConverter</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">value.converter.schemas.enable</span><span style="color:#f92672">=</span><span style="color:#e6db74">false</span>
</span></span></code></pre></div><p>You&rsquo;ll also need to create an IAM service role for your connectors. You can find the cluster ARN in the console on your cluster summary page. I use the policy below for both my Twitter and S3 connectors.</p>
<details>
  <summary>Example IAM policy</summary>
  <div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;Version&#34;</span>: <span style="color:#e6db74">&#34;2012-10-17&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;Statement&#34;</span>: [
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;Effect&#34;</span>: <span style="color:#e6db74">&#34;Allow&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;Action&#34;</span>: [
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;kafka-cluster:Connect&#34;</span>,
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;kafka-cluster:DescribeCluster&#34;</span>
</span></span><span style="display:flex;"><span>            ],
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;Resource&#34;</span>: [
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;arn:aws:kafka:&lt;region&gt;:&lt;account_id&gt;:cluster/&lt;cluster_name&gt;/&lt;cluster_uuid&gt;&#34;</span>
</span></span><span style="display:flex;"><span>            ]
</span></span><span style="display:flex;"><span>        },
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;Effect&#34;</span>: <span style="color:#e6db74">&#34;Allow&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;Action&#34;</span>: [
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;kafka-cluster:ReadData&#34;</span>,
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;kafka-cluster:DescribeTopic&#34;</span>
</span></span><span style="display:flex;"><span>            ],
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;Resource&#34;</span>: [
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;arn:aws:kafka:&lt;region&gt;:&lt;account_id&gt;:cluster/&lt;cluster_name&gt;/&lt;cluster_uuid&gt;/*&#34;</span>,
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;arn:aws:kafka:&lt;region&gt;:&lt;account_id&gt;:topic/&lt;cluster_name&gt;/&lt;cluster_uuid&gt;/*&#34;</span>
</span></span><span style="display:flex;"><span>            ]
</span></span><span style="display:flex;"><span>        },
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;Effect&#34;</span>: <span style="color:#e6db74">&#34;Allow&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;Action&#34;</span>: [
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;kafka-cluster:WriteData&#34;</span>,
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;kafka-cluster:DescribeTopic&#34;</span>
</span></span><span style="display:flex;"><span>            ],
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;Resource&#34;</span>: [
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;arn:aws:kafka:&lt;region&gt;:&lt;account_id&gt;:cluster/&lt;cluster_name&gt;/&lt;cluster_uuid&gt;/*&#34;</span>,
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;arn:aws:kafka:&lt;region&gt;:&lt;account_id&gt;:topic/&lt;cluster_name&gt;/&lt;cluster_uuid&gt;/*&#34;</span>
</span></span><span style="display:flex;"><span>            ]
</span></span><span style="display:flex;"><span>        },
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;Effect&#34;</span>: <span style="color:#e6db74">&#34;Allow&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;Action&#34;</span>: [
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;kafka-cluster:CreateTopic&#34;</span>,
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;kafka-cluster:WriteData&#34;</span>,
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;kafka-cluster:ReadData&#34;</span>,
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;kafka-cluster:DescribeTopic&#34;</span>
</span></span><span style="display:flex;"><span>            ],
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;Resource&#34;</span>: [
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;arn:aws:kafka:&lt;region&gt;:&lt;account_id&gt;:topic/&lt;cluster_name&gt;/&lt;cluster_uuid&gt;/__amazon_msk_connect_*&#34;</span>,
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;arn:aws:kafka:&lt;region&gt;:&lt;account_id&gt;:topic/&lt;cluster_name&gt;/&lt;cluster_uuid&gt;/*&#34;</span>
</span></span><span style="display:flex;"><span>            ]
</span></span><span style="display:flex;"><span>        },
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;Effect&#34;</span>: <span style="color:#e6db74">&#34;Allow&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;Action&#34;</span>: [
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;kafka-cluster:AlterGroup&#34;</span>,
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;kafka-cluster:DescribeGroup&#34;</span>
</span></span><span style="display:flex;"><span>            ],
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;Resource&#34;</span>: [
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;arn:aws:kafka:&lt;region&gt;:&lt;account_id&gt;:group/&lt;cluster_name&gt;/&lt;cluster_uuid&gt;/__amazon_msk_connect_*&#34;</span>,
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;arn:aws:kafka:&lt;region&gt;:&lt;account_id&gt;:group/&lt;cluster_name&gt;/&lt;cluster_uuid&gt;/connect-*&#34;</span>
</span></span><span style="display:flex;"><span>            ]
</span></span><span style="display:flex;"><span>        },
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;Effect&#34;</span>: <span style="color:#e6db74">&#34;Allow&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;Action&#34;</span>: [
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;s3:ListAllMyBuckets&#34;</span>
</span></span><span style="display:flex;"><span>            ],
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;Resource&#34;</span>: <span style="color:#e6db74">&#34;arn:aws:s3:::*&#34;</span>
</span></span><span style="display:flex;"><span>        },
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;Effect&#34;</span>: <span style="color:#e6db74">&#34;Allow&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;Action&#34;</span>: [
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;s3:ListBucket&#34;</span>,
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;s3:GetBucketLocation&#34;</span>
</span></span><span style="display:flex;"><span>            ],
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;Resource&#34;</span>: <span style="color:#e6db74">&#34;arn:aws:s3:::s3-bucket-name&#34;</span>
</span></span><span style="display:flex;"><span>        },
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;Effect&#34;</span>: <span style="color:#e6db74">&#34;Allow&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;Action&#34;</span>: [
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;s3:PutObject&#34;</span>,
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;s3:GetObject&#34;</span>,
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;s3:AbortMultipartUpload&#34;</span>,
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;s3:ListMultipartUploadParts&#34;</span>,
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#34;s3:ListBucketMultipartUploads&#34;</span>
</span></span><span style="display:flex;"><span>            ],
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;Resource&#34;</span>: <span style="color:#e6db74">&#34;arn:aws:s3:::s3-bucket-name/*&#34;</span>
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>
</details>

<h4 id="s3-connector">S3 connector</h4>
<p>Now we create another custom plugin for S3 and then a connector using that plugin. All the options are the same as the above, but we provide this connector configuration. You&rsquo;ll need to update the <code>s3.region</code> and <code>s3.bucket.name</code> keys with your specific values.</p>
<p>By default, this configuration will output line-based JSON files partitioned by year-month-day-hour (<code>path.format</code>) and flush data to S3 every 1,000 records (<code>flush.size</code>). The <code>ByteArrayFormat</code> is used so the data gets written to S3 in raw JSON format (per <a href="https://stackoverflow.com/q/68223215/5141922">https://stackoverflow.com/q/68223215/5141922</a>). Per the <a href="https://docs.confluent.io/kafka-connect-s3-sink/current/overview.html#s3-object-uploads">Kafka Connect S3 docs</a>, I also configured &ldquo;scheduled rotation&rdquo; on the sink so that data will be flushed to S3 on a regular basis.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-ini" data-lang="ini"><span style="display:flex;"><span><span style="color:#a6e22e">connector.class</span><span style="color:#f92672">=</span><span style="color:#e6db74">io.confluent.connect.s3.S3SinkConnector</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">s3.bucket.name</span><span style="color:#f92672">=</span><span style="color:#e6db74">&lt;S3_BUCKET_NAME&gt;</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">s3.region</span><span style="color:#f92672">=</span><span style="color:#e6db74">&lt;S3_BUCKET_REGION&gt;</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">flush.size</span><span style="color:#f92672">=</span><span style="color:#e6db74">1000</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">format.bytearray.extension</span><span style="color:#f92672">=</span><span style="color:#e6db74">.json</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">format.class</span><span style="color:#f92672">=</span><span style="color:#e6db74">io.confluent.connect.s3.format.bytearray.ByteArrayFormat</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">locale</span><span style="color:#f92672">=</span><span style="color:#e6db74">en-US</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">partition.duration.ms</span><span style="color:#f92672">=</span><span style="color:#e6db74">600000</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">partitioner.class</span><span style="color:#f92672">=</span><span style="color:#e6db74">io.confluent.connect.storage.partitioner.TimeBasedPartitioner</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">path.format</span><span style="color:#f92672">=</span><span style="color:#e6db74">&#39;year&#39;=YYYY/&#39;month&#39;=MM/&#39;day&#39;=dd/&#39;hour&#39;=HH</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">rotate.schedule.interval.ms</span><span style="color:#f92672">=</span><span style="color:#e6db74">300000</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">s3.compression.type</span><span style="color:#f92672">=</span><span style="color:#e6db74">gzip</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">schema.compatibility</span><span style="color:#f92672">=</span><span style="color:#e6db74">NONE</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">storage.class</span><span style="color:#f92672">=</span><span style="color:#e6db74">io.confluent.connect.s3.storage.S3Storage</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">tasks.max</span><span style="color:#f92672">=</span><span style="color:#e6db74">2</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">timezone</span><span style="color:#f92672">=</span><span style="color:#e6db74">UTC</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">topics.dir</span><span style="color:#f92672">=</span><span style="color:#e6db74">wordle_data/json</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">topics</span><span style="color:#f92672">=</span><span style="color:#e6db74">twitter_json_02,twitter_deletes_json_02</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">value.converter</span><span style="color:#f92672">=</span><span style="color:#e6db74">org.apache.kafka.connect.converters.ByteArrayConverter</span>
</span></span></code></pre></div><p>The tricky part here was figuring out the right combination of format classes and key/value converters. My initial attempts had Java Structs getting written out to S3 and then escaped JSON, but I finally found the right combination of <code>JsonConverter</code> for the producer and <code>ByteArrayFormat</code> for the consumer. I wanted JSON so I could easily consume the data in Athena!</p>
<h2 id="setting-up-amazon-athena">Setting up Amazon Athena</h2>
<p>With the configuration above, you should now have gzip&rsquo;ed JSON Twitter data streaming into your S3 bucket!</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>‚ûú aws s3 ls s3://<span style="color:#e6db74">${</span>S3_BUCKET<span style="color:#e6db74">}</span>/wordle_data/json/ --recursive | head
</span></span><span style="display:flex;"><span>2022-01-10 16:23:33     <span style="color:#ae81ff">346352</span> wordle_data/json/twitter_json/year<span style="color:#f92672">=</span>2022/month<span style="color:#f92672">=</span>01/day<span style="color:#f92672">=</span>11/hour<span style="color:#f92672">=</span>00/twitter_json+0+0000000000.json.gz
</span></span><span style="display:flex;"><span>2022-01-10 16:23:34     <span style="color:#ae81ff">346449</span> wordle_data/json/twitter_json/year<span style="color:#f92672">=</span>2022/month<span style="color:#f92672">=</span>01/day<span style="color:#f92672">=</span>11/hour<span style="color:#f92672">=</span>00/twitter_json+0+0000001000.json.gz
</span></span><span style="display:flex;"><span>2022-01-10 16:23:35     <span style="color:#ae81ff">329079</span> wordle_data/json/twitter_json/year<span style="color:#f92672">=</span>2022/month<span style="color:#f92672">=</span>01/day<span style="color:#f92672">=</span>11/hour<span style="color:#f92672">=</span>00/twitter_json+0+0000002000.json.gz
</span></span><span style="display:flex;"><span>2022-01-10 16:25:01     <span style="color:#ae81ff">163478</span> wordle_data/json/twitter_json/year<span style="color:#f92672">=</span>2022/month<span style="color:#f92672">=</span>01/day<span style="color:#f92672">=</span>11/hour<span style="color:#f92672">=</span>00/twitter_json+0+0000003000.json.gz
</span></span><span style="display:flex;"><span>2022-01-10 16:30:01      <span style="color:#ae81ff">58634</span> wordle_data/json/twitter_json/year<span style="color:#f92672">=</span>2022/month<span style="color:#f92672">=</span>01/day<span style="color:#f92672">=</span>11/hour<span style="color:#f92672">=</span>00/twitter_json+0+0000003505.json.gz
</span></span><span style="display:flex;"><span>2022-01-10 16:35:01      <span style="color:#ae81ff">55148</span> wordle_data/json/twitter_json/year<span style="color:#f92672">=</span>2022/month<span style="color:#f92672">=</span>01/day<span style="color:#f92672">=</span>11/hour<span style="color:#f92672">=</span>00/twitter_json+0+0000003681.json.gz
</span></span></code></pre></div><p>Next, we&rsquo;ll set up a <a href="https://console.aws.amazon.com/glue/home#catalog:tab=crawlers">Glue Crawler</a> to create our data catalog table and keep the partitions up-to-date.</p>
<p>When creating the crawler, point it to the S3 bucket and prefix you configured above (<code>s3://&lt;BUCKET_NAME&gt;/wordle_data/json/</code>). I also configured it to &ldquo;Crawl new folders only&rdquo; and run at 6 minutes after the hour (<code>06 0/1 * * ? *</code>). Since I configured the S3 sink connector to rate files every 5 minute, this ensures that every run should pick up the new partition as quick as possible. Here&rsquo;s a screenshot of my final configuration:</p>
<p><img loading="lazy" src="/posts/serverless-analytics-of-twitter-data/glue_crawler_config.png"></p>
<p>You may want to run your crawler manually the first time, but once you do you should see a new table show up in your data catalog! And you should be able to immediately query the data in Athena! (<em>If you haven&rsquo;t setup Athena yet, you&rsquo;ll need to create a bucket for results.</em>)</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sql" data-lang="sql"><span style="display:flex;"><span><span style="color:#66d9ef">SELECT</span> createdat, text,
</span></span><span style="display:flex;"><span>    regexp_extract(text, <span style="color:#e6db74">&#39;Wordle (\d+) (\d)/6&#39;</span>, <span style="color:#ae81ff">1</span>) <span style="color:#66d9ef">AS</span> wordle_index,
</span></span><span style="display:flex;"><span>    regexp_extract(text, <span style="color:#e6db74">&#39;Wordle (\d+) (\d)/6&#39;</span>, <span style="color:#ae81ff">2</span>) <span style="color:#66d9ef">AS</span> num_tries,
</span></span><span style="display:flex;"><span>    regexp_extract_all(
</span></span><span style="display:flex;"><span>        regexp_replace(text, <span style="color:#e6db74">&#39;\u2B1C&#39;</span>, <span style="color:#e6db74">&#39;‚¨õ&#39;</span>),
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#39;(\u2B1C|\u2B1B|\x{1F7E9}|\x{1F7E8})+&#39;</span>
</span></span><span style="display:flex;"><span>    ) <span style="color:#66d9ef">AS</span> guess_blocks
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">FROM</span> <span style="color:#e6db74">&#34;AwsDataCatalog&#34;</span>.<span style="color:#e6db74">&#34;default&#34;</span>.<span style="color:#e6db74">&#34;wordle_json&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">WHERE</span> retweet<span style="color:#f92672">=</span><span style="color:#66d9ef">false</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">AND</span> <span style="color:#66d9ef">CAST</span>(regexp_extract(text, <span style="color:#e6db74">&#39;Wordle (\d+) (\d)/6&#39;</span>, <span style="color:#ae81ff">1</span>) <span style="color:#66d9ef">as</span> int) <span style="color:#66d9ef">BETWEEN</span> <span style="color:#ae81ff">100</span> <span style="color:#66d9ef">AND</span> <span style="color:#ae81ff">300</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">limit</span> <span style="color:#ae81ff">10</span>;
</span></span></code></pre></div><p><img loading="lazy" src="/posts/serverless-analytics-of-twitter-data/athena_query.png"></p>
<p>And that&rsquo;s it! Now you can query to your heart&rsquo;s content. :) <em>Note that <code>SELECT *</code> won&rsquo;t work due to some of the nested fields not being defined properly.</em></p>
]]></content:encoded></item><item><title>An Introduction to Modern Data Lake Storage Layers</title><link>https://dacort.xyz/posts/modern-data-lake-storage-layers/</link><pubDate>Wed, 02 Feb 2022 14:22:22 -0800</pubDate><guid>https://dacort.xyz/posts/modern-data-lake-storage-layers/</guid><description>&lt;p&gt;In recent years we&amp;rsquo;ve seen a rise in new storage layers for data lakes. In 2017, &lt;a href="https://eng.uber.com/hoodie/"&gt;Uber announced Hudi&lt;/a&gt; - an incremental processing framework for data pipelines. In 2018, &lt;a href="https://conferences.oreilly.com/strata/strata-ny-2018/public/schedule/detail/69503.html"&gt;Netflix introduced Iceberg&lt;/a&gt; - a new table format for managing extremely large cloud datasets. And in 2019, &lt;a href="https://techcrunch.com/2019/04/24/databricks-open-sources-delta-lake-to-make-data-lakes-more-reliable/"&gt;Databricks open-sourced Delta Lake&lt;/a&gt; - originally intended to bring ACID transactions to data lakes.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;üìπ &lt;em&gt;If you&amp;rsquo;d like to watch a video that discusses the content of this post, I&amp;rsquo;ve also recorded &lt;a href="https://www.youtube.com/watch?v=fryfx0Zg7KA"&gt;an overview here&lt;/a&gt;. Each relevant section below will also link to individual timestamps.&lt;/em&gt;&lt;/p&gt;</description><content:encoded><![CDATA[<p>In recent years we&rsquo;ve seen a rise in new storage layers for data lakes. In 2017, <a href="https://eng.uber.com/hoodie/">Uber announced Hudi</a> - an incremental processing framework for data pipelines. In 2018, <a href="https://conferences.oreilly.com/strata/strata-ny-2018/public/schedule/detail/69503.html">Netflix introduced Iceberg</a> - a new table format for managing extremely large cloud datasets. And in 2019, <a href="https://techcrunch.com/2019/04/24/databricks-open-sources-delta-lake-to-make-data-lakes-more-reliable/">Databricks open-sourced Delta Lake</a> - originally intended to bring ACID transactions to data lakes.</p>
<blockquote>
<p>üìπ <em>If you&rsquo;d like to watch a video that discusses the content of this post, I&rsquo;ve also recorded <a href="https://www.youtube.com/watch?v=fryfx0Zg7KA">an overview here</a>. Each relevant section below will also link to individual timestamps.</em></p>
</blockquote>
<p>This post aims to introduce each of these engines and give some insight into how they function under the hood and some of the differences in each. While I&rsquo;ll summarize the findings here, you can also view my Jupyter notebooks for each in my <a href="https://github.com/dacort/modern-data-lake-storage-layers/tree/main/notebooks">modern-data-lake-storage-layers</a> repository. We begin with basic operations of writing and updating datasets.</p>
<p>One thing to note about all of these frameworks is that each began with a different challenge they were solving for, but over time they have begun to converge on a common set of functionality. I should <strong>also</strong> note that I am learning about these frameworks as well - the comments here are neither authoritative or comprehensive. ü§ó</p>
<h2 id="apache-hudi">Apache Hudi</h2>
<blockquote>
<p>üìπ <a href="https://www.youtube.com/watch?v=fryfx0Zg7KA&amp;t=323s">Intro to Apache Hudi video</a></p>
</blockquote>
<p>Apache Hudi (Hadoop Upsert Delete and Incremental) was originally designed as an incremental stream processing framework and was built to combine the benefits of stream and batch processing. Hudi can be used with Spark, Flink, Presto, Trino and Hive, but much of the original work was focused around Spark and that&rsquo;s what I use for these examples. One of the other huge benefits of Hudi is the concept of a self-managed data layer. For example, Hudi can automatically perform <a href="https://hudi.apache.org/docs/compaction">asynchronous compaction</a> to optimize data lakes and also supports <a href="https://hudi.apache.org/docs/concurrency_control">multi-writer gaurantees</a>. Hudi also offers flexibility in storage formats depending on read/write requirements and data size.</p>
<p>For Hudi, we create a simple Spark DataFrame partitioned by <code>creation_date</code> and write that to S3.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Create a DataFrame</span>
</span></span><span style="display:flex;"><span>inputDF <span style="color:#f92672">=</span> spark<span style="color:#f92672">.</span>createDataFrame(
</span></span><span style="display:flex;"><span>    [
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;100&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01T13:51:39.340396Z&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;101&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01T12:14:58.597216Z&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;102&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01T13:51:40.417052Z&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;103&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01T13:51:40.519832Z&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;104&#34;</span>, <span style="color:#e6db74">&#34;2015-01-02&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01T12:15:00.512679Z&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;105&#34;</span>, <span style="color:#e6db74">&#34;2015-01-02&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01T13:51:42.248818Z&#34;</span>),
</span></span><span style="display:flex;"><span>    ],
</span></span><span style="display:flex;"><span>    [<span style="color:#e6db74">&#34;id&#34;</span>, <span style="color:#e6db74">&#34;creation_date&#34;</span>, <span style="color:#e6db74">&#34;last_update_time&#34;</span>],
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Specify common DataSourceWriteOptions in the single hudiOptions variable</span>
</span></span><span style="display:flex;"><span>hudiOptions <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;hoodie.table.name&#34;</span>: <span style="color:#e6db74">&#34;my_hudi_table&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;hoodie.datasource.write.recordkey.field&#34;</span>: <span style="color:#e6db74">&#34;id&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;hoodie.datasource.write.partitionpath.field&#34;</span>: <span style="color:#e6db74">&#34;creation_date&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;hoodie.datasource.write.precombine.field&#34;</span>: <span style="color:#e6db74">&#34;last_update_time&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;hoodie.datasource.hive_sync.enable&#34;</span>: <span style="color:#e6db74">&#34;true&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;hoodie.datasource.hive_sync.table&#34;</span>: <span style="color:#e6db74">&#34;my_hudi_table&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;hoodie.datasource.hive_sync.partition_fields&#34;</span>: <span style="color:#e6db74">&#34;creation_date&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;hoodie.datasource.hive_sync.partition_extractor_class&#34;</span>: <span style="color:#e6db74">&#34;org.apache.hudi.hive.MultiPartKeysValueExtractor&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;hoodie.index.type&#34;</span>: <span style="color:#e6db74">&#34;GLOBAL_BLOOM&#34;</span>,  <span style="color:#75715e"># This is required if we want to ensure we upsert a record, even if the partition changes</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;hoodie.bloom.index.update.partition.path&#34;</span>: <span style="color:#e6db74">&#34;true&#34;</span>,  <span style="color:#75715e"># This is required to write the data into the new partition (defaults to false in 0.8.0, true in 0.9.0)</span>
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Write a DataFrame as a Hudi dataset</span>
</span></span><span style="display:flex;"><span>inputDF<span style="color:#f92672">.</span>write<span style="color:#f92672">.</span>format(<span style="color:#e6db74">&#34;org.apache.hudi&#34;</span>)<span style="color:#f92672">.</span>option(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;hoodie.datasource.write.operation&#34;</span>, <span style="color:#e6db74">&#34;insert&#34;</span>
</span></span><span style="display:flex;"><span>)<span style="color:#f92672">.</span>options(<span style="color:#f92672">**</span>hudiOptions)<span style="color:#f92672">.</span>mode(<span style="color:#e6db74">&#34;overwrite&#34;</span>)<span style="color:#f92672">.</span>save(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;s3://</span><span style="color:#e6db74">{</span>S3_BUCKET_NAME<span style="color:#e6db74">}</span><span style="color:#e6db74">/tmp/hudi/&#34;</span>)
</span></span></code></pre></div><p>When we look at the file structure on S3, we see a few things:</p>
<ol>
<li>A <code>hoodie.properties</code> file</li>
</ol>
<pre tabindex="0"><code>2022-01-14 00:33:46        503 tmp/hudi/.hoodie/hoodie.properties
</code></pre><p>This file contains certain metadata about the Hudi dataset:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-ini" data-lang="ini"><span style="display:flex;"><span><span style="color:#75715e">#Properties saved on Fri Jan 14 00:33:45 UTC 2022</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#Fri Jan 14 00:33:45 UTC 2022</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">hoodie.table.precombine.field</span><span style="color:#f92672">=</span><span style="color:#e6db74">last_update_time</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">hoodie.table.partition.fields</span><span style="color:#f92672">=</span><span style="color:#e6db74">creation_date</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">hoodie.table.type</span><span style="color:#f92672">=</span><span style="color:#e6db74">COPY_ON_WRITE</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">hoodie.archivelog.folder</span><span style="color:#f92672">=</span><span style="color:#e6db74">archived</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">hoodie.populate.meta.fields</span><span style="color:#f92672">=</span><span style="color:#e6db74">true</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">hoodie.timeline.layout.version</span><span style="color:#f92672">=</span><span style="color:#e6db74">1</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">hoodie.table.version</span><span style="color:#f92672">=</span><span style="color:#e6db74">2</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">hoodie.table.recordkey.fields</span><span style="color:#f92672">=</span><span style="color:#e6db74">id</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">hoodie.table.base.file.format</span><span style="color:#f92672">=</span><span style="color:#e6db74">PARQUET</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">hoodie.table.keygenerator.class</span><span style="color:#f92672">=</span><span style="color:#e6db74">org.apache.hudi.keygen.SimpleKeyGenerator</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">hoodie.table.name</span><span style="color:#f92672">=</span><span style="color:#e6db74">my_hudi_table</span>
</span></span></code></pre></div><ol start="2">
<li>A set of commit-related files</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>2022-01-14 00:33:57       2706 tmp/hudi/.hoodie/20220114003341.commit
</span></span><span style="display:flex;"><span>2022-01-14 00:33:48          0 tmp/hudi/.hoodie/20220114003341.commit.requested
</span></span><span style="display:flex;"><span>2022-01-14 00:33:52       1842 tmp/hudi/.hoodie/20220114003341.inflight
</span></span></code></pre></div><ol start="3">
<li>The actual <code>.parquet</code> data files and associated metadata organized into date-based partitions.</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>2022-01-14 00:33:54         93 tmp/hudi/2015-01-01/.hoodie_partition_metadata
</span></span><span style="display:flex;"><span>2022-01-14 00:33:54     434974 tmp/hudi/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet
</span></span><span style="display:flex;"><span>2022-01-14 00:33:55         93 tmp/hudi/2015-01-02/.hoodie_partition_metadata
</span></span><span style="display:flex;"><span>2022-01-14 00:33:55     434943 tmp/hudi/2015-01-02/43051d12-87e7-4dfb-8201-6ce293cf0df7-0_1-6-99_20220114003341.parquet
</span></span></code></pre></div><p>We then update the <code>creation_date</code> of one row in this dataset.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql.functions <span style="color:#f92672">import</span> lit
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create a new DataFrame from the first row of inputDF with a different creation_date value</span>
</span></span><span style="display:flex;"><span>updateDF <span style="color:#f92672">=</span> inputDF<span style="color:#f92672">.</span>where(<span style="color:#e6db74">&#34;id = 100&#34;</span>)<span style="color:#f92672">.</span>withColumn(<span style="color:#e6db74">&#34;creation_date&#34;</span>, lit(<span style="color:#e6db74">&#34;2022-01-11&#34;</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>updateDF<span style="color:#f92672">.</span>show()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Update by using the &#34;upsert&#34; operation</span>
</span></span><span style="display:flex;"><span>updateDF<span style="color:#f92672">.</span>write<span style="color:#f92672">.</span>format(<span style="color:#e6db74">&#34;org.apache.hudi&#34;</span>)<span style="color:#f92672">.</span>option(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;hoodie.datasource.write.operation&#34;</span>, <span style="color:#e6db74">&#34;upsert&#34;</span>
</span></span><span style="display:flex;"><span>)<span style="color:#f92672">.</span>options(<span style="color:#f92672">**</span>hudiOptions)<span style="color:#f92672">.</span>mode(<span style="color:#e6db74">&#34;append&#34;</span>)<span style="color:#f92672">.</span>save(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;s3://</span><span style="color:#e6db74">{</span>S3_BUCKET_NAME<span style="color:#e6db74">}</span><span style="color:#e6db74">/tmp/hudi/&#34;</span>)
</span></span></code></pre></div><p>One thing to note here is that since we&rsquo;re updating a partition value (<strong>DANGER!</strong>), we had to set the <code>hoodie.index.type</code> to <code>GLOBAL_BLOOM</code> as well as setting <code>hoodie.bloom.index.update.partition.path</code> to <code>true</code>. This can have a large impact on performance so normally we would try not to change a partition value in a production environment, but it&rsquo;s useful here to see the impact it has. You can mind more details in the Hudi FAQ about <a href="https://hudi.apache.org/learn/faq/#how-does-the-hudi-indexing-work--what-are-its-benefits">Hudi indexing</a>.</p>
<p>After this write, we have a new set of commit-related files on S3:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>2022-01-14 00:34:15       2706 tmp/hudi/.hoodie/20220114003401.commit
</span></span><span style="display:flex;"><span>2022-01-14 00:34:03          0 tmp/hudi/.hoodie/20220114003401.commit.requested
</span></span><span style="display:flex;"><span>2022-01-14 00:34:08       2560 tmp/hudi/.hoodie/20220114003401.inflight
</span></span></code></pre></div><p>And we actually have <strong>2</strong> new <code>.parquet</code> files:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>2022-01-14 00:34:12     434925 tmp/hudi/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-37-13680_20220114003401.parquet
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>2022-01-14 00:34:13         93 tmp/hudi/2022-01-11/.hoodie_partition_metadata
</span></span><span style="display:flex;"><span>2022-01-14 00:34:14     434979 tmp/hudi/2022-01-11/0c210872-484e-428b-a9ca-90a26e42125c-0_1-43-13681_20220114003401.parquet
</span></span></code></pre></div><p>So what happened with the update is that the old partition (<code>2015-01-01</code>) had its data overwritten and the new partition (<code>2022-01-11</code>) <em>also</em> had data written to it. You can now see why the global bloom index could have such a large impact on write performance as there is significant potential for write amplication.</p>
<p>If we query the data and add the source filename for each row, we can also see that data for the old partition now comes from the new parquet file (notice the commit ID <code>20220114003401</code> shows up in the filename):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span>  pyspark.sql.functions <span style="color:#f92672">import</span> input_file_name
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>snapshotQueryDF <span style="color:#f92672">=</span> spark<span style="color:#f92672">.</span>read \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>format(<span style="color:#e6db74">&#39;org.apache.hudi&#39;</span>) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>load(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;s3://</span><span style="color:#e6db74">{</span>S3_BUCKET_NAME<span style="color:#e6db74">}</span><span style="color:#e6db74">/tmp/hudi/&#34;</span>) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>select(<span style="color:#e6db74">&#39;id&#39;</span>, <span style="color:#e6db74">&#39;creation_date&#39;</span>) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>withColumn(<span style="color:#e6db74">&#34;filename&#34;</span>, input_file_name())
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>snapshotQueryDF<span style="color:#f92672">.</span>show(truncate<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>+---+-------------+------------------------------------------------------------------------------------------------------------------------------+
</span></span><span style="display:flex;"><span>|id |creation_date|filename                                                                                                                      |
</span></span><span style="display:flex;"><span>+---+-------------+------------------------------------------------------------------------------------------------------------------------------+
</span></span><span style="display:flex;"><span>|100|2022-01-11   |/hudi/2022-01-11/0c210872-484e-428b-a9ca-90a26e42125c-0_1-43-13681_20220114003401.parquet                                     |
</span></span><span style="display:flex;"><span>|105|2015-01-02   |/hudi/2015-01-02/43051d12-87e7-4dfb-8201-6ce293cf0df7-0_1-6-99_20220114003341.parquet                                         |
</span></span><span style="display:flex;"><span>|104|2015-01-02   |/hudi/2015-01-02/43051d12-87e7-4dfb-8201-6ce293cf0df7-0_1-6-99_20220114003341.parquet                                         |
</span></span><span style="display:flex;"><span>|102|2015-01-01   |/hudi/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-37-13680_20220114003401.parquet                                     |
</span></span><span style="display:flex;"><span>|103|2015-01-01   |/hudi/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-37-13680_20220114003401.parquet                                     |
</span></span><span style="display:flex;"><span>|101|2015-01-01   |/hudi/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-37-13680_20220114003401.parquet                                     |
</span></span><span style="display:flex;"><span>+---+-------------+------------------------------------------------------------------------------------------------------------------------------+
</span></span></code></pre></div><p>One other thing to note is that Hudi adds quite a bit of metadata to your Parquet files. This data helps enable record-level change streams - more detail can be found in this <a href="https://hudi.apache.org/blog/2021/07/21/streaming-data-lake-platform/#writers">comprehensive blog post about the Hudi platform</a>. If we use native Spark to read one of the Parquet files and show it, we see that there&rsquo;s various <code>_hoodie</code>-prefixed keys.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql.functions <span style="color:#f92672">import</span> split
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>rawDF <span style="color:#f92672">=</span> (
</span></span><span style="display:flex;"><span>    spark<span style="color:#f92672">.</span>read<span style="color:#f92672">.</span>parquet(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;s3://</span><span style="color:#e6db74">{</span>S3_BUCKET_NAME<span style="color:#e6db74">}</span><span style="color:#e6db74">/tmp/hudi/*/*.parquet&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>withColumn(<span style="color:#e6db74">&#34;filename&#34;</span>, split(input_file_name(), <span style="color:#e6db74">&#34;tmp/hudi&#34;</span>)<span style="color:#f92672">.</span>getItem(<span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>sort(<span style="color:#e6db74">&#34;_hoodie_commit_time&#34;</span>, <span style="color:#e6db74">&#34;_hoodie_commit_seqno&#34;</span>)
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>rawDF<span style="color:#f92672">.</span>show(truncate<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>+-------------------+--------------------+------------------+----------------------+------------------------------------------------------------------------+---+-------------+---------------------------+------------------------------------------------------------------------------------+
</span></span><span style="display:flex;"><span>|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|_hoodie_file_name                                                       |id |creation_date|last_update_time           |filename                                                                            |
</span></span><span style="display:flex;"><span>+-------------------+--------------------+------------------+----------------------+------------------------------------------------------------------------+---+-------------+---------------------------+------------------------------------------------------------------------------------+
</span></span><span style="display:flex;"><span>|20220114003341     |20220114003341_0_1  |100               |2015-01-01            |57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet    |100|2015-01-01   |2015-01-01T13:51:39.340396Z|/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet    |
</span></span><span style="display:flex;"><span>|20220114003341     |20220114003341_0_2  |102               |2015-01-01            |57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet    |102|2015-01-01   |2015-01-01T13:51:40.417052Z|/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-37-13680_20220114003401.parquet|
</span></span><span style="display:flex;"><span>|20220114003341     |20220114003341_0_2  |102               |2015-01-01            |57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet    |102|2015-01-01   |2015-01-01T13:51:40.417052Z|/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet    |
</span></span><span style="display:flex;"><span>|20220114003341     |20220114003341_0_3  |103               |2015-01-01            |57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet    |103|2015-01-01   |2015-01-01T13:51:40.519832Z|/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-37-13680_20220114003401.parquet|
</span></span><span style="display:flex;"><span>|20220114003341     |20220114003341_0_3  |103               |2015-01-01            |57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet    |103|2015-01-01   |2015-01-01T13:51:40.519832Z|/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet    |
</span></span><span style="display:flex;"><span>|20220114003341     |20220114003341_0_4  |101               |2015-01-01            |57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet    |101|2015-01-01   |2015-01-01T12:14:58.597216Z|/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-37-13680_20220114003401.parquet|
</span></span><span style="display:flex;"><span>|20220114003341     |20220114003341_0_4  |101               |2015-01-01            |57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet    |101|2015-01-01   |2015-01-01T12:14:58.597216Z|/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet    |
</span></span><span style="display:flex;"><span>|20220114003341     |20220114003341_1_5  |105               |2015-01-02            |43051d12-87e7-4dfb-8201-6ce293cf0df7-0_1-6-99_20220114003341.parquet    |105|2015-01-02   |2015-01-01T13:51:42.248818Z|/2015-01-02/43051d12-87e7-4dfb-8201-6ce293cf0df7-0_1-6-99_20220114003341.parquet    |
</span></span><span style="display:flex;"><span>|20220114003341     |20220114003341_1_6  |104               |2015-01-02            |43051d12-87e7-4dfb-8201-6ce293cf0df7-0_1-6-99_20220114003341.parquet    |104|2015-01-02   |2015-01-01T12:15:00.512679Z|/2015-01-02/43051d12-87e7-4dfb-8201-6ce293cf0df7-0_1-6-99_20220114003341.parquet    |
</span></span><span style="display:flex;"><span>|20220114003401     |20220114003401_1_1  |100               |2022-01-11            |0c210872-484e-428b-a9ca-90a26e42125c-0_1-43-13681_20220114003401.parquet|100|2022-01-11   |2015-01-01T13:51:39.340396Z|/2022-01-11/0c210872-484e-428b-a9ca-90a26e42125c-0_1-43-13681_20220114003401.parquet|
</span></span><span style="display:flex;"><span>+-------------------+--------------------+------------------+----------------------+------------------------------------------------------------------------+---+-------------+---------------------------+------------------------------------------------------------------------------------+
</span></span></code></pre></div><p>In the background, Hudi figures out which commits and values to show based on the commit files and metadata in the parquet files.</p>
<h2 id="apache-iceberg">Apache Iceberg</h2>
<blockquote>
<p>üìπ <a href="https://www.youtube.com/watch?v=fryfx0Zg7KA&amp;t=1039s">Intro to Apache Iceberg video</a></p>
</blockquote>
<p>When I first heard about Iceberg, the phrase &ldquo;table format for storing large, slow-moving tabular data&rdquo; didn&rsquo;t really make sense to me. But after working with data lakes at scale, it became quite clear. Apache Hive is a popular data warehouse project that provides a SQL-like interface to large datasets. Built on top of Hadoop, it originally used HDFS as its data store. With cloud migrations, object stores like Amazon S3 enabled the ability to store even more data particularly without the operational concerns of a large Hadoop cluster, but with some limitations when compared to HDFS. Specifically, directory listings are slower (simple physics here, network calls are slower), renames are not atomic (by design), and results were previously eventually consistent.</p>
<p>So imagine you are Netflix, you have <a href="https://netflixtechblog.com/optimizing-data-warehouse-storage-7b94a48fdcbe">hundreds of petabytes of data</a> stored on S3, and you need a way for your organization to efficiently query this. You need a data storage layer that reduces or removes directory listings, you want atomic changes, and you want to ensure that when you&rsquo;re reading your data you get consistent results. <em>There is more to Iceberg, but I&rsquo;m simplifying because this helped me understand. :)</em></p>
<p>These were some of the original goals for Iceberg, so let&rsquo;s dive in and see how it works. Similar to Hudi, we&rsquo;ll create a simple Spark DataFrame and write that to S3 in Iceberg format.</p>
<p>I should note that much of Iceberg is focused around Spark SQL, so I will switch to that below for certain operations.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Create a DataFrame</span>
</span></span><span style="display:flex;"><span>inputDF <span style="color:#f92672">=</span> spark<span style="color:#f92672">.</span>createDataFrame(
</span></span><span style="display:flex;"><span>    [
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;100&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01T13:51:39.340396Z&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;101&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01T12:14:58.597216Z&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;102&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01T13:51:40.417052Z&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;103&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01T13:51:40.519832Z&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;104&#34;</span>, <span style="color:#e6db74">&#34;2015-01-02&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01T12:15:00.512679Z&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;105&#34;</span>, <span style="color:#e6db74">&#34;2015-01-02&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01T13:51:42.248818Z&#34;</span>),
</span></span><span style="display:flex;"><span>    ],
</span></span><span style="display:flex;"><span>    [<span style="color:#e6db74">&#34;id&#34;</span>, <span style="color:#e6db74">&#34;creation_date&#34;</span>, <span style="color:#e6db74">&#34;last_update_time&#34;</span>],
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Write a DataFrame as an Iceberg dataset</span>
</span></span><span style="display:flex;"><span>inputDF<span style="color:#f92672">.</span>write<span style="color:#f92672">.</span>format(<span style="color:#e6db74">&#34;iceberg&#34;</span>)<span style="color:#f92672">.</span>mode(<span style="color:#e6db74">&#34;overwrite&#34;</span>)<span style="color:#f92672">.</span>partitionBy(<span style="color:#e6db74">&#34;creation_date&#34;</span>)<span style="color:#f92672">.</span>option(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;path&#34;</span>, <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;s3://</span><span style="color:#e6db74">{</span>S3_BUCKET_NAME<span style="color:#e6db74">}</span><span style="color:#e6db74">/tmp/iceberg/&#34;</span>
</span></span><span style="display:flex;"><span>)<span style="color:#f92672">.</span>saveAsTable(ICEBERG_TABLE_NAME)
</span></span></code></pre></div><p>There are two main differences here - there is not as much &ldquo;configuration&rdquo; as we had to do with Hudi and we also explicitly use <code>saveAsTable</code>. With Iceberg, much of the metadata is stored in a data catalog so creating the table is necessary. Let&rsquo;s see what happened on S3.</p>
<ol>
<li>First, we have a <code>metadata.json</code> file</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>2022-01-28 06:03:50       2457 tmp/iceberg/metadata/00000-bb1d38a9-af77-42c4-a7b7-69416fe36d9c.metadata.json
</span></span></code></pre></div><ol start="2">
<li>Then a snapshot <strong>manifest list</strong> file</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>2022-01-28 06:03:50       3785 tmp/iceberg/metadata/snap-7934053180928033536-1-e79c79ba-c7f0-45ad-8f2e-fd1bc349db55.avro
</span></span></code></pre></div><ol start="3">
<li>And a manifest file</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>2022-01-28 06:03:50       6244 tmp/iceberg/metadata/e79c79ba-c7f0-45ad-8f2e-fd1bc349db55-m0.avro
</span></span></code></pre></div><ol start="4">
<li>And finally, we&rsquo;ve got our Parquet data files</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>2022-01-28 06:03:49       1197 tmp/iceberg/data/creation_date=2015-01-01/00000-4-fa9a18fd-abc4-4e04-91b4-e2ac4c9531be-00001.parquet
</span></span><span style="display:flex;"><span>2022-01-28 06:03:49       1171 tmp/iceberg/data/creation_date=2015-01-01/00001-5-eab30115-a1d6-4918-abb4-a198ac12b262-00001.parquet
</span></span><span style="display:flex;"><span>2022-01-28 06:03:50       1182 tmp/iceberg/data/creation_date=2015-01-02/00001-5-eab30115-a1d6-4918-abb4-a198ac12b262-00002.parquet
</span></span></code></pre></div><p>There are a lot of moving pieces here, but the image from the Iceberg spec illustrates it quite well.</p>
<p><img alt="Iceberg Metadata Diagram" loading="lazy" src="/posts/modern-data-lake-storage-layers/iceberg-metadata.png"></p>
<p>Similar to Hudi, our data is written to Parquet files in each partition, although Hive-style partitioning is used by default. Hudi can also do this by setting the <a href="https://hudi.apache.org/docs/configurations/#hoodiedatasourcewritehive_style_partitioning"><code>hoodie.datasource.write.hive_style_partitioning </code></a> parameter.</p>
<p>Different from Hudi, though, is the default usage of the data catalog to identify the current metadata file to use. That metadata file contains references to a list of manifest files to use to determine which data files compose the dataset for that particular version, also known as snapshots. As of <a href="https://hudi.apache.org/releases/release-0.7.0/#metadata-table">Hudi 0.7.0</a>, it also supports a metadata table to reduce the performance impact of file listings. The snapshot data also has quite a bit of additional information. Let&rsquo;s update our dataset then take a look at S3 again and the snapshot portion of the metadata file.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>spark<span style="color:#f92672">.</span>sql(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;UPDATE </span><span style="color:#e6db74">{</span>ICEBERG_TABLE_NAME<span style="color:#e6db74">}</span><span style="color:#e6db74"> SET creation_date = &#39;2022-01-11&#39; WHERE id = 100&#34;</span>)
</span></span></code></pre></div><p>We can see that we have:</p>
<ul>
<li>2 new .parquet data files</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>2022-01-28 06:07:07       1180 tmp/iceberg/data/creation_date=2015-01-01/00000-16-033354bd-7b02-44f4-95e2-7045e10706fc-00001.parquet
</span></span><span style="display:flex;"><span>2022-01-28 06:07:08       1171 tmp/iceberg/data/creation_date=2022-01-11/00000-16-033354bd-7b02-44f4-95e2-7045e10706fc-00002.parquet
</span></span></code></pre></div><p>As well as:</p>
<ul>
<li>1 new metadata.json file</li>
<li>2 new .avro metadata listings</li>
<li>1 new snap-*.avro snapshot file</li>
</ul>
<p>Let&rsquo;s look at the snapshot portion of the <code>metadata.json</code> file.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span><span style="color:#e6db74">&#34;snapshots&#34;</span><span style="color:#960050;background-color:#1e0010">:</span> [
</span></span><span style="display:flex;"><span>    {
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;manifest-list&#34;</span>: <span style="color:#e6db74">&#34;s3://&lt;BUCKET&gt;/tmp/iceberg/metadata/snap-7934053180928033536-1-e79c79ba-c7f0-45ad-8f2e-fd1bc349db55.avro&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;schema-id&#34;</span>: <span style="color:#ae81ff">0</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;snapshot-id&#34;</span>: <span style="color:#ae81ff">7934053180928033536</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;summary&#34;</span>: {
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;added-data-files&#34;</span>: <span style="color:#e6db74">&#34;3&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;added-files-size&#34;</span>: <span style="color:#e6db74">&#34;3550&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;added-records&#34;</span>: <span style="color:#e6db74">&#34;6&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;changed-partition-count&#34;</span>: <span style="color:#e6db74">&#34;2&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;operation&#34;</span>: <span style="color:#e6db74">&#34;append&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;spark.app.id&#34;</span>: <span style="color:#e6db74">&#34;application_1643153254969_0029&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;total-data-files&#34;</span>: <span style="color:#e6db74">&#34;3&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;total-delete-files&#34;</span>: <span style="color:#e6db74">&#34;0&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;total-equality-deletes&#34;</span>: <span style="color:#e6db74">&#34;0&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;total-files-size&#34;</span>: <span style="color:#e6db74">&#34;3550&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;total-position-deletes&#34;</span>: <span style="color:#e6db74">&#34;0&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;total-records&#34;</span>: <span style="color:#e6db74">&#34;6&#34;</span>
</span></span><span style="display:flex;"><span>        },
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;timestamp-ms&#34;</span>: <span style="color:#ae81ff">1643349829278</span>
</span></span><span style="display:flex;"><span>    },
</span></span><span style="display:flex;"><span>    {
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;manifest-list&#34;</span>: <span style="color:#e6db74">&#34;s3://&lt;BUCKET&gt;/tmp/iceberg/metadata/snap-5441092870212826638-1-605de48f-8ccf-450c-935e-bbd4194ee8cc.avro&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;parent-snapshot-id&#34;</span>: <span style="color:#ae81ff">7934053180928033536</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;schema-id&#34;</span>: <span style="color:#ae81ff">0</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;snapshot-id&#34;</span>: <span style="color:#ae81ff">5441092870212826638</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;summary&#34;</span>: {
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;added-data-files&#34;</span>: <span style="color:#e6db74">&#34;2&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;added-files-size&#34;</span>: <span style="color:#e6db74">&#34;2351&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;added-records&#34;</span>: <span style="color:#e6db74">&#34;3&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;changed-partition-count&#34;</span>: <span style="color:#e6db74">&#34;2&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;deleted-data-files&#34;</span>: <span style="color:#e6db74">&#34;1&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;deleted-records&#34;</span>: <span style="color:#e6db74">&#34;3&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;operation&#34;</span>: <span style="color:#e6db74">&#34;overwrite&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;removed-files-size&#34;</span>: <span style="color:#e6db74">&#34;1197&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;spark.app.id&#34;</span>: <span style="color:#e6db74">&#34;application_1643153254969_0029&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;total-data-files&#34;</span>: <span style="color:#e6db74">&#34;4&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;total-delete-files&#34;</span>: <span style="color:#e6db74">&#34;0&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;total-equality-deletes&#34;</span>: <span style="color:#e6db74">&#34;0&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;total-files-size&#34;</span>: <span style="color:#e6db74">&#34;4704&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;total-position-deletes&#34;</span>: <span style="color:#e6db74">&#34;0&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;total-records&#34;</span>: <span style="color:#e6db74">&#34;6&#34;</span>
</span></span><span style="display:flex;"><span>        },
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;timestamp-ms&#34;</span>: <span style="color:#ae81ff">1643350027635</span>
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>]
</span></span></code></pre></div><p>This is pretty amazing - we see how many files <strong>and records</strong> were added or deleted, what the file sizes were, and even what the Spark <code>app_id</code> was! ü§Ø Some of this data is in the <code>manifest-list</code> files as well, but you can begin to see <em>just</em> how much you could potentially optimize your queries using this data.</p>
<h2 id="delta-lake">Delta Lake</h2>
<blockquote>
<p>üìπ <a href="https://www.youtube.com/watch?v=fryfx0Zg7KA&amp;t=1814s">Intro to Delta Lake video</a></p>
</blockquote>
<p>Delta Lake was also introduced by Databricks as a way to address many of the challenges of Data Lakes. Similar to Hudi and Iceberg its goals include unifying batch and stream processing, ACID transactions, and scalable metadata handling among others.</p>
<p>Again, we&rsquo;ll create a simple Spark DataFrame and write it to S3 in Delta format.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Create a DataFrame</span>
</span></span><span style="display:flex;"><span>inputDF <span style="color:#f92672">=</span> spark<span style="color:#f92672">.</span>createDataFrame(
</span></span><span style="display:flex;"><span>    [
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;100&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01T13:51:39.340396Z&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;101&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01T12:14:58.597216Z&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;102&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01T13:51:40.417052Z&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;103&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01T13:51:40.519832Z&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;104&#34;</span>, <span style="color:#e6db74">&#34;2015-01-02&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01T12:15:00.512679Z&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;105&#34;</span>, <span style="color:#e6db74">&#34;2015-01-02&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01T13:51:42.248818Z&#34;</span>),
</span></span><span style="display:flex;"><span>    ],
</span></span><span style="display:flex;"><span>    [<span style="color:#e6db74">&#34;id&#34;</span>, <span style="color:#e6db74">&#34;creation_date&#34;</span>, <span style="color:#e6db74">&#34;last_update_time&#34;</span>],
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Write a DataFrame as a Delta dataset</span>
</span></span><span style="display:flex;"><span>inputDF<span style="color:#f92672">.</span>write<span style="color:#f92672">.</span>format(<span style="color:#e6db74">&#34;delta&#34;</span>)<span style="color:#f92672">.</span>mode(<span style="color:#e6db74">&#34;overwrite&#34;</span>)<span style="color:#f92672">.</span>option(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;overwriteSchema&#34;</span>, <span style="color:#e6db74">&#34;true&#34;</span>
</span></span><span style="display:flex;"><span>)<span style="color:#f92672">.</span>partitionBy(<span style="color:#e6db74">&#34;creation_date&#34;</span>)<span style="color:#f92672">.</span>save(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;s3://</span><span style="color:#e6db74">{</span>S3_BUCKET_NAME<span style="color:#e6db74">}</span><span style="color:#e6db74">/tmp/delta/&#34;</span>)
</span></span></code></pre></div><p>On S3, we now see the following files:</p>
<ol>
<li>a <code>00000000000000000000.json</code> file</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>2022-01-24 22:57:54       2120 tmp/delta/_delta_log/00000000000000000000.json
</span></span></code></pre></div><ol start="2">
<li>Several <code>.snappy.parquet</code> files</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>2022-01-24 22:57:52        875 tmp/delta/creation_date=2015-01-01/part-00005-2e09dbe4-469e-40dc-9b36-833480f6d375.c000.snappy.parquet
</span></span><span style="display:flex;"><span>2022-01-24 22:57:52        875 tmp/delta/creation_date=2015-01-01/part-00010-848c69e1-71fb-4f8f-a19a-dd74e0ef1b8a.c000.snappy.parquet
</span></span><span style="display:flex;"><span>2022-01-24 22:57:53        875 tmp/delta/creation_date=2015-01-01/part-00015-937d1837-0f03-4306-9b4e-4366207e688d.c000.snappy.parquet
</span></span><span style="display:flex;"><span>2022-01-24 22:57:54        875 tmp/delta/creation_date=2015-01-01/part-00021-978a808e-4c36-4646-b7b1-ef5a21e706d8.c000.snappy.parquet
</span></span><span style="display:flex;"><span>2022-01-24 22:57:54        875 tmp/delta/creation_date=2015-01-02/part-00026-538e1ac6-055e-4e72-9177-63daaaae1f98.c000.snappy.parquet
</span></span><span style="display:flex;"><span>2022-01-24 22:57:52        875 tmp/delta/creation_date=2015-01-02/part-00031-8a03451a-0297-4c43-b64d-56db25807d02.c000.snappy.parquet
</span></span></code></pre></div><p>OK, so what&rsquo;s in that <code>_delta_log</code> file? Similar to Iceberg, quite a bit of information about this initial write to S3 including the number of files written, the schema of the dataset, and even the individual <code>add</code> operations for each file.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;commitInfo&#34;</span>: {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;timestamp&#34;</span>: <span style="color:#ae81ff">1643065073634</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;operation&#34;</span>: <span style="color:#e6db74">&#34;WRITE&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;operationParameters&#34;</span>: {
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;mode&#34;</span>: <span style="color:#e6db74">&#34;Overwrite&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;partitionBy&#34;</span>: <span style="color:#e6db74">&#34;[\&#34;creation_date\&#34;]&#34;</span>
</span></span><span style="display:flex;"><span>    },
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;isBlindAppend&#34;</span>: <span style="color:#66d9ef">false</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;operationMetrics&#34;</span>: {
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;numFiles&#34;</span>: <span style="color:#e6db74">&#34;6&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;numOutputBytes&#34;</span>: <span style="color:#e6db74">&#34;5250&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;numOutputRows&#34;</span>: <span style="color:#e6db74">&#34;6&#34;</span>
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;protocol&#34;</span>: {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;minReaderVersion&#34;</span>: <span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;minWriterVersion&#34;</span>: <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;metaData&#34;</span>: {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;id&#34;</span>: <span style="color:#e6db74">&#34;a7f4b1d1-09f6-4475-894a-0eec90d1aab5&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;format&#34;</span>: {
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;provider&#34;</span>: <span style="color:#e6db74">&#34;parquet&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;options&#34;</span>: {}
</span></span><span style="display:flex;"><span>    },
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;schemaString&#34;</span>: <span style="color:#e6db74">&#34;{\&#34;type\&#34;:\&#34;struct\&#34;,\&#34;fields\&#34;:[{\&#34;name\&#34;:\&#34;id\&#34;,\&#34;type\&#34;:\&#34;string\&#34;,\&#34;nullable\&#34;:true,\&#34;metadata\&#34;:{}},{\&#34;name\&#34;:\&#34;creation_date\&#34;,\&#34;type\&#34;:\&#34;string\&#34;,\&#34;nullable\&#34;:true,\&#34;metadata\&#34;:{}},{\&#34;name\&#34;:\&#34;last_update_time\&#34;,\&#34;type\&#34;:\&#34;string\&#34;,\&#34;nullable\&#34;:true,\&#34;metadata\&#34;:{}}]}&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;partitionColumns&#34;</span>: [
</span></span><span style="display:flex;"><span>      <span style="color:#e6db74">&#34;creation_date&#34;</span>
</span></span><span style="display:flex;"><span>    ],
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;configuration&#34;</span>: {},
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;createdTime&#34;</span>: <span style="color:#ae81ff">1643065064066</span>
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;add&#34;</span>: {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;path&#34;</span>: <span style="color:#e6db74">&#34;creation_date=2015-01-01/part-00005-2e09dbe4-469e-40dc-9b36-833480f6d375.c000.snappy.parquet&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;partitionValues&#34;</span>: {
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;creation_date&#34;</span>: <span style="color:#e6db74">&#34;2015-01-01&#34;</span>
</span></span><span style="display:flex;"><span>    },
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;size&#34;</span>: <span style="color:#ae81ff">875</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;modificationTime&#34;</span>: <span style="color:#ae81ff">1643065072000</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;dataChange&#34;</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;add&#34;</span>: {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;path&#34;</span>: <span style="color:#e6db74">&#34;creation_date=2015-01-01/part-00010-848c69e1-71fb-4f8f-a19a-dd74e0ef1b8a.c000.snappy.parquet&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;partitionValues&#34;</span>: {
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;creation_date&#34;</span>: <span style="color:#e6db74">&#34;2015-01-01&#34;</span>
</span></span><span style="display:flex;"><span>    },
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;size&#34;</span>: <span style="color:#ae81ff">875</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;modificationTime&#34;</span>: <span style="color:#ae81ff">1643065072000</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;dataChange&#34;</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;add&#34;</span>: {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;path&#34;</span>: <span style="color:#e6db74">&#34;creation_date=2015-01-01/part-00015-937d1837-0f03-4306-9b4e-4366207e688d.c000.snappy.parquet&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;partitionValues&#34;</span>: {
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;creation_date&#34;</span>: <span style="color:#e6db74">&#34;2015-01-01&#34;</span>
</span></span><span style="display:flex;"><span>    },
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;size&#34;</span>: <span style="color:#ae81ff">875</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;modificationTime&#34;</span>: <span style="color:#ae81ff">1643065073000</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;dataChange&#34;</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;add&#34;</span>: {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;path&#34;</span>: <span style="color:#e6db74">&#34;creation_date=2015-01-01/part-00021-978a808e-4c36-4646-b7b1-ef5a21e706d8.c000.snappy.parquet&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;partitionValues&#34;</span>: {
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;creation_date&#34;</span>: <span style="color:#e6db74">&#34;2015-01-01&#34;</span>
</span></span><span style="display:flex;"><span>    },
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;size&#34;</span>: <span style="color:#ae81ff">875</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;modificationTime&#34;</span>: <span style="color:#ae81ff">1643065074000</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;dataChange&#34;</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;add&#34;</span>: {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;path&#34;</span>: <span style="color:#e6db74">&#34;creation_date=2015-01-02/part-00026-538e1ac6-055e-4e72-9177-63daaaae1f98.c000.snappy.parquet&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;partitionValues&#34;</span>: {
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;creation_date&#34;</span>: <span style="color:#e6db74">&#34;2015-01-02&#34;</span>
</span></span><span style="display:flex;"><span>    },
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;size&#34;</span>: <span style="color:#ae81ff">875</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;modificationTime&#34;</span>: <span style="color:#ae81ff">1643065074000</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;dataChange&#34;</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;add&#34;</span>: {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;path&#34;</span>: <span style="color:#e6db74">&#34;creation_date=2015-01-02/part-00031-8a03451a-0297-4c43-b64d-56db25807d02.c000.snappy.parquet&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;partitionValues&#34;</span>: {
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;creation_date&#34;</span>: <span style="color:#e6db74">&#34;2015-01-02&#34;</span>
</span></span><span style="display:flex;"><span>    },
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;size&#34;</span>: <span style="color:#ae81ff">875</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;modificationTime&#34;</span>: <span style="color:#ae81ff">1643065072000</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;dataChange&#34;</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>Alright, let&rsquo;s go ahead and update one of our rows. Delta Lake provides a merge operation that we can use. We&rsquo;ll use the syntax <a href="https://docs.delta.io/latest/quick-start.html#update-table-data">from the docs</a> that&rsquo;s slightly different from native Spark as it creates a DeltaTable object.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql.functions <span style="color:#f92672">import</span> lit
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create a new DataFrame from the first row of inputDF with a different creation_date value</span>
</span></span><span style="display:flex;"><span>updateDF <span style="color:#f92672">=</span> inputDF<span style="color:#f92672">.</span>where(<span style="color:#e6db74">&#34;id = 100&#34;</span>)<span style="color:#f92672">.</span>withColumn(<span style="color:#e6db74">&#34;creation_date&#34;</span>, lit(<span style="color:#e6db74">&#34;2022-01-11&#34;</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> delta.tables <span style="color:#f92672">import</span> <span style="color:#f92672">*</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql.functions <span style="color:#f92672">import</span> <span style="color:#f92672">*</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>deltaTable <span style="color:#f92672">=</span> DeltaTable<span style="color:#f92672">.</span>forPath(spark, <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;s3://</span><span style="color:#e6db74">{</span>S3_BUCKET_NAME<span style="color:#e6db74">}</span><span style="color:#e6db74">/tmp/delta/&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>deltaTable<span style="color:#f92672">.</span>alias(<span style="color:#e6db74">&#34;oldData&#34;</span>) \
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">.</span>merge(
</span></span><span style="display:flex;"><span>    updateDF<span style="color:#f92672">.</span>alias(<span style="color:#e6db74">&#34;newData&#34;</span>),
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;oldData.id = newData.id&#34;</span>) \
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">.</span>whenMatchedUpdate(set <span style="color:#f92672">=</span> { <span style="color:#e6db74">&#34;creation_date&#34;</span>: col(<span style="color:#e6db74">&#34;newData.creation_date&#34;</span>) }) \
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">.</span>execute()
</span></span></code></pre></div><p>Interestingly, now when we look at S3 we see 1 new <code>json</code> file and only 1 new <code>parquet</code> file (Remember Hudi and Iceberg both had 2 new <code>parquet</code> files).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>2022-01-24 23:05:46       1018 tmp/delta/_delta_log/00000000000000000001.json
</span></span><span style="display:flex;"><span>2022-01-24 23:05:46        875 tmp/delta/creation_date=2022-01-11/part-00000-3f3fd83a-b876-4b6f-8f64-d8a4189392ae.c000.snappy.parquet
</span></span></code></pre></div><p>If we look at that new JSON file we see something really interesting:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;commitInfo&#34;</span>: {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;timestamp&#34;</span>: <span style="color:#ae81ff">1643065545396</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;operation&#34;</span>: <span style="color:#e6db74">&#34;MERGE&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;operationParameters&#34;</span>: {
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;predicate&#34;</span>: <span style="color:#e6db74">&#34;(oldData.`id` = newData.`id`)&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;matchedPredicates&#34;</span>: <span style="color:#e6db74">&#34;[{\&#34;actionType\&#34;:\&#34;update\&#34;}]&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;notMatchedPredicates&#34;</span>: <span style="color:#e6db74">&#34;[]&#34;</span>
</span></span><span style="display:flex;"><span>    },
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;readVersion&#34;</span>: <span style="color:#ae81ff">0</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;isBlindAppend&#34;</span>: <span style="color:#66d9ef">false</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;operationMetrics&#34;</span>: {
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;numTargetRowsCopied&#34;</span>: <span style="color:#e6db74">&#34;0&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;numTargetRowsDeleted&#34;</span>: <span style="color:#e6db74">&#34;0&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;numTargetFilesAdded&#34;</span>: <span style="color:#e6db74">&#34;1&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;executionTimeMs&#34;</span>: <span style="color:#e6db74">&#34;4705&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;numTargetRowsInserted&#34;</span>: <span style="color:#e6db74">&#34;0&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;scanTimeMs&#34;</span>: <span style="color:#e6db74">&#34;3399&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;numTargetRowsUpdated&#34;</span>: <span style="color:#e6db74">&#34;1&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;numOutputRows&#34;</span>: <span style="color:#e6db74">&#34;1&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;numSourceRows&#34;</span>: <span style="color:#e6db74">&#34;1&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;numTargetFilesRemoved&#34;</span>: <span style="color:#e6db74">&#34;1&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;rewriteTimeMs&#34;</span>: <span style="color:#e6db74">&#34;1265&#34;</span>
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;remove&#34;</span>: {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;path&#34;</span>: <span style="color:#e6db74">&#34;creation_date=2015-01-01/part-00005-2e09dbe4-469e-40dc-9b36-833480f6d375.c000.snappy.parquet&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;deletionTimestamp&#34;</span>: <span style="color:#ae81ff">1643065545378</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;dataChange&#34;</span>: <span style="color:#66d9ef">true</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;extendedFileMetadata&#34;</span>: <span style="color:#66d9ef">true</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;partitionValues&#34;</span>: {
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;creation_date&#34;</span>: <span style="color:#e6db74">&#34;2015-01-01&#34;</span>
</span></span><span style="display:flex;"><span>    },
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;size&#34;</span>: <span style="color:#ae81ff">875</span>
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;add&#34;</span>: {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;path&#34;</span>: <span style="color:#e6db74">&#34;creation_date=2022-01-11/part-00000-3f3fd83a-b876-4b6f-8f64-d8a4189392ae.c000.snappy.parquet&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;partitionValues&#34;</span>: {
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;creation_date&#34;</span>: <span style="color:#e6db74">&#34;2022-01-11&#34;</span>
</span></span><span style="display:flex;"><span>    },
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;size&#34;</span>: <span style="color:#ae81ff">875</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;modificationTime&#34;</span>: <span style="color:#ae81ff">1643065546000</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;dataChange&#34;</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>In addition to the <code>operationMetrics</code> that gives us insight into how the data changed on &ldquo;disk&rdquo;, we also now see both a <code>remove</code> and <code>add</code> operation. In Delta Lake (and I&rsquo;m not quite sure why this happened yet&hellip;), each row was written to an individual <code>.parquet</code> file! So for this second version of the data, the fact that that row was updated simply lives in the metadata because it was the only row stored in that Parquet file. I&rsquo;m guessing this is simply because my dataset is so small the default number of partitions in Spark/Delta Lake resulted in this write configuration.</p>
<h2 id="snapshots">Snapshots</h2>
<p>So now we&rsquo;ve got a good idea of the semantics of each of these storage layers. Let&rsquo;s take one more look at an important component of all of them and that&rsquo;s snapshots!</p>
<h3 id="hudi">Hudi</h3>
<p>Hudi has a concept of &ldquo;point-in-time&rdquo; queries where you provide it a range of two commit timestamps and it will show you what the data looked like at that point in time.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Query data from the first version of the table</span>
</span></span><span style="display:flex;"><span>readOptions <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#39;hoodie.datasource.query.type&#39;</span>: <span style="color:#e6db74">&#39;incremental&#39;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#39;hoodie.datasource.read.begin.instanttime&#39;</span>: <span style="color:#e6db74">&#39;0&#39;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#39;hoodie.datasource.read.end.instanttime&#39;</span>: <span style="color:#e6db74">&#39;20220114003341&#39;</span>,
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>incQueryDF <span style="color:#f92672">=</span> spark<span style="color:#f92672">.</span>read \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>format(<span style="color:#e6db74">&#39;org.apache.hudi&#39;</span>) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>options(<span style="color:#f92672">**</span>readOptions) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>load(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;s3://</span><span style="color:#e6db74">{</span>S3_BUCKET_NAME<span style="color:#e6db74">}</span><span style="color:#e6db74">/tmp/hudi&#34;</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>incQueryDF<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h3 id="iceberg">Iceberg</h3>
<p>Iceberg supports a similar mechanism called time travel and you can use either a <code>snapshot-id</code> or <code>as-of-timestamp</code> similar to Hudi.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># time travel to 2022-01-27 22:04:00 -0800</span>
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> spark<span style="color:#f92672">.</span>read \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>option(<span style="color:#e6db74">&#34;as-of-timestamp&#34;</span>, <span style="color:#e6db74">&#34;1643349840000&#34;</span>) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>format(<span style="color:#e6db74">&#34;iceberg&#34;</span>) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>load(ICEBERG_TABLE_NAME)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>df<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h3 id="delta-lake-1">Delta Lake</h3>
<p>And, of course, Delta Lake supports this as well using either <a href="https://docs.delta.io/latest/delta-batch.html#-deltatimetravel">Spark SQL or DataFrames</a>. And similar to Iceberg you can use <code>versionAsOf</code> or <code>timestampAsOf</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># time travel to 2022-01-24 23:00</span>
</span></span><span style="display:flex;"><span>df1 <span style="color:#f92672">=</span> (
</span></span><span style="display:flex;"><span>    spark<span style="color:#f92672">.</span>read<span style="color:#f92672">.</span>format(<span style="color:#e6db74">&#34;delta&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>option(<span style="color:#e6db74">&#34;timestampAsOf&#34;</span>, <span style="color:#e6db74">&#34;2022-01-24 23:00&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>load(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;s3://</span><span style="color:#e6db74">{</span>S3_BUCKET_NAME<span style="color:#e6db74">}</span><span style="color:#e6db74">/tmp/delta/&#34;</span>)
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h2 id="deletes">Deletes</h2>
<p>I bet you&rsquo;re surprised I haven&rsquo;t mentioned deletes or GDPR yet. Don&rsquo;t worry&hellip;I will. üòÄ But first I just wanted to understand exactly how these different systems work.</p>
<h2 id="wrapup">Wrapup</h2>
<p>In this post, we reviewed the basics of Apache Hudi, Apache Iceberg, and Delta Lake - modern data lake storage layers. All these frameworks enable a set of functionality that optimize working with data in cloud-based object stores, albeit with slightly different approaches.</p>
]]></content:encoded></item><item><title>SSH to EC2 Instances with Session Manager</title><link>https://dacort.xyz/posts/ssh-to-ec2-instances-with-session-manager/</link><pubDate>Wed, 29 Sep 2021 09:49:03 -0700</pubDate><guid>https://dacort.xyz/posts/ssh-to-ec2-instances-with-session-manager/</guid><description>&lt;p&gt;I&amp;rsquo;m kind of an old-school sys admin (aka, managed NT4 in the 90&amp;rsquo;s) so I&amp;rsquo;m really used to SSH&amp;rsquo;ing into hosts. More often than not, however, I&amp;rsquo;m working with AWS EC2 instances in a private subnet.&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;re not familiar with it &lt;a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html"&gt;AWS Systems Manager Session Manager&lt;/a&gt; is a pretty sweet feature that allows you to connect remotely to EC2 instances with the AWS CLI, without needing to open up ports for SSH or utilize a bastion host.&lt;/p&gt;</description><content:encoded><![CDATA[<p>I&rsquo;m kind of an old-school sys admin (aka, managed NT4 in the 90&rsquo;s) so I&rsquo;m really used to SSH&rsquo;ing into hosts. More often than not, however, I&rsquo;m working with AWS EC2 instances in a private subnet.</p>
<p>If you&rsquo;re not familiar with it <a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html">AWS Systems Manager Session Manager</a> is a pretty sweet feature that allows you to connect remotely to EC2 instances with the AWS CLI, without needing to open up ports for SSH or utilize a bastion host.</p>
<p>I&rsquo;ve been using it in my browser occasionally, which is pretty handy, but I wanted to use it from my terminal. It required a couple steps to get working.</p>
<h2 id="set-up-session-manager-with-aws-cli">Set up Session Manager with AWS CLI</h2>
<ol>
<li>Install the <a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-install-plugin.html">Session Manager plugin</a> for the AWS CLI</li>
</ol>
<p>I&rsquo;m on a mac, so I just installed the plugin with the signed installer</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>curl <span style="color:#e6db74">&#34;https://s3.amazonaws.com/session-manager-downloads/plugin/latest/mac/session-manager-plugin.pkg&#34;</span> -o ~/Downloads/session-manager-plugin.pkg
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>sudo installer -pkg ~/Downloads/session-manager-plugin.pkg -target /
</span></span><span style="display:flex;"><span>sudo ln -s /usr/local/sessionmanagerplugin/bin/session-manager-plugin /usr/local/bin/session-manager-plugin
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>‚ûú session-manager-plugin
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>The Session Manager plugin was installed successfully. Use the AWS CLI to start a session.
</span></span></code></pre></div><p>Sweet, good to go there!</p>
<ol start="2">
<li>Now use the AWS CLI to connect to an instance!</li>
</ol>
<p><em>You may need to specify the region your instance is in</em></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>aws ssm start-session --target i-abcdefgh123456789 --region us-west-2
</span></span></code></pre></div><p>Awesome! You&rsquo;re good to go!</p>
<p><img alt="Session Output" loading="lazy" src="/posts/ssh-to-ec2-instances-with-session-manager/session-output.png"></p>
<h2 id="enable-logging">Enable Logging</h2>
<p>The other nice thing if your memory is as bad as mine (or you want auditing, which is a more legitimate reason), you can also enable logging of your sessions to S3 or CloudWatch.</p>
<p>This is what the default config looks like:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>aws ssm get-document <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    --region us-west-2 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    --name <span style="color:#e6db74">&#34;SSM-SessionManagerRunShell&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    --document-version <span style="color:#e6db74">&#34;\$LATEST&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    | jq <span style="color:#e6db74">&#39;.Content | fromjson&#39;</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;schemaVersion&#34;</span>: <span style="color:#e6db74">&#34;1.0&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;description&#34;</span>: <span style="color:#e6db74">&#34;Document to hold regional settings for Session Manager&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;sessionType&#34;</span>: <span style="color:#e6db74">&#34;Standard_Stream&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;inputs&#34;</span>: {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;s3BucketName&#34;</span>: <span style="color:#e6db74">&#34;&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;s3KeyPrefix&#34;</span>: <span style="color:#e6db74">&#34;&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;s3EncryptionEnabled&#34;</span>: <span style="color:#66d9ef">true</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;cloudWatchLogGroupName&#34;</span>: <span style="color:#e6db74">&#34;&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;cloudWatchEncryptionEnabled&#34;</span>: <span style="color:#66d9ef">true</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;cloudWatchStreamingEnabled&#34;</span>: <span style="color:#66d9ef">true</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;idleSessionTimeout&#34;</span>: <span style="color:#e6db74">&#34;20&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;kmsKeyId&#34;</span>: <span style="color:#e6db74">&#34;&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;runAsEnabled&#34;</span>: <span style="color:#66d9ef">false</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;runAsDefaultUser&#34;</span>: <span style="color:#e6db74">&#34;&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;shellProfile&#34;</span>: {
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;windows&#34;</span>: <span style="color:#e6db74">&#34;&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;linux&#34;</span>: <span style="color:#e6db74">&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>So we&rsquo;ll just update that to add in the S3 configuration.</p>
<ol>
<li>Update Session Manager preferences</li>
</ol>
<p><em>Note that I <strong>do not</strong> enable encryption here.</em> This setting needs to match your bucket setting and you need to make sure your VPC has the proper endpoints and access to write to S3. Check <a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-troubleshooting.html#session-manager-troubleshooting-start-blank-screen">troubleshooting</a> if you get a blank screen when trying to start a session.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>BUCKET<span style="color:#f92672">=</span>&lt;BUCKET_NAME&gt;
</span></span><span style="display:flex;"><span>PREFIX<span style="color:#f92672">=</span>logs/session_manager/
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>aws ssm update-document <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    --region us-west-2 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    --name <span style="color:#e6db74">&#34;SSM-SessionManagerRunShell&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    --document-version <span style="color:#e6db74">&#34;\$LATEST&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    --content <span style="color:#e6db74">&#39;{
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">  &#34;schemaVersion&#34;: &#34;1.0&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">  &#34;description&#34;: &#34;Document to hold regional settings for Session Manager&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">  &#34;sessionType&#34;: &#34;Standard_Stream&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">  &#34;inputs&#34;: {
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;s3BucketName&#34;: &#34;&#39;</span><span style="color:#e6db74">${</span>BUCKET<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;s3KeyPrefix&#34;: &#34;&#39;</span><span style="color:#e6db74">${</span>PREFIX<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;s3EncryptionEnabled&#34;: false,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;cloudWatchLogGroupName&#34;: &#34;&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;cloudWatchEncryptionEnabled&#34;: true,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;cloudWatchStreamingEnabled&#34;: true,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;idleSessionTimeout&#34;: &#34;20&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;kmsKeyId&#34;: &#34;&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;runAsEnabled&#34;: false,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;runAsDefaultUser&#34;: &#34;&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;shellProfile&#34;: {
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">      &#34;windows&#34;: &#34;&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">      &#34;linux&#34;: &#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    }
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">  }
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">}&#39;</span>
</span></span></code></pre></div><ol start="2">
<li>Create another session!</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>aws ssm start-session --target i-abcdefgh123456789 --region us-west-2
</span></span></code></pre></div><p>Once you&rsquo;re done with your session and exit, you should have a log file in your S3 bucket.</p>
<ol start="3">
<li>View logs</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>aws s3 ls s3://&lt;BUCKET_NAME&gt;/logs/session_manager/
</span></span></code></pre></div><pre tabindex="0"><code>2021-09-29 10:21:24       4177 your-aws-username-abcdefgh123456789.log
</code></pre><p>And that log file will have the full contents of your session.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>aws s3 cp s3://&lt;BUCKET_NAME&gt;/logs/session_manager/your-aws-username-abcdefgh123456789.log -
</span></span></code></pre></div><p><img alt="SSM Log Output" loading="lazy" src="/posts/ssh-to-ec2-instances-with-session-manager/log-output.png"></p>
<p>And yes, the <em>FULL CONTENTS</em>. So if you enter a password or sensitive info, you should follow the steps <a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-logging.html">here</a>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>stty -echo; read passwd; stty echo;
</span></span></code></pre></div>]]></content:encoded></item><item><title>Updating Partition Values With Apache Hudi</title><link>https://dacort.xyz/posts/updating-partition-values-with-apache-hudi/</link><pubDate>Thu, 23 Sep 2021 11:51:57 -0700</pubDate><guid>https://dacort.xyz/posts/updating-partition-values-with-apache-hudi/</guid><description>&lt;p&gt;If you&amp;rsquo;re not familiar with &lt;a href="https://hudi.apache.org/"&gt;Apache Hudi&lt;/a&gt;, it&amp;rsquo;s a pretty awesome piece of software that brings transactions and record-level updates/deletes to data lakes.&lt;/p&gt;
&lt;p&gt;More specifically, if you&amp;rsquo;re doing Analytics with S3, Hudi provides a way for you to &lt;em&gt;consistently&lt;/em&gt; update records in your data lake, which historically has been pretty challenging. It can also optimize file sizes, allow for rollbacks, and makes &lt;a href="https://aws.amazon.com/blogs/big-data/new-features-from-apache-hudi-available-in-amazon-emr/"&gt;streaming CDC data impressively easy&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="updating-partition-values"&gt;Updating Partition Values&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;m learning more about Hudi and was following this &lt;a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hudi-work-with-dataset.html"&gt;EMR guide to working with a Hudi dataset&lt;/a&gt;, but the &amp;ldquo;Upsert&amp;rdquo; operation didn&amp;rsquo;t quite work as I expected. Instead of overwriting the desired record, it added a second one with the same ID. ü§î&lt;/p&gt;</description><content:encoded><![CDATA[<p>If you&rsquo;re not familiar with <a href="https://hudi.apache.org/">Apache Hudi</a>, it&rsquo;s a pretty awesome piece of software that brings transactions and record-level updates/deletes to data lakes.</p>
<p>More specifically, if you&rsquo;re doing Analytics with S3, Hudi provides a way for you to <em>consistently</em> update records in your data lake, which historically has been pretty challenging. It can also optimize file sizes, allow for rollbacks, and makes <a href="https://aws.amazon.com/blogs/big-data/new-features-from-apache-hudi-available-in-amazon-emr/">streaming CDC data impressively easy</a>.</p>
<h2 id="updating-partition-values">Updating Partition Values</h2>
<p>I&rsquo;m learning more about Hudi and was following this <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hudi-work-with-dataset.html">EMR guide to working with a Hudi dataset</a>, but the &ldquo;Upsert&rdquo; operation didn&rsquo;t quite work as I expected. Instead of overwriting the desired record, it added a second one with the same ID. ü§î</p>
<p>After some furious searching, I finally came across this post about <a href="https://hudi.apache.org/blog/2020/11/11/hudi-indexing-mechanisms/">employing the right indexes in Apache Hudi</a>. Specifically, this line caught my attention:</p>
<blockquote>
<p><strong>Global indexes enforce uniqueness of keys across all partitions of a table i.e guarantees that exactly one record exists in the table for a given record key.</strong></p>
</blockquote>
<p>Ah-ha! In the example, we&rsquo;re updating a partition value. <em>BY DEFAULT</em>, the <code>hoodie.index.type</code> is <code>BLOOM</code>. I tried changing it to <code>GLOBAL_BLOOM</code>, and when updating the record, it wrote it into the old partition. It turns out that there is <em>also</em> a <code>hoodie.bloom.index.update.partition.path</code> setting that will also update the partition path. This defaults to <code>true</code> in Hudi v0.9.0, but I&rsquo;m using v0.8.0 where it defaults to <code>false</code>.</p>
<p><em>Note that there is a performance/storage impact to enabling global indexes</em></p>
<p>So flipping that, I got the expected behavior. Using the example from the EMR docs, my code now looks like this:</p>
<h3 id="writing-initial-dataset">Writing initial dataset</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Create a DataFrame</span>
</span></span><span style="display:flex;"><span>inputDF <span style="color:#f92672">=</span> spark<span style="color:#f92672">.</span>createDataFrame(
</span></span><span style="display:flex;"><span>    [
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;100&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01T13:51:39.340396Z&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;101&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01T12:14:58.597216Z&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;102&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01T13:51:40.417052Z&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;103&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01T13:51:40.519832Z&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;104&#34;</span>, <span style="color:#e6db74">&#34;2015-01-02&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01T12:15:00.512679Z&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;105&#34;</span>, <span style="color:#e6db74">&#34;2015-01-02&#34;</span>, <span style="color:#e6db74">&#34;2015-01-01T13:51:42.248818Z&#34;</span>),
</span></span><span style="display:flex;"><span>    ],
</span></span><span style="display:flex;"><span>    [<span style="color:#e6db74">&#34;id&#34;</span>, <span style="color:#e6db74">&#34;creation_date&#34;</span>, <span style="color:#e6db74">&#34;last_update_time&#34;</span>],
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Specify common DataSourceWriteOptions in the single hudiOptions variable</span>
</span></span><span style="display:flex;"><span>hudiOptions <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;hoodie.table.name&#34;</span>: <span style="color:#e6db74">&#34;my_hudi_table&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;hoodie.datasource.write.recordkey.field&#34;</span>: <span style="color:#e6db74">&#34;id&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;hoodie.datasource.write.partitionpath.field&#34;</span>: <span style="color:#e6db74">&#34;creation_date&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;hoodie.datasource.write.precombine.field&#34;</span>: <span style="color:#e6db74">&#34;last_update_time&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;hoodie.datasource.hive_sync.enable&#34;</span>: <span style="color:#e6db74">&#34;true&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;hoodie.datasource.hive_sync.table&#34;</span>: <span style="color:#e6db74">&#34;my_hudi_table&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;hoodie.datasource.hive_sync.partition_fields&#34;</span>: <span style="color:#e6db74">&#34;creation_date&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;hoodie.datasource.hive_sync.partition_extractor_class&#34;</span>: <span style="color:#e6db74">&#34;org.apache.hudi.hive.MultiPartKeysValueExtractor&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;hoodie.index.type&#34;</span>: <span style="color:#e6db74">&#34;GLOBAL_BLOOM&#34;</span>,                 <span style="color:#75715e"># This is required if we want to ensure we upsert a record, even if the partition changes</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;hoodie.bloom.index.update.partition.path&#34;</span>: <span style="color:#e6db74">&#34;true&#34;</span>,  <span style="color:#75715e"># This is required to write the data into the new partition (defaults to false in 0.8.0, true in 0.9.0)</span>
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Write a DataFrame as a Hudi dataset</span>
</span></span><span style="display:flex;"><span>(
</span></span><span style="display:flex;"><span>    inputDF<span style="color:#f92672">.</span>write<span style="color:#f92672">.</span>format(<span style="color:#e6db74">&#34;org.apache.hudi&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>option(<span style="color:#e6db74">&#34;hoodie.datasource.write.operation&#34;</span>, <span style="color:#e6db74">&#34;insert&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>options(<span style="color:#f92672">**</span>hudiOptions)
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>mode(<span style="color:#e6db74">&#34;overwrite&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>save(<span style="color:#e6db74">&#34;s3://&lt;BUCKET&gt;/tmp/myhudidataset_001/&#34;</span>)
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h3 id="updating-one-partition-row">Updating one <em>partition</em> row</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql.functions <span style="color:#f92672">import</span> lit
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>updateDF <span style="color:#f92672">=</span> inputDF<span style="color:#f92672">.</span>limit(<span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>withColumn(<span style="color:#e6db74">&#39;creation_date&#39;</span>, lit(<span style="color:#e6db74">&#39;2021-09-22&#39;</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>(
</span></span><span style="display:flex;"><span>    updateDF<span style="color:#f92672">.</span>write<span style="color:#f92672">.</span>format(<span style="color:#e6db74">&#34;org.apache.hudi&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>option(<span style="color:#e6db74">&#34;hoodie.datasource.write.operation&#34;</span>, <span style="color:#e6db74">&#34;upsert&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>options(<span style="color:#f92672">**</span>hudiOptions)
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>mode(<span style="color:#e6db74">&#34;append&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>save(<span style="color:#e6db74">&#34;s3://&lt;BUCKET&gt;/tmp/myhudidataset_001/&#34;</span>)
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h3 id="resulting-parquet-files">Resulting Parquet Files</h3>
<p>Now if we look at the Parquet files on S3, we can see that:</p>
<ol>
<li>The old partition has a new Parquet file with the record removed</li>
<li>There is a new partition with the single record</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>aws s3 ls s3://&lt;BUCKET&gt;/tmp/myhudidataset_001/
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>2021-09-23 11:45:23     <span style="color:#ae81ff">434901</span> tmp/myhudidataset_001/2015-01-01/cd4b4b74-13f7-4c1e-a7ce-110bba8e16fd-0_0-404-90423_20210923184511.parquet
</span></span><span style="display:flex;"><span>2021-09-23 11:45:44     <span style="color:#ae81ff">434864</span> tmp/myhudidataset_001/2015-01-01/cd4b4b74-13f7-4c1e-a7ce-110bba8e16fd-0_0-442-103950_20210923184526.parquet
</span></span><span style="display:flex;"><span>2021-09-23 11:45:23     <span style="color:#ae81ff">434863</span> tmp/myhudidataset_001/2015-01-02/578ea02b-09f0-4952-afe5-94d44d158d29-0_1-404-90424_20210923184511.parquet
</span></span><span style="display:flex;"><span>2021-09-23 11:45:43     <span style="color:#ae81ff">434895</span> tmp/myhudidataset_001/2021-09-22/d67c9b50-1034-44b2-8ec9-2f3b1dcbf26c-0_1-442-103951_20210923184526.parquet
</span></span></code></pre></div><h3 id="athena-compatibility">Athena Compatibility</h3>
<p>We can also successfully query this dataset from Athena and see the updated data as well!</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sql" data-lang="sql"><span style="display:flex;"><span><span style="color:#66d9ef">SELECT</span> <span style="color:#f92672">*</span> <span style="color:#66d9ef">FROM</span> <span style="color:#e6db74">&#34;default&#34;</span>.<span style="color:#e6db74">&#34;my_hudi_table&#34;</span> 
</span></span></code></pre></div><p><img alt="Athena Results" loading="lazy" src="/posts/updating-partition-values-with-apache-hudi/athena-results.png"></p>
<p><em>Note the the different <code>_hoodie_file_name</code> for record id <code>100</code>.</em></p>
<p>Awesome! Now that I understand what&rsquo;s going on, it makes perfect sense. üôå</p>
]]></content:encoded></item><item><title>Continuous Deployment of Jupyter Notebooks</title><link>https://dacort.xyz/posts/continuous-deployment-of-jupyter-notebooks/</link><pubDate>Wed, 14 Jul 2021 16:00:00 -0700</pubDate><guid>https://dacort.xyz/posts/continuous-deployment-of-jupyter-notebooks/</guid><description>&lt;p&gt;This is a guide on how to use AWS CodePipeline to continuously deploy Jupyter notebooks to an S3-backed static website.&lt;/p&gt;
&lt;h2 id="overview"&gt;Overview&lt;/h2&gt;
&lt;p&gt;Since I started using &lt;a href="https://aws.amazon.com/emr/features/studio/"&gt;EMR Studio&lt;/a&gt;, I&amp;rsquo;ve been making more use of Jupyter notebooks as scratch pads and often want to be able to easily share the results of my research. I hunted around for a few different solutions and while there are some good ones like &lt;a href="https://nbconvert.readthedocs.io/en/latest/"&gt;nbconvert&lt;/a&gt; and &lt;a href="https://github.com/mwouts/jupytext/"&gt;jupytext&lt;/a&gt;, I wanted something a bit simpler and off-the-shelf. This post from &lt;a href="https://www.linkedin.com/in/mikkelhartmann/"&gt;Mikkel Hartmann&lt;/a&gt; about &lt;a href="http://mikkelhartmann.dk/2019/05/14/static-website-from-jupyter-notebooks.html"&gt;making a static website from Jupyter Notebooks&lt;/a&gt; led me to &lt;a href="https://www.mkdocs.org/"&gt;MkDocs&lt;/a&gt; and luckily, I came across &lt;a href="https://github.com/greenape/mknotebooks"&gt;mknotebooks&lt;/a&gt;, which offers a simple plugin for MkDocs. üòÖ&lt;/p&gt;</description><content:encoded><![CDATA[<p>This is a guide on how to use AWS CodePipeline to continuously deploy Jupyter notebooks to an S3-backed static website.</p>
<h2 id="overview">Overview</h2>
<p>Since I started using <a href="https://aws.amazon.com/emr/features/studio/">EMR Studio</a>, I&rsquo;ve been making more use of Jupyter notebooks as scratch pads and often want to be able to easily share the results of my research. I hunted around for a few different solutions and while there are some good ones like <a href="https://nbconvert.readthedocs.io/en/latest/">nbconvert</a> and <a href="https://github.com/mwouts/jupytext/">jupytext</a>, I wanted something a bit simpler and off-the-shelf. This post from <a href="https://www.linkedin.com/in/mikkelhartmann/">Mikkel Hartmann</a> about <a href="http://mikkelhartmann.dk/2019/05/14/static-website-from-jupyter-notebooks.html">making a static website from Jupyter Notebooks</a> led me to <a href="https://www.mkdocs.org/">MkDocs</a> and luckily, I came across <a href="https://github.com/greenape/mknotebooks">mknotebooks</a>, which offers a simple plugin for MkDocs. üòÖ</p>
<p>So, by using a simple static site generator that&rsquo;s geared toward project documentation, and a plugin that renders Jupyter notebooks quite well, and a few fancy code pipelines&hellip;I can easily push my notebooks to production. Let&rsquo;s go!</p>
<h2 id="architecture">Architecture</h2>
<p>This is the architecture we&rsquo;ll be implementing. This will all be built using the <a href="https://aws.amazon.com/cdk/">AWS Cloud Development Kit</a> (CDK).</p>
<p><img loading="lazy" src="/posts/continuous-deployment-of-jupyter-notebooks/jupyter_notebook_cd_architecture.png"></p>
<p>We&rsquo;ll be creating the following:</p>
<ul>
<li>2 S3 buckets to store our logs and website artifacts</li>
<li>A CodeCommmit repository that holds our site and notebooks</li>
<li>A CodeBuild project that generates the static site</li>
<li>A CodePipeline that is triggered by new commits, builds the site, and deploys it to S3</li>
<li>A CloudFront Distribution that serves the site</li>
<li>And optionally an ACM certificate if you want an alternate domain name</li>
</ul>
<p>I won&rsquo;t go into the details of the entire CDK stack, but instead will show how to deploy the CD pipeline.</p>
<h2 id="deploying">Deploying</h2>
<h3 id="pre-requisites">Pre-requisites</h3>
<p>You&rsquo;ll need to have <a href="https://docs.aws.amazon.com/cdk/latest/guide/getting_started.html#getting_started_prerequisites">CDK installed</a> (&gt;= v1.107.0) and Python &gt;= 3.9.</p>
<p>I use <a href="https://github.com/nodenv/nodenv">nodenv</a> and <a href="https://virtualenv.pypa.io/en/latest/">virtualenv</a> for my respective environments.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#75715e"># I use node 14.5.0</span>
</span></span><span style="display:flex;"><span>nodenv shell 14.5.0
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># And Python3</span>
</span></span><span style="display:flex;"><span>virtualenv -p python3.8 .venv
</span></span><span style="display:flex;"><span>source .venv/bin/activate
</span></span></code></pre></div><h3 id="bootstrapping">Bootstrapping</h3>
<p>The source code is available in <a href="https://github.com/dacort/jupyter-static-website">dacort/jupyter-static-website</a>. In order to get started, we just need to clone that repo and deploy our CDK stack!</p>
<p>This project is a two-phased deploy due to the fact that CloudFront certificates need to be in <code>us-east-1</code>. If you <em>do not</em> need a custom domain, you can skip the first part.</p>
<p>First, clone the project and install the necessary requirements.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>git clone https://github.com/dacort/jupyter-static-website.git
</span></span><span style="display:flex;"><span>cd jupyter-static-website
</span></span><span style="display:flex;"><span>pip install -r requirements.txt
</span></span></code></pre></div><p>You&rsquo;ll also need to <a href="https://docs.aws.amazon.com/cdk/latest/guide/bootstrapping.html">bootstrap</a> your AWS CDK environment in the account and region you want to deploy Part 2 in.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>cdk bootstrap aws://ACCOUNT-NUMBER-1/REGION-1
</span></span></code></pre></div><h3 id="part-1---cloudfront-certificate">Part 1 - CloudFront Certificate</h3>
<p><em>If you are not using a custom domain, skip to Part 2</em></p>
<p>This project only supports using the default CloudFront certificate and a DNS-validated CNAME. In order to generate the certificate, you&rsquo;ll need to run the command below, go into the <a href="https://console.aws.amazon.com/acm/home?region=us-east-1#/">AWS Certificate Manager console</a> and make sure you follow the validation instructions.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>cdk deploy CloudfrontCertificateStack -c domain_name<span style="color:#f92672">=</span>notebooks.example.com
</span></span></code></pre></div><p>Once the domain is validated, the stack should finish provisioning.</p>
<p>One of the outputs from this stack will be <code>CloudfrontCertificateStack.certificatearn</code> - you&rsquo;ll need the value of this for the next phase.</p>
<h3 id="part-2---jupyter-cd-pipeline">Part 2 - Jupyter CD Pipeline</h3>
<p><em>If you are not using a custom domain, you can omit both of the <code>-c</code> options below.</em></p>
<p>If you want to deploy to a different region, make sure you set the <code>AWS_DEFAULT_REGION</code> environment variable.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>cdk deploy EmrStudioPublisherStack <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    -c domain_name<span style="color:#f92672">=</span>notebooks.example.com <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    -c certificate_arn<span style="color:#f92672">=</span>arn:aws:acm:us-east-1:012345678912:certificate/f07b01a4-3e8c-4639-8a22-b7a20a832de3
</span></span></code></pre></div><p>Once this stack finishes, you should have a CodeCommit repository you can make changes to, a CloudFront distribution, and a publicly accessible URL (found in the <code>EmrStudioPublisherStack.cloudfrontendpoint</code> output) that has a pre-populated example site.</p>
<p>The site will take a few minutes to deploy - you&rsquo;ll be able to keep an eye on the status in the <a href="https://console.aws.amazon.com/codesuite/codepipeline/pipelines">CodePipeline console</a>.</p>
<h2 id="usage">Usage</h2>
<p>Usage is pretty straight-forward. <code>git clone</code> the repository, add a new notebook, and push it back up! If you&rsquo;re using EMR Studio, you can add your CodeCommit repository and make your changes to your Jupyter notebooks there.</p>
<p>I made a video about <a href="https://www.youtube.com/watch?v=ZdbUTxBjBIs">connecting to Git in EMR Studio</a> that you might find useful.</p>
<p>Any new notebooks added in the <code>site/docs/notebooks/</code> directory will automatically be published.</p>
<p>You can add links to the notebooks by updating the <code>nav</code> section of the <code>mkdocs.yml</code> file.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">nav</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">Home</span>: <span style="color:#ae81ff">index.md</span>
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">Notebooks</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#f92672">Oura Sleep Analysis</span>: <span style="color:#ae81ff">notebooks/damons_sleep.ipynb</span>
</span></span><span style="display:flex;"><span>    - <span style="color:#f92672">Intro to Data Processing on AWS</span>: <span style="color:#ae81ff">notebooks/intro_data_processing_aws.ipynb</span>
</span></span></code></pre></div><p>This YAML config will generate a nav dropdown like so.</p>
<p><img alt="Navigation example" loading="lazy" src="/posts/continuous-deployment-of-jupyter-notebooks/nav_dropdown.png"></p>
<h3 id="advanced-usage">Advanced Usage</h3>
<p>Note that not <em>all</em> images or libraries render nicely when converting to HTML. This is why, for example, in my plotly example I had to use <code>fig.show(renderer=&quot;jupyterlab&quot;)</code></p>
<p>In addition, if you paste multiple images into your notebook&rsquo;s Markdown, mknotebooks <a href="https://github.com/greenape/mknotebooks/issues/466">only renders one of them</a>. In order to work around this, I added a pre-build step that uniquify&rsquo;s all the image attachments in Markdown cells.</p>
]]></content:encoded></item><item><title>Build your own Air Quality Monitor with OpenAQ and EMR on EKS</title><link>https://dacort.xyz/posts/emr-eks-custom-images-with-openaq/</link><pubDate>Thu, 24 Jun 2021 21:20:00 +0000</pubDate><guid>https://dacort.xyz/posts/emr-eks-custom-images-with-openaq/</guid><description>&lt;p&gt;Fire season is closely approaching and as somebody that spent two weeks last year hunkered down inside with my browser glued to various air quality sites, I wanted to show how to use data from OpenAQ to build your own air quality analysis.&lt;/p&gt;
&lt;p&gt;With Amazon EMR on EKS, you can now &lt;a href="https://aws.amazon.com/blogs/aws/customize-and-package-dependencies-with-your-apache-spark-applications-on-amazon-emr-on-amazon-eks/"&gt;customize and package your own Apache Spark dependencies&lt;/a&gt; and I use that functionality for this post.&lt;/p&gt;
&lt;h2 id="overview"&gt;Overview&lt;/h2&gt;
&lt;p&gt;OpenAQ maintains a &lt;a href="https://registry.opendata.aws/openaq/"&gt;publicly accessible dataset of various air quality metrics&lt;/a&gt; that&amp;rsquo;s updated every half hour. &lt;a href="https://docs.bokeh.org/en/latest/index.html"&gt;Bokeh&lt;/a&gt; is a popular library for Python data visualization. While it includes sample data for US county and state boundaries, we&amp;rsquo;re going to use &lt;a href="https://www.census.gov/geographies/mapping-files/time-series/geo/cartographic-boundary.html"&gt;shapefiles from census.gov&lt;/a&gt;.&lt;/p&gt;</description><content:encoded><![CDATA[<p>Fire season is closely approaching and as somebody that spent two weeks last year hunkered down inside with my browser glued to various air quality sites, I wanted to show how to use data from OpenAQ to build your own air quality analysis.</p>
<p>With Amazon EMR on EKS, you can now <a href="https://aws.amazon.com/blogs/aws/customize-and-package-dependencies-with-your-apache-spark-applications-on-amazon-emr-on-amazon-eks/">customize and package your own Apache Spark dependencies</a> and I use that functionality for this post.</p>
<h2 id="overview">Overview</h2>
<p>OpenAQ maintains a <a href="https://registry.opendata.aws/openaq/">publicly accessible dataset of various air quality metrics</a> that&rsquo;s updated every half hour. <a href="https://docs.bokeh.org/en/latest/index.html">Bokeh</a> is a popular library for Python data visualization. While it includes sample data for US county and state boundaries, we&rsquo;re going to use <a href="https://www.census.gov/geographies/mapping-files/time-series/geo/cartographic-boundary.html">shapefiles from census.gov</a>.</p>
<p>We&rsquo;ll use an Apache Spark job on EMR on EKS to read the initial dataset from the S3 bucket, filter it for use case, and then combine it with the boundary data from census.gov in order to draw a map of the current air quality.</p>
<p>This post also shows how to use the custom containers support in EMR on EKS to build our own container image with the necessary dependencies.</p>
<h2 id="pre-requisites">Pre-requisites</h2>
<ul>
<li>An AWS account with access to Amazon Elastic Container Registry (ECR)</li>
<li>An EMR on EKS cluster already setup</li>
<li>Docker</li>
<li>A container registry to push your image to</li>
</ul>
<h2 id="building-the-emr-on-eks-container-image">Building the EMR on EKS Container Image</h2>
<h3 id="download-the-emr-base-image">Download the EMR base image</h3>
<p>For this post, we&rsquo;ll be using the <code>us-west-2</code> region and EMR 6.3.0 release. Each region and release has a different base image URL, and you can find the full list <a href="https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/docker-custom-images-tag.html">here</a>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>aws ecr get-login-password --region us-west-2 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    | docker login --username AWS --password-stdin 895885662937.dkr.ecr.us-west-2.amazonaws.com
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>docker pull 895885662937.dkr.ecr.us-west-2.amazonaws.com/notebook-spark/emr-6.3.0:latest
</span></span></code></pre></div><h3 id="customize-the-image">Customize the image</h3>
<p>EMR on EKS comes with a variety of <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-install-libraries-and-kernels.html">default libraries</a> installed including plotly and seaborn, but we wanted to try out Bokeh for my illustration as they have a great <a href="https://docs.bokeh.org/en/latest/docs/gallery/texas.html">choropleth example</a> and it&rsquo;s a library I&rsquo;ve been hearing about occasionally. I was hoping to use Bokeh&rsquo;s <code>sampledata</code> for US and county, but I ended up using <a href="https://geopandas.org/">GeoPandas</a> to re-project my map to a conic projection so Michigan wasn&rsquo;t squashed up against Wisconsin. :) GeoPandas makes it easy to read in shapefiles, so I just used the census.gov provided state and county data.</p>
<p>Bokeh also uses Selenium and Chrome for it&rsquo;s static image generation, so we go ahead and install Chrome on the container image as well.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-dockerfile" data-lang="dockerfile"><span style="display:flex;"><span><span style="color:#66d9ef">FROM</span> <span style="color:#e6db74">895885662937.dkr.ecr.us-west-2.amazonaws.com/notebook-spark/emr-6.3.0:latest</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">USER</span> <span style="color:#e6db74">root</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"># Install Chrome</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">RUN</span> curl https://intoli.com/install-google-chrome.sh | bash <span style="color:#f92672">&amp;&amp;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    mv /usr/bin/google-chrome-stable /usr/bin/chrome<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"># We need to upgrade pip in order to install pyproj</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">RUN</span> pip3 install --upgrade pip<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"># If you pip install as root, use this</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">RUN</span> pip3 install <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    bokeh<span style="color:#f92672">==</span>2.3.2 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    boto3<span style="color:#f92672">==</span>1.17.93 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    chromedriver-py<span style="color:#f92672">==</span>91.0.4472.19.0 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    geopandas<span style="color:#f92672">==</span>0.9.0 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    selenium<span style="color:#f92672">==</span>3.141.0 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    shapely<span style="color:#f92672">==</span>1.7.1<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">RUN</span> ln -s /usr/local/lib/python3.7/site-packages/chromedriver_py/chromedriver_linux64 /usr/local/bin/chromedriver<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"># Install bokeh sample data to /usr/local/share</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">RUN</span> mkdir /root/.bokeh <span style="color:#f92672">&amp;&amp;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    echo <span style="color:#e6db74">&#34;sampledata_dir: /usr/local/share/bokeh&#34;</span> &gt; /root/.bokeh/config <span style="color:#f92672">&amp;&amp;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    bokeh sampledata<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"># Also install census data into the image :)</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">ADD</span> https://www2.census.gov/geo/tiger/GENZ2020/shp/cb_2020_us_state_500k.zip  /usr/local/share/bokeh/<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">ADD</span> https://www2.census.gov/geo/tiger/GENZ2020/shp/cb_2020_us_county_500k.zip /usr/local/share/bokeh/<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">RUN</span> chmod <span style="color:#ae81ff">644</span> /usr/local/share/bokeh/cb*.zip<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"># This is a simple test to make sure generating the image works properly</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">COPY</span> test /test/<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">USER</span> <span style="color:#e6db74">hadoop:hadoop</span><span style="color:#960050;background-color:#1e0010">
</span></span></span></code></pre></div><h3 id="build-and-push">Build and push</h3>
<p>Great, we&rsquo;ve customized our image ‚Äì¬†now we just need to build and push it to a container registery somewhere! For this post, I chose GitHub but you can use any container registry like ECR or DockerHub.</p>
<p><em>The below commands assume you have a GitHub Personal Access Token that has access to push images in the <code>CR_PAT</code> environment variable.</em></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>docker build -t emr-6.3.0-bokeh:latest .
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>export USERNAME<span style="color:#f92672">=</span>GH_USERNAME
</span></span><span style="display:flex;"><span>echo $CR_PAT| docker login ghcr.io -u <span style="color:#e6db74">${</span>USERNAME<span style="color:#e6db74">}</span> --password-stdin
</span></span><span style="display:flex;"><span>docker tag emr-6.3.0-bokeh:latest ghcr.io/<span style="color:#e6db74">${</span>USERNAME<span style="color:#e6db74">}</span>/emr-6.3.0-bokeh:latest
</span></span><span style="display:flex;"><span>docker push ghcr.io/<span style="color:#e6db74">${</span>USERNAME<span style="color:#e6db74">}</span>/emr-6.3.0-bokeh:latest
</span></span></code></pre></div><p>Great, now your image is ready to go! Let&rsquo;s look at the code we&rsquo;re going to use to generate our air quality map.</p>
<h2 id="code-walkthrough">Code walkthrough</h2>
<p>If you already built your image, you can run the below code locally. In order to access S3 data, you&rsquo;ll have to set your <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_KEY_ID</code> environment variables.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>docker run --rm -it --name airq-demo <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    -e AWS_ACCESS_KEY_ID <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    -e AWS_SECRET_ACCESS_KEY <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    emr-6.3.0-bokeh <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    pyspark --deploy-mode client --master <span style="color:#e6db74">&#39;local[1]&#39;</span>
</span></span></code></pre></div><h3 id="reading-and-filtering-openaq-data">Reading and filtering OpenAQ Data</h3>
<p>The first thing we need to do is read the the data for today&rsquo;s date into a Spark dataframe.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> datetime
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>date <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>datetime<span style="color:#f92672">.</span>datetime<span style="color:#f92672">.</span>utcnow()<span style="color:#f92672">.</span>date()<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> spark<span style="color:#f92672">.</span>read<span style="color:#f92672">.</span>json(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;s3://openaq-fetches/realtime-gzipped/</span><span style="color:#e6db74">{</span>date<span style="color:#e6db74">}</span><span style="color:#e6db74">/&#34;</span>)
</span></span><span style="display:flex;"><span>df<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><pre tabindex="0"><code>+--------------------+---------------+---------+--------------------+-------+--------------------+--------------------+------+---------+-----------------+----------+-----+-----+
|         attribution|averagingPeriod|     city|         coordinates|country|                date|            location|mobile|parameter|       sourceName|sourceType| unit|value|
+--------------------+---------------+---------+--------------------+-------+--------------------+--------------------+------+---------+-----------------+----------+-----+-----+
|[{EPA AirNow DOS,...|   {hours, 1.0}|Abu Dhabi|{24.424399, 54.43...|     AE|{2021-06-13T22:00...|US Diplomatic Pos...| false|     pm25|StateAir_AbuDhabi|government|¬µg/m¬≥| 25.0|
|[{EPA AirNow DOS,...|   {hours, 1.0}|Abu Dhabi|{24.424399, 54.43...|     AE|{2021-06-13T23:00...|US Diplomatic Pos...| false|     pm25|StateAir_AbuDhabi|government|¬µg/m¬≥| 16.0|
|[{EPA AirNow DOS,...|   {hours, 1.0}|Abu Dhabi|{24.424399, 54.43...|     AE|{2021-06-14T00:00...|US Diplomatic Pos...| false|     pm25|StateAir_AbuDhabi|government|¬µg/m¬≥| 18.0|
|[{EPA AirNow DOS,...|   {hours, 1.0}|Abu Dhabi|{24.424399, 54.43...|     AE|{2021-06-14T01:00...|US Diplomatic Pos...| false|     pm25|StateAir_AbuDhabi|government|¬µg/m¬≥| 23.0|
|[{EPA AirNow DOS,...|   {hours, 1.0}|Abu Dhabi|{24.424399, 54.43...|     AE|{2021-06-14T02:00...|US Diplomatic Pos...| false|     pm25|StateAir_AbuDhabi|government|¬µg/m¬≥| 23.0|
|[{EPA AirNow DOS,...|   {hours, 1.0}|Abu Dhabi|{24.424399, 54.43...|     AE|{2021-06-14T03:00...|US Diplomatic Pos...| false|     pm25|StateAir_AbuDhabi|government|¬µg/m¬≥| 21.0|
|[{EPA AirNow DOS,...|   {hours, 1.0}|Abu Dhabi|{24.424399, 54.43...|     AE|{2021-06-14T04:00...|US Diplomatic Pos...| false|     pm25|StateAir_AbuDhabi|government|¬µg/m¬≥| 20.0|
|[{EPA AirNow DOS,...|   {hours, 1.0}|Abu Dhabi|{24.424399, 54.43...|     AE|{2021-06-14T05:00...|US Diplomatic Pos...| false|     pm25|StateAir_AbuDhabi|government|¬µg/m¬≥| 16.0|
|[{EPA AirNow DOS,...|   {hours, 1.0}|Abu Dhabi|{24.424399, 54.43...|     AE|{2021-06-14T06:00...|US Diplomatic Pos...| false|     pm25|StateAir_AbuDhabi|government|¬µg/m¬≥| 17.0|
|[{EPA AirNow DOS,...|   {hours, 1.0}|Abu Dhabi|{24.424399, 54.43...|     AE|{2021-06-14T07:00...|US Diplomatic Pos...| false|     pm25|StateAir_AbuDhabi|government|¬µg/m¬≥| 18.0|
|[{EPA AirNow DOS,...|   {hours, 1.0}|Abu Dhabi|{24.424399, 54.43...|     AE|{2021-06-14T08:00...|US Diplomatic Pos...| false|     pm25|StateAir_AbuDhabi|government|¬µg/m¬≥| 20.0|
|[{EPA AirNow DOS,...|   {hours, 1.0}|Abu Dhabi|{24.424399, 54.43...|     AE|{2021-06-14T09:00...|US Diplomatic Pos...| false|     pm25|StateAir_AbuDhabi|government|¬µg/m¬≥| 26.0|
|[{EPA AirNow DOS,...|   {hours, 1.0}|Abu Dhabi|{24.424399, 54.43...|     AE|{2021-06-14T10:00...|US Diplomatic Pos...| false|     pm25|StateAir_AbuDhabi|government|¬µg/m¬≥| 29.0|
|[{EPA AirNow DOS,...|   {hours, 1.0}|Abu Dhabi|{24.424399, 54.43...|     AE|{2021-06-14T11:00...|US Diplomatic Pos...| false|     pm25|StateAir_AbuDhabi|government|¬µg/m¬≥| 34.0|
|[{EPA AirNow DOS,...|   {hours, 1.0}|Abu Dhabi|{24.424399, 54.43...|     AE|{2021-06-14T12:00...|US Diplomatic Pos...| false|     pm25|StateAir_AbuDhabi|government|¬µg/m¬≥| 33.0|
|[{EPA AirNow DOS,...|   {hours, 1.0}|Abu Dhabi|{24.424399, 54.43...|     AE|{2021-06-14T13:00...|US Diplomatic Pos...| false|     pm25|StateAir_AbuDhabi|government|¬µg/m¬≥| 40.0|
|[{EPA AirNow DOS,...|   {hours, 1.0}|Abu Dhabi|{24.424399, 54.43...|     AE|{2021-06-14T14:00...|US Diplomatic Pos...| false|     pm25|StateAir_AbuDhabi|government|¬µg/m¬≥| 39.0|
|[{EPA AirNow DOS,...|   {hours, 1.0}|Abu Dhabi|{24.424399, 54.43...|     AE|{2021-06-14T15:00...|US Diplomatic Pos...| false|     pm25|StateAir_AbuDhabi|government|¬µg/m¬≥| 41.0|
|[{EPA AirNow DOS,...|   {hours, 1.0}|Abu Dhabi|{24.424399, 54.43...|     AE|{2021-06-14T16:00...|US Diplomatic Pos...| false|     pm25|StateAir_AbuDhabi|government|¬µg/m¬≥| 50.0|
|[{EPA AirNow DOS,...|   {hours, 1.0}|Abu Dhabi|{24.424399, 54.43...|     AE|{2021-06-14T17:00...|US Diplomatic Pos...| false|     pm25|StateAir_AbuDhabi|government|¬µg/m¬≥| 56.0|
+--------------------+---------------+---------+--------------------+-------+--------------------+--------------------+------+---------+-----------------+----------+-----+-----+
</code></pre><p>We can quickly see a few things:</p>
<ol>
<li>Data is provided from all over the globe, we just want US</li>
<li>We have coordinates and country, but that&rsquo;s it for location data</li>
<li>There are multiple different types of readings</li>
<li>There are multiple different readings per day per location</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>df<span style="color:#f92672">.</span>select(<span style="color:#e6db74">&#39;unit&#39;</span>, <span style="color:#e6db74">&#39;parameter&#39;</span>)<span style="color:#f92672">.</span>distinct()<span style="color:#f92672">.</span>sort(<span style="color:#e6db74">&#34;parameter&#34;</span>)<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><pre tabindex="0"><code>+-----+---------+                                                               
| unit|parameter|
+-----+---------+
|¬µg/m¬≥|       bc|
|¬µg/m¬≥|       co|
|  ppm|       co|
|  ppm|      no2|
|¬µg/m¬≥|      no2|
|¬µg/m¬≥|       o3|
|  ppm|       o3|
|¬µg/m¬≥|     pm10|
|¬µg/m¬≥|     pm25|
|  ppm|      so2|
|¬µg/m¬≥|      so2|
+-----+---------+
</code></pre><p>So, let&rsquo;s go ahead and filter down to the most recent PM2.5 reading in the United States.</p>
<p>To do that, it&rsquo;s a couple <code>where</code> filters and then we can utilize a window function (<code>last</code>) to get the last reading.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Filter down to US locations and PM2.5 readings only</span>
</span></span><span style="display:flex;"><span>usdf <span style="color:#f92672">=</span> (
</span></span><span style="display:flex;"><span>    df<span style="color:#f92672">.</span>where(df<span style="color:#f92672">.</span>country <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;US&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>where(df<span style="color:#f92672">.</span>parameter <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;pm25&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>select(<span style="color:#e6db74">&#34;coordinates&#34;</span>, <span style="color:#e6db74">&#34;date&#34;</span>, <span style="color:#e6db74">&#34;parameter&#34;</span>, <span style="color:#e6db74">&#34;unit&#34;</span>, <span style="color:#e6db74">&#34;value&#34;</span>, <span style="color:#e6db74">&#34;location&#34;</span>)
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Retrieve the most recent pm2.5 reading per county</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql.window <span style="color:#f92672">import</span> Window
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql.functions <span style="color:#f92672">import</span> last
</span></span><span style="display:flex;"><span>windowSpec <span style="color:#f92672">=</span> (
</span></span><span style="display:flex;"><span>    Window<span style="color:#f92672">.</span>partitionBy(<span style="color:#e6db74">&#34;location&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>orderBy(<span style="color:#e6db74">&#34;date.utc&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>rangeBetween(Window<span style="color:#f92672">.</span>unboundedPreceding, Window<span style="color:#f92672">.</span>unboundedFollowing)
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>last_reading_df <span style="color:#f92672">=</span> (
</span></span><span style="display:flex;"><span>    usdf<span style="color:#f92672">.</span>withColumn(<span style="color:#e6db74">&#34;last_value&#34;</span>, last(<span style="color:#e6db74">&#34;value&#34;</span>)<span style="color:#f92672">.</span>over(windowSpec))
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>select(<span style="color:#e6db74">&#34;coordinates&#34;</span>, <span style="color:#e6db74">&#34;last_value&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>distinct()
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>last_reading_df<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p>We also only selected the <code>coordinates</code> and <code>last_value</code> columns as these are all we need at this point.</p>
<pre tabindex="0"><code>+--------------------+----------+                                               
|         coordinates|last_value|
+--------------------+----------+
|{38.6619, -121.7278}|       2.0|
| {41.9767, -91.6878}|       4.9|
|{39.54092, -119.7...|       8.0|
|{43.629605, -72.3...|       9.0|
|{46.8505, -111.98...|      10.0|
|{39.818715, -75.4...|       8.5|
+--------------------+----------+
</code></pre><h3 id="mapping-coordinates-to-counties">Mapping coordinates to counties</h3>
<p>This was the most &ldquo;fun&rdquo; part of this journey. Bokeh provides some sample data and I initially just created a UDF that looked up the first county ID using the Polygon <code>intersects</code> method. Unfortunately, I then wanted to re-project the map to a conical projection (Albers). Bokeh&rsquo;s geo support isn&rsquo;t very strong, so I ended up looking at using GeoPandas to do the reprojection. That worked well, but the Bokeh county data wasn&rsquo;t in a format I could use with GeoPandas so I ended up downloading <a href="https://www.census.gov/geographies/mapping-files/time-series/geo/cartographic-boundary.html">Shapefiles from the Census Bureau</a>.</p>
<p>So, we&rsquo;ve got our <code>last_reading_df</code> dataframe. Lets map those coordinates to counties. The county data is relatively small (12mb zipped) so what I did was create a broadcast variable of <code>GEOID</code> -&gt; <code>Geometry</code> mappings that could be used in a UDF to figure out if a PM2.5 reading is inside a specific county.</p>
<ul>
<li>Download the census data and create a broadcast variable</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> geopandas <span style="color:#66d9ef">as</span> gpd
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>COUNTY_URL <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;https://www2.census.gov/geo/tiger/GENZ2020/shp/cb_2020_us_county_500k.zip&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>countydf <span style="color:#f92672">=</span> gpd<span style="color:#f92672">.</span>read_file(COUNTY_URL)
</span></span><span style="display:flex;"><span>bc_county <span style="color:#f92672">=</span> sc<span style="color:#f92672">.</span>broadcast(dict(zip(countydf[<span style="color:#e6db74">&#34;GEOID&#34;</span>], countydf[<span style="color:#e6db74">&#34;geometry&#34;</span>])))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>countydf<span style="color:#f92672">.</span>head()
</span></span></code></pre></div><p>We can see we&rsquo;re just mapping the <code>GEOID</code> column to the <code>geometry</code> column which is a polygon object containing the county boundaries.</p>
<pre tabindex="0"><code>  STATEFP COUNTYFP  COUNTYNS        AFFGEOID  GEOID       NAME          NAMELSAD STUSPS  STATE_NAME LSAD       ALAND     AWATER                                           geometry
0      21      141  00516917  0500000US21141  21141      Logan      Logan County     KY    Kentucky   06  1430224002   12479211  POLYGON ((-87.06037 36.68085, -87.06002 36.708...
1      36      081  00974139  0500000US36081  36081     Queens     Queens County     NY    New York   06   281594050  188444349  POLYGON ((-73.96262 40.73903, -73.96243 40.739...
2      34      017  00882278  0500000US34017  34017     Hudson     Hudson County     NJ  New Jersey   06   119640822   41836491  MULTIPOLYGON (((-74.04220 40.69997, -74.03900 ...
3      34      019  00882228  0500000US34019  34019  Hunterdon  Hunterdon County     NJ  New Jersey   06  1108086284   24761598  POLYGON ((-75.19511 40.57969, -75.19466 40.581...
4      21      147  00516926  0500000US21147  21147   McCreary   McCreary County     KY    Kentucky   06  1105416696   10730402  POLYGON ((-84.77845 36.60329, -84.73068 36.665...
</code></pre><ul>
<li>Create a UDF to find the county a coordinate is in</li>
</ul>
<p>This just brute forces the list of GEOIDs/polygons and returns the first GEOID that intersects. There is likely a more elegant to do this.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql.functions <span style="color:#f92672">import</span> udf
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql.types <span style="color:#f92672">import</span> StringType
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> shapely.geometry <span style="color:#f92672">import</span> Point
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">find_first_county_id</span>(longitude: float, latitude: float):
</span></span><span style="display:flex;"><span>    p <span style="color:#f92672">=</span> Point(longitude, latitude)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> index, geo <span style="color:#f92672">in</span> bc_county<span style="color:#f92672">.</span>value<span style="color:#f92672">.</span>items():
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> geo<span style="color:#f92672">.</span>intersects(p):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> index
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>find_first_county_id_udf <span style="color:#f92672">=</span> udf(find_first_county_id, StringType())
</span></span></code></pre></div><ul>
<li>Now we apply to the UDF to our <code>last_reading_df</code> dataframe</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Find the county that this reading is from</span>
</span></span><span style="display:flex;"><span>mapped_county_df <span style="color:#f92672">=</span> last_reading_df<span style="color:#f92672">.</span>withColumn(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;GEOID&#34;</span>,
</span></span><span style="display:flex;"><span>    find_first_county_id_udf(
</span></span><span style="display:flex;"><span>        last_reading_df<span style="color:#f92672">.</span>coordinates<span style="color:#f92672">.</span>longitude, last_reading_df<span style="color:#f92672">.</span>coordinates<span style="color:#f92672">.</span>latitude
</span></span><span style="display:flex;"><span>    ),
</span></span><span style="display:flex;"><span>)<span style="color:#f92672">.</span>select(<span style="color:#e6db74">&#34;GEOID&#34;</span>, <span style="color:#e6db74">&#34;last_value&#34;</span>)
</span></span></code></pre></div><ul>
<li>And then finally we calculate the average PM2.5 value per county</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Calculate the average reading per county</span>
</span></span><span style="display:flex;"><span>pm_avg_by_county <span style="color:#f92672">=</span> (
</span></span><span style="display:flex;"><span>    mapped_county_df<span style="color:#f92672">.</span>groupBy(<span style="color:#e6db74">&#34;GEOID&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>agg({<span style="color:#e6db74">&#34;last_value&#34;</span>: <span style="color:#e6db74">&#34;avg&#34;</span>})
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>withColumnRenamed(<span style="color:#e6db74">&#34;avg(last_value)&#34;</span>, <span style="color:#e6db74">&#34;avg_value&#34;</span>)
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pm_avg_by_county<span style="color:#f92672">.</span>show(<span style="color:#ae81ff">5</span>)
</span></span></code></pre></div><pre tabindex="0"><code>+-----+------------------+                                                      
|GEOID|         avg_value|
+-----+------------------+
|31157|              16.0|
|49053|               3.0|
|26153|               6.9|
|36029|               1.1|
|42101|             10.66|
+-----+------------------+
</code></pre><p>Cool! So now we have a <code>GEOID</code> we can use in our GeoPandas dataframe and an average value of the most recent PM2.5 reading for that county.</p>
<h3 id="generating-our-air-quality-map">Generating our Air Quality map</h3>
<p>Now that we&rsquo;ve got an average PM2.5 value per county, we need to join this with our map data and generate an image!</p>
<p>The first step is reading in US State and County shapefiles. We fetched these from census.gov while building the image and they&rsquo;re stored in <code>/usr/local/share/bokeh</code>. We also exclude any state not in the continental US.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> geopandas <span style="color:#66d9ef">as</span> gpd
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>STATE_FILE <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;file:///usr/local/share/bokeh/cb_2020_us_state_500k.zip&#34;</span>
</span></span><span style="display:flex;"><span>COUNTY_FILE <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;file:///usr/local/share/bokeh/cb_2020_us_county_500k.zip&#34;</span>
</span></span><span style="display:flex;"><span>EXCLUDED_STATES <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;AK&#34;</span>, <span style="color:#e6db74">&#34;HI&#34;</span>, <span style="color:#e6db74">&#34;PR&#34;</span>, <span style="color:#e6db74">&#34;GU&#34;</span>, <span style="color:#e6db74">&#34;VI&#34;</span>, <span style="color:#e6db74">&#34;MP&#34;</span>, <span style="color:#e6db74">&#34;AS&#34;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>county_df <span style="color:#f92672">=</span> gpd<span style="color:#f92672">.</span>read_file(COUNTY_FILE)<span style="color:#f92672">.</span>query(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;STUSPS not in </span><span style="color:#e6db74">{</span>EXCLUDED_STATES<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>state_df <span style="color:#f92672">=</span> gpd<span style="color:#f92672">.</span>read_file(STATE_FILE)<span style="color:#f92672">.</span>query(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;STUSPS not in </span><span style="color:#e6db74">{</span>EXCLUDED_STATES<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><p>Now we just do a simple <code>merge</code> on the GeoPandas dataframe, convert our maps to the Albers projection and save them as JSON objects.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Merge in our air quality data</span>
</span></span><span style="display:flex;"><span>county_aqi_df <span style="color:#f92672">=</span> county_df<span style="color:#f92672">.</span>merge(pm_avg_by_county<span style="color:#f92672">.</span>toPandas(), on<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;GEOID&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Convert to a &#34;proper&#34; Albers projection :)</span>
</span></span><span style="display:flex;"><span>state_json <span style="color:#f92672">=</span> state_df<span style="color:#f92672">.</span>to_crs(<span style="color:#e6db74">&#34;ESRI:102003&#34;</span>)<span style="color:#f92672">.</span>to_json()
</span></span><span style="display:flex;"><span>county_json <span style="color:#f92672">=</span> county_aqi_df<span style="color:#f92672">.</span>to_crs(<span style="color:#e6db74">&#34;ESRI:102003&#34;</span>)<span style="color:#f92672">.</span>to_json()
</span></span></code></pre></div><p>Now comes the fun part! Our data is all prepped, we&rsquo;ve averaged the most recent data by county, and built a GeoJSON file of everything we need. Let&rsquo;s map it!</p>
<p>I won&rsquo;t go into the details of every line, but we&rsquo;ll make use of Bokeh&rsquo;s awesome <code>GeoJSONDataSource</code> functionality, add a <code>LinearColorMapper</code> that automatically shades the counties for us by the <code>avg_value</code> column using the <code>Reds9</code> palette, and adds a <code>ColorBar</code> on the right-hand side.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> bokeh.models <span style="color:#f92672">import</span> ColorBar, GeoJSONDataSource, LinearColorMapper
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> bokeh.palettes <span style="color:#f92672">import</span> Reds9 <span style="color:#66d9ef">as</span> palette
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> bokeh.plotting <span style="color:#f92672">import</span> figure
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>p <span style="color:#f92672">=</span> figure(
</span></span><span style="display:flex;"><span>    title<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;US Air Quality Data&#34;</span>,
</span></span><span style="display:flex;"><span>    plot_width<span style="color:#f92672">=</span><span style="color:#ae81ff">1100</span>,
</span></span><span style="display:flex;"><span>    plot_height<span style="color:#f92672">=</span><span style="color:#ae81ff">700</span>,
</span></span><span style="display:flex;"><span>    toolbar_location<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>,
</span></span><span style="display:flex;"><span>    x_axis_location<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>,
</span></span><span style="display:flex;"><span>    y_axis_location<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>,
</span></span><span style="display:flex;"><span>    tooltips<span style="color:#f92672">=</span>[
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;County&#34;</span>, <span style="color:#e6db74">&#34;@NAME&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;Air Quality Index&#34;</span>, <span style="color:#e6db74">&#34;@avg_value&#34;</span>),
</span></span><span style="display:flex;"><span>    ],
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>p<span style="color:#f92672">.</span>grid<span style="color:#f92672">.</span>grid_line_color <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># This just adds our state lines</span>
</span></span><span style="display:flex;"><span>p<span style="color:#f92672">.</span>patches(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;xs&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;ys&#34;</span>,
</span></span><span style="display:flex;"><span>    fill_alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>,
</span></span><span style="display:flex;"><span>    line_color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;black&#34;</span>,
</span></span><span style="display:flex;"><span>    line_width<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>,
</span></span><span style="display:flex;"><span>    source<span style="color:#f92672">=</span>GeoJSONDataSource(geojson<span style="color:#f92672">=</span>state_json),
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Add our county data and shade them based on &#34;avg_value&#34;</span>
</span></span><span style="display:flex;"><span>color_mapper <span style="color:#f92672">=</span> LinearColorMapper(palette<span style="color:#f92672">=</span>tuple(reversed(palette)))
</span></span><span style="display:flex;"><span>color_column <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;avg_value&#34;</span>
</span></span><span style="display:flex;"><span>p<span style="color:#f92672">.</span>patches(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;xs&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;ys&#34;</span>,
</span></span><span style="display:flex;"><span>    fill_alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.7</span>,
</span></span><span style="display:flex;"><span>    fill_color<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#34;field&#34;</span>: color_column, <span style="color:#e6db74">&#34;transform&#34;</span>: color_mapper},
</span></span><span style="display:flex;"><span>    line_color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;black&#34;</span>,
</span></span><span style="display:flex;"><span>    line_width<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>,
</span></span><span style="display:flex;"><span>    source<span style="color:#f92672">=</span>GeoJSONDataSource(geojson<span style="color:#f92672">=</span>county_json),
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Now add a color bar legend on the right-hand side</span>
</span></span><span style="display:flex;"><span>color_bar <span style="color:#f92672">=</span> ColorBar(color_mapper<span style="color:#f92672">=</span>color_mapper, label_standoff<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>, width<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>p<span style="color:#f92672">.</span>add_layout(color_bar, <span style="color:#e6db74">&#34;right&#34;</span>)
</span></span></code></pre></div><p>Finally, let&rsquo;s go ahead export the png!</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> bokeh.io <span style="color:#f92672">import</span> export_png
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> bokeh.io.webdriver <span style="color:#f92672">import</span> create_chromium_webdriver
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>driver <span style="color:#f92672">=</span> create_chromium_webdriver([<span style="color:#e6db74">&#34;--no-sandbox&#34;</span>])
</span></span><span style="display:flex;"><span>export_png(p, filename<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;map.png&#34;</span>, webdriver<span style="color:#f92672">=</span>driver)
</span></span></code></pre></div><p>Now, if you&rsquo;re running on a mac, you can just copy the generated map to your local system and open it up!</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>docker cp airq-demo:/home/hadoop/map.png .
</span></span><span style="display:flex;"><span>open map.png
</span></span></code></pre></div><p><img alt="Air Quality map" loading="lazy" src="/images/posts-airq-map.png"></p>
<h3 id="running-on-emr-on-eks">Running on EMR on EKS</h3>
<p>I&rsquo;ve bundled this all up into a pyspark script in my <code>demo-code</code> repo.</p>
<p>This demo assumes you already have an EMR on EKS virtual cluster up and running, you&rsquo;ve built the image in the first part and pushed it to a container registry, and the IAM Role you use to run the job has access to both read and write to an S3 bucket.</p>
<p>First, download the <code>generate_aqi_map.py</code> code from the GitHub repo.</p>
<p>Then, upload that script to an S3 bucket you have access to.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>aws s3 cp generate_aqi_map.py s3://&lt;BUCKET&gt;/code/
</span></span></code></pre></div><p>Now, just run your job! The pyspark script takes a few parameters:</p>
<ul>
<li><code>&lt;S3_BUCKET&gt;</code> - The S3 bucket where you want to upload the generated image to</li>
<li><code>&lt;PREFIX&gt;</code> - The prefix in the bucket where you want the image located</li>
<li><code>--date 2021-01-01</code> (optional)  - A specific date for which you want to generate data for
<ul>
<li>Defaults to UTC today</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>export S3_BUCKET<span style="color:#f92672">=</span>&lt;BUCKET_NAME&gt;
</span></span><span style="display:flex;"><span>export EMR_EKS_CLUSTER_ID<span style="color:#f92672">=</span>abcdefghijklmno1234567890
</span></span><span style="display:flex;"><span>export EMR_EKS_EXECUTION_ARN<span style="color:#f92672">=</span>arn:aws:iam::123456789012:role/emr_eks_default_role
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#75715e"># Replace ghcr.io/OWNER/emr-6.3.0-bokeh:latest below with your image URL</span>
</span></span><span style="display:flex;"><span>aws emr-containers start-job-run <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    --virtual-cluster-id <span style="color:#e6db74">${</span>EMR_EKS_CLUSTER_ID<span style="color:#e6db74">}</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    --name openaq-conus <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    --execution-role-arn <span style="color:#e6db74">${</span>EMR_EKS_EXECUTION_ARN<span style="color:#e6db74">}</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    --release-label emr-6.3.0-latest <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    --job-driver <span style="color:#e6db74">&#39;{
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;sparkSubmitJobDriver&#34;: {
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            &#34;entryPoint&#34;: &#34;s3://&#39;</span><span style="color:#e6db74">${</span>S3_BUCKET<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;/code/generate_aqi_map.py&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            &#34;entryPointArguments&#34;: [&#34;&#39;</span><span style="color:#e6db74">${</span>S3_BUCKET<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;&#34;, &#34;output/airq/&#34;],
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            &#34;sparkSubmitParameters&#34;: &#34;--conf spark.kubernetes.container.image=ghcr.io/OWNER/emr-6.3.0-bokeh:latest&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        }
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    }&#39;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    --configuration-overrides <span style="color:#e6db74">&#39;{
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;monitoringConfiguration&#34;: {
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            &#34;s3MonitoringConfiguration&#34;: { &#34;logUri&#34;: &#34;s3://&#39;</span><span style="color:#e6db74">${</span>S3_BUCKET<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;/logs/&#34; }
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        }
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    }&#39;</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;id&#34;</span>: <span style="color:#e6db74">&#34;0000000abcdefg12345&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;name&#34;</span>: <span style="color:#e6db74">&#34;openaq-conus&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;arn&#34;</span>: <span style="color:#e6db74">&#34;arn:aws:emr-containers:us-east-2:123456789012:/virtualclusters/abcdefghijklmno1234567890/jobruns/0000000abcdefg12345&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;virtualClusterId&#34;</span>: <span style="color:#e6db74">&#34;abcdefghijklmno1234567890&#34;</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>While the job is running, you can get the fetch the status of the job using the <code>emr-containers describe-job-run</code> command.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>aws emr-containers describe-job-run <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    --virtual-cluster-id <span style="color:#e6db74">${</span>EMR_EKS_CLUSTER_ID<span style="color:#e6db74">}</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    --id 0000000abcdefg12345
</span></span></code></pre></div><p>Once the job is in the <code>COMPLETED</code> state, you should be able to copy the resulting image from your S3 bucket!</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>aws s3 cp s3://<span style="color:#e6db74">${</span>S3_BUCKET<span style="color:#e6db74">}</span>/output/airq/2021-06-24-latest.png .
</span></span></code></pre></div><p>And if you open that file, you&rsquo;ll get the most recent PM2.5 readings!</p>
<p><img alt="Air Quality map" loading="lazy" src="/images/posts-airq-map.png"></p>
<h2 id="wrapup">Wrapup</h2>
<p>Be sure to check out the <a href="https://aws.amazon.com/blogs/aws/customize-and-package-dependencies-with-your-apache-spark-applications-on-amazon-emr-on-amazon-eks/">launch post</a> for more details, the documentation for <a href="https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/docker-custom-images.html">customing docker images for EMR on EKS</a>, and my <a href="https://youtu.be/0x4DRKmNPfQ">demo video</a>.</p>
]]></content:encoded></item><item><title>Big Data Stack with CDK</title><link>https://dacort.xyz/posts/cdk-big-data-stack/</link><pubDate>Fri, 18 Jun 2021 21:59:00 -0700</pubDate><guid>https://dacort.xyz/posts/cdk-big-data-stack/</guid><description>How I built my own Apache Spark environment on AWS using Amazon EMR, Amazon EKS, and the AWS Cloud Development Kit (CDK).</description><content:encoded><![CDATA[<p>I wanted to write a post about how I built my own Apache Spark environment on AWS using Amazon EMR, Amazon EKS, and the AWS Cloud Development Kit (CDK). This stack also creates an EMR Studio environment that can be used to build and deploy data notebooks.</p>
<p><em>Disclaimer: I work for AWS on the EMR team and built this stack for my <a href="https://www.youtube.com/channel/UCKtlXVZC2DqzayRlZYLObZw">various demos</a> and it is not intended for production use-cases.</em> üôè</p>
<p>Also note that the commands below are for demonstration purposes only, the full code is available in my <a href="https://github.com/dacort/demo-code/tree/main/cdk/big-data-stack">demo-code/cdk/big-data-stack</a> repository.</p>
<h2 id="overview">Overview</h2>
<p>At re:Invent 2020, AWS introduced EMR on EKS - a way to run Apache Spark workloads on Kubernetes using managed services. Once you&rsquo;re up and running, you can submit a Spark job to EMR on EKS with a single AWS CLI command. But provisioning EKS, ensuring it has all the necessary resources to run your jobs, creating IAM roles and Kubernetes service accounts, and linking those back to EMR can be a lot of manual effort. Using AWS CDK, we can easily provision the entire stack:</p>
<ul>
<li>A VPC in 3 availability zones</li>
<li>An EKS cluster with the following addons
<ul>
<li>Cluster Autoscaler</li>
<li>Kubernetes Dashboard</li>
</ul>
</li>
<li>An EMR virtual cluster connected to our EKS cluster</li>
<li>An EMR Studio environment
<ul>
<li>A Service Catalog cluster template</li>
</ul>
</li>
</ul>
<p>This is the high-level architecture of what the resulting stack will look like. Now let&rsquo;s walk through each different piece.</p>
<p><img alt="CDK Big Data Stack architecture" loading="lazy" src="/images/cdk-big-data-stack-arch.png"></p>
<h2 id="vpc">VPC</h2>
<p>This is probably the easiest part of the whole setup. In CDK, you can provision a VPC with the following code:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> aws_cdk <span style="color:#f92672">import</span> aws_ec2 <span style="color:#66d9ef">as</span> ec2
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ec2<span style="color:#f92672">.</span>Vpc(self, <span style="color:#e6db74">&#34;EMRDemos&#34;</span>, max_azs<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>)
</span></span></code></pre></div><p>And you&rsquo;re done. üéâ In my stack, I make the resulting <code>ec2.Vpc</code> object an attribute on the VPC stack so I can use it in other parts of my stack.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>app <span style="color:#f92672">=</span> cdk<span style="color:#f92672">.</span>App()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>vpc <span style="color:#f92672">=</span> VPCStack(app, <span style="color:#e6db74">&#34;VPCStack&#34;</span>)
</span></span><span style="display:flex;"><span>eks <span style="color:#f92672">=</span> EKSStack(app, <span style="color:#e6db74">&#34;EKSStack&#34;</span>, vpc<span style="color:#f92672">.</span>vpc)
</span></span><span style="display:flex;"><span>emr_containers <span style="color:#f92672">=</span> EMRContainersStack(app, <span style="color:#e6db74">&#34;EMRContainers&#34;</span>, vpc<span style="color:#f92672">.</span>vpc, eks<span style="color:#f92672">.</span>cluster)
</span></span><span style="display:flex;"><span>emr_studio <span style="color:#f92672">=</span> EMRStudio(app, <span style="color:#e6db74">&#34;EMRStudio&#34;</span>, vpc<span style="color:#f92672">.</span>vpc, <span style="color:#e6db74">&#34;big-data-studio&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>app<span style="color:#f92672">.</span>synth()
</span></span></code></pre></div><p>Let&rsquo;s walk through what happens in each of these stacks.</p>
<h2 id="eksstack">EKSStack</h2>
<p>This one is a little more complex. In many cases, you only need to stand up your EKS cluster once and you&rsquo;ll use that in perpetuity. But there certainly are cases where you might provision multiple EKS stacks for security or organizational reasons. Provisioning an EKS stack in CDK is relatively straightforward.</p>
<ul>
<li>First, provision the EKS Cluster</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> aws_cdk <span style="color:#f92672">import</span> (
</span></span><span style="display:flex;"><span>    aws_eks <span style="color:#66d9ef">as</span> eks,
</span></span><span style="display:flex;"><span>    aws_ec2 <span style="color:#66d9ef">as</span> ec2,
</span></span><span style="display:flex;"><span>    aws_iam <span style="color:#66d9ef">as</span> iam,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cluster <span style="color:#f92672">=</span> eks<span style="color:#f92672">.</span>Cluster(
</span></span><span style="display:flex;"><span>    self,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;EksForSpark&#34;</span>,
</span></span><span style="display:flex;"><span>    cluster_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;data-team&#34;</span>,
</span></span><span style="display:flex;"><span>    version<span style="color:#f92672">=</span>eks<span style="color:#f92672">.</span>KubernetesVersion<span style="color:#f92672">.</span>V1_19,
</span></span><span style="display:flex;"><span>    default_capacity<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>,
</span></span><span style="display:flex;"><span>    endpoint_access<span style="color:#f92672">=</span>eks<span style="color:#f92672">.</span>EndpointAccess<span style="color:#f92672">.</span>PUBLIC_AND_PRIVATE,
</span></span><span style="display:flex;"><span>    vpc<span style="color:#f92672">=</span>vpc,
</span></span><span style="display:flex;"><span>    vpc_subnets<span style="color:#f92672">=</span>[ec2<span style="color:#f92672">.</span>SubnetSelection(subnet_type<span style="color:#f92672">=</span>ec2<span style="color:#f92672">.</span>SubnetType<span style="color:#f92672">.</span>PRIVATE)],
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><ul>
<li>Then, add one or more <a href="https://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html">managed node groups</a>.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>nodegroup <span style="color:#f92672">=</span> cluster<span style="color:#f92672">.</span>add_nodegroup_capacity(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;base-node-group&#34;</span>,
</span></span><span style="display:flex;"><span>    instance_types<span style="color:#f92672">=</span>[ec2<span style="color:#f92672">.</span>InstanceType(<span style="color:#e6db74">&#34;m5.xlarge&#34;</span>)],     <span style="color:#75715e"># You could add additional instance types here</span>
</span></span><span style="display:flex;"><span>    min_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,                                         <span style="color:#75715e"># The node group can scale to 1</span>
</span></span><span style="display:flex;"><span>    max_size<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>,                                        <span style="color:#75715e"># And up to 20 nodes</span>
</span></span><span style="display:flex;"><span>    disk_size<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>,                                       <span style="color:#75715e"># Give each node 50gb of disk</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><ul>
<li>You&rsquo;ll also likely want to grant an IAM role admin access to the cluster</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>admin_iam_role_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Admin&#34;</span>           <span style="color:#75715e"># This role must already exist in your AWS account</span>
</span></span><span style="display:flex;"><span>account_id <span style="color:#f92672">=</span> cdk<span style="color:#f92672">.</span>Aws<span style="color:#f92672">.</span>ACCOUNT_ID
</span></span><span style="display:flex;"><span>admin_role <span style="color:#f92672">=</span> iam<span style="color:#f92672">.</span>Role<span style="color:#f92672">.</span>from_role_arn(
</span></span><span style="display:flex;"><span>    self, <span style="color:#e6db74">&#34;admin_role&#34;</span>, <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;arn:aws:iam::</span><span style="color:#e6db74">{</span>account_id<span style="color:#e6db74">}</span><span style="color:#e6db74">:role/</span><span style="color:#e6db74">{</span>admin_role_name<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>cluster<span style="color:#f92672">.</span>aws_auth<span style="color:#f92672">.</span>add_masters_role(admin_role)
</span></span></code></pre></div><h3 id="cluster-autoscaler">Cluster Autoscaler</h3>
<p>Finally, a critical component to install is the <a href="https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html">Cluster Autoscaler</a>. The EKS documentation shows how to deploy it using the <code>kubectl</code> command, but I wanted to wrap this up in the CDK stack. I also ran into <a href="https://github.com/kubernetes/autoscaler/issues/3901">this issue</a> where a Helm chart for 1.19 doesn&rsquo;t exist. üôÅ.</p>
<p>So I created an <a href="https://github.com/dacort/demo-code/blob/main/cdk/big-data-stack/plugins/eks/autoscaler.py">autoscaler module</a> that I can easily apply to my cluster and node groups.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>cluster_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;data-team&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ClusterAutoscaler(
</span></span><span style="display:flex;"><span>    cluster_name, self, cluster, [nodegroup]
</span></span><span style="display:flex;"><span>)<span style="color:#f92672">.</span>enable_autoscaling()
</span></span></code></pre></div><p>This code performs the following steps in order to enable the cluster autoscaler on an EKS cluster:</p>
<ul>
<li>Adds tags to the passed node groups to enable auto discovery</li>
<li>Creates a new IAM Role that can change Auto Scaling Groups</li>
<li>Creates a Kubernetes service account</li>
<li>Creates a new cluster-autoscaler deployment</li>
<li>Creates the corresponding rbac rules</li>
</ul>
<h3 id="kubernetes-dashboard">Kubernetes Dashboard</h3>
<p>The Kubernetes Dashboard is also extremely valuable to be able to examine what&rsquo;s happening in your EKS cluster. It&rsquo;s easy to install by using the CDK <code>add_helm_chart</code> functionality.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>chart <span style="color:#f92672">=</span> cluster<span style="color:#f92672">.</span>add_helm_chart(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;kubernetes-dashboard&#34;</span>,
</span></span><span style="display:flex;"><span>    namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;kubernetes-dashboard&#34;</span>,
</span></span><span style="display:flex;"><span>    chart<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;kubernetes-dashboard&#34;</span>,
</span></span><span style="display:flex;"><span>    repository<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;https://kubernetes.github.io/dashboard/&#34;</span>,
</span></span><span style="display:flex;"><span>    values<span style="color:#f92672">=</span>{
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;fullnameOverride&#34;</span>: <span style="color:#e6db74">&#34;kubernetes-dashboard&#34;</span>,  <span style="color:#75715e"># This must be set to acccess the UI via `kubectl proxy`</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;extraArgs&#34;</span>: [<span style="color:#e6db74">&#34;--token-ttl=0&#34;</span>],              <span style="color:#75715e"># This prevents your access token from expiring, but is not intended for a production environment</span>
</span></span><span style="display:flex;"><span>    },
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>üíÅ Note that I&rsquo;m setting <code>--token-ttl=0</code> so I don&rsquo;t have to re-sign in to the Dashboard every 10(!) minutes, but you shouldn&rsquo;t use this in a production environment. There&rsquo;s some more info about <a href="https://blinkeye.github.io/post/public/2019-05-30-kubernetes-dashboard/">adjusting the timeout the dashboard here</a>.</p>
<h2 id="emrcontainersstack">EMRContainersStack</h2>
<p>A pre-requisite to running EMR on EKS is that you have to map the <code>AWSServiceRoleForAmazonEMRContainers</code> role to the EKS cluster. In order to prevent a circular dependency, I ended up doing this in the EKS stack.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>service_role_name <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;arn:aws:iam::</span><span style="color:#e6db74">{</span>cdk<span style="color:#f92672">.</span>Aws<span style="color:#f92672">.</span>ACCOUNT_ID<span style="color:#e6db74">}</span><span style="color:#e6db74">:role/AWSServiceRoleForAmazonEMRContainers&#34;</span>
</span></span><span style="display:flex;"><span>emrsvcrole <span style="color:#f92672">=</span> iam<span style="color:#f92672">.</span>Role<span style="color:#f92672">.</span>from_role_arn(
</span></span><span style="display:flex;"><span>    self, <span style="color:#e6db74">&#34;EmrSvcRole&#34;</span>, service_role_name, mutable<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>cluster<span style="color:#f92672">.</span>aws_auth<span style="color:#f92672">.</span>add_role_mapping(
</span></span><span style="display:flex;"><span>    emrsvcrole, groups<span style="color:#f92672">=</span>[], username<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;emr-containers&#34;</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p><a href="https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/setting-up.html">Setting up EMR on EKS</a> requires several steps:</p>
<ul>
<li>Create a namespace for EMR to use</li>
<li>Create a k8s cluster role for EMR</li>
<li>Bind the cluster role to the <code>emr-containers</code> user we created above</li>
<li>Create a (nicely-named) job execution role</li>
<li>And then create our EMR virtual cluster</li>
</ul>
<p>If you want to <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-create-eks-cluster.html">use EMR Studio with EMR on EKS</a>, you can also (optionally) create a managed endpoint. I don&rsquo;t quite have this fully automated with CDK yet.</p>
<p>I won&rsquo;t detail the whole setup here because it&rsquo;s about 400 lines of Python code, but you can see my <a href="https://github.com/dacort/demo-code/blob/main/cdk/big-data-stack/stacks/eks.py">EMR on EKS Stack</a> on GitHub.</p>
<p>‚ö†Ô∏è One thing to call out is that inside the <code>create_job_execution_role</code> method, we create a new job role we can use to run our jobs on EMR on EKS. This role is, admittedly, way overscoped and allows full access to S3, EC2, Glue, and CloudWatch. It is heavily recommended that you scope down the permissions this role has.</p>
<h2 id="emrstudio">EMRStudio</h2>
<p><a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-set-up.html">Setting up EMR Studio</a> requires the creation of several Studio-specific IAM roles, an S3 bucket for Studio assets, and mapping an AWS SSO user to the Studio.</p>
<p>The <code>EMRStudio</code> stack performs all these steps as well as tagging the provided VPC with <code>for-use-with-amazon-emr-managed-policies=true</code> so that if you&rsquo;re creating EMR clusters from Studio, the necessary resources can be created.</p>
<p>If you&rsquo;re creating a Studio environment from scratch, there is also an <a href="https://github.com/aws-samples/emr-studio-samples">EMR Studio Samples</a> GitHub repository that provides CloudFormation templates for creating an environment as well as a sample Apache Airflow DAG for triggering EMR on EKS jobs.</p>
<p>I won&rsquo;t dive too deep into this because it&rsquo;s again about 600 lines of Python code (largely IAM policies), but this is the CDK code that creates the EMR Studio.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>studio <span style="color:#f92672">=</span> emr<span style="color:#f92672">.</span>CfnStudio(
</span></span><span style="display:flex;"><span>    self,
</span></span><span style="display:flex;"><span>    construct_id,
</span></span><span style="display:flex;"><span>    name<span style="color:#f92672">=</span>name,
</span></span><span style="display:flex;"><span>    auth_mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;SSO&#34;</span>,
</span></span><span style="display:flex;"><span>    vpc_id<span style="color:#f92672">=</span>vpc<span style="color:#f92672">.</span>vpc_id,
</span></span><span style="display:flex;"><span>    default_s3_location<span style="color:#f92672">=</span>studio_bucket<span style="color:#f92672">.</span>s3_url_for_object(),
</span></span><span style="display:flex;"><span>    engine_security_group_id<span style="color:#f92672">=</span>engine_sg<span style="color:#f92672">.</span>security_group_id,
</span></span><span style="display:flex;"><span>    workspace_security_group_id<span style="color:#f92672">=</span>workspace_sg<span style="color:#f92672">.</span>security_group_id,
</span></span><span style="display:flex;"><span>    service_role<span style="color:#f92672">=</span>service_role<span style="color:#f92672">.</span>role_arn,
</span></span><span style="display:flex;"><span>    user_role<span style="color:#f92672">=</span>user_role<span style="color:#f92672">.</span>role_arn,
</span></span><span style="display:flex;"><span>    subnet_ids<span style="color:#f92672">=</span>vpc<span style="color:#f92672">.</span>select_subnets()<span style="color:#f92672">.</span>subnet_ids,
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h2 id="deploying">Deploying</h2>
<h3 id="prerequisites">Prerequisites</h3>
<ul>
<li>Node &gt;=14.x and <a href="https://docs.aws.amazon.com/cdk/latest/guide/getting_started.html">CDK</a></li>
<li>Python &gt;= 3.9 and <a href="https://docs.python-guide.org/dev/virtualenvs/#lower-level-virtualenv">virtualenv</a></li>
<li>An IAM role you want to grant admin access to EKS</li>
<li><a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-enable-sso.html">AWS SSO configured</a> in the AWS Region where you are deploying
<ul>
<li>A user in SSO you want to grant access to EMR Studio</li>
</ul>
</li>
<li><a href="https://kubernetes.io/docs/tasks/tools/">kubectl</a> if you wish to access the Kubernetes Dashboard</li>
</ul>
<h3 id="setup">Setup</h3>
<ul>
<li>Clone my <a href="https://github.com/dacort/demo-code"><code>demo-code</code></a> repository and <code>cd</code> into the <code>cdk/big-data-stack</code> directory</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>git clone https://github.com/dacort/demo-code.git
</span></span><span style="display:flex;"><span>cd demo-code/cdk/big-data-stack
</span></span></code></pre></div><ul>
<li>Create a Python virtualenv and install the necessary dependencies</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>virtualenv -p python3 .venv
</span></span><span style="display:flex;"><span>source .venv/bin/activate
</span></span><span style="display:flex;"><span>pip install -r requirements.txt
</span></span></code></pre></div><h3 id="deploy">Deploy</h3>
<ul>
<li>Bootstrap CDK for the AWS account and Region you want to deploy in</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>cdk bootstrap aws://account/region
</span></span></code></pre></div><ul>
<li>Deploy your big data stack!
<ul>
<li><code>eks_admin_role_name</code> is the IAM role you want to have admin access to EKS</li>
<li><code>studio_admin_user_name</code> is the AWS SSO user you want to grant access to EMR Studio</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>cdk deploy --all -c eks_admin_role_name<span style="color:#f92672">=</span>Admin -c studio_admin_user_name<span style="color:#f92672">=</span>dacort
</span></span></code></pre></div><p>This will synthesize your stack into a set of CloudFormation templates and then roll out each piece of the stack.</p>
<p>If everything goes well, you should get a set of big green checkmarks!</p>
<p>Each stack may also print out a set of &ldquo;Outputs&rdquo; and there are three in particular I want to highlight.</p>
<ol>
<li><code>EMRContainers.EMRVirtualClusterID = abcdefghijklmno1234567890</code></li>
</ol>
<p>This is the ID of your EMR virtual cluster - you&rsquo;ll need this to run jobs.</p>
<ol>
<li><code>EMRContainers.JobRoleArn = arn:aws:iam::012345678912:role/emr_eks_default_role</code></li>
</ol>
<p>This is the IAM role created in the <code>EMRContainers</code> stack that you can use to run your EMR on EKS jobs.</p>
<ol start="2">
<li><code>EKSStack.EksForSparkConfigCommandABCD1234 = aws eks update-kubeconfig --name data-team --region us-east-2 --role-arn arn:aws:iam::012345678912:role/EKSStack-EksForSparkMastersRoleABCD1234-ABCDEF123456</code></li>
</ol>
<p>This is the command you can use to add the new EKS cluster to your kubeconfig file so you can use <code>kubectl</code> commands.</p>
<h3 id="access-the-kubernetes-dashboard">Access the Kubernetes Dashboard</h3>
<p>We want to be able to inspect pods and other resources on our clusters. In order to do that, we need to use <code>kubectl</code> to proxy location connections to the EKS cluster.</p>
<p>First, run the command in the <code>EKSStack.EksForSparkConfigCommandABCD1234</code> output above. Then, use <code>kubectl proxy</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>‚ûú kubectl proxy
</span></span><span style="display:flex;"><span>Starting to serve on 127.0.0.1:8001
</span></span></code></pre></div><p>Next, fetch a token that you can use to login to the Dashboard.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>SECRET_NAME<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>kubectl -n kube-system get secret | grep eks-admin | awk <span style="color:#e6db74">&#39;{print $1}&#39;</span><span style="color:#66d9ef">)</span>
</span></span><span style="display:flex;"><span>TOKEN<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>kubectl -n kube-system describe secret $SECRET_NAME | grep -E <span style="color:#e6db74">&#39;^token&#39;</span> | cut -f2 -d<span style="color:#e6db74">&#39;:&#39;</span> | tr -d <span style="color:#e6db74">&#34; &#34;</span><span style="color:#66d9ef">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>echo $TOKEN
</span></span></code></pre></div><p>Copy the value that&rsquo;s output, open up the following URL in your browser, and paste the token where it says &ldquo;Enter token *&rdquo;</p>
<p><a href="http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:https/proxy/#!/login">http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:https/proxy/#!/login</a></p>
<p><img loading="lazy" src="/posts/cdk-big-data-stack/k8s-dashboard-login.png"></p>
<blockquote>
<p>A quick note about the URL above. It is constructed dynamically from the namespace and service names used in the kubernetes-dashboard helm chart. We&rsquo;ve defined the <code>fullnameOverride</code> value in the config for that chart or else the service name would be dynamic.</p>
</blockquote>
<p>Now that you&rsquo;re logged in, you should be able to browse to the &ldquo;Pods&rdquo; section and select <code>emr-jobs</code> from the namespace dropdown next to the search box. You won&rsquo;t have any pods yet, but they&rsquo;ll show up there when you run a job.</p>
<h3 id="run-a-job">Run a job</h3>
<p>With everything all deployed, you should be able to run a sample job through EMR on EKS.</p>
<blockquote>
<p>Note that the values of <code>virtual-cluster-id</code> and <code>execution-role-arn</code> can be obtained from the values output by your <code>cdk deploy command</code></p>
</blockquote>
<pre tabindex="0"><code>EMRContainers.EMRVirtualClusterID = abcdefghijklmno1234567890
EMRContainers.JobRoleArn = arn:aws:iam::012345678912:role/emr_eks_default_role
</code></pre><p>Take those values and replace in the job below!</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>export EMR_EKS_CLUSTER_ID<span style="color:#f92672">=</span>&lt;EMRVirtualClusterID&gt;
</span></span><span style="display:flex;"><span>export EMR_EKS_EXECUTION_ARN<span style="color:#f92672">=</span>&lt;JobRoleArn&gt;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>aws emr-containers start-job-run <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    --virtual-cluster-id <span style="color:#e6db74">${</span>EMR_EKS_CLUSTER_ID<span style="color:#e6db74">}</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    --name pi-test <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    --execution-role-arn <span style="color:#e6db74">${</span>EMR_EKS_EXECUTION_ARN<span style="color:#e6db74">}</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    --release-label emr-6.3.0-latest <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    --job-driver <span style="color:#e6db74">&#39;{
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;sparkSubmitJobDriver&#34;: {
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            &#34;entryPoint&#34;: &#34;local:///usr/lib/spark/examples/src/main/python/pi.py&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            &#34;sparkSubmitParameters&#34;: &#34;--conf spark.executor.instances=1 --conf spark.executor.memory=2G --conf spark.executor.cores=1 --conf spark.driver.cores=1&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        }
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    }&#39;</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;id&#34;</span>: <span style="color:#e6db74">&#34;0000000abcdef123456&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;name&#34;</span>: <span style="color:#e6db74">&#34;pi-test&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;arn&#34;</span>: <span style="color:#e6db74">&#34;arn:aws:emr-containers:us-east-2:012345678912:/virtualclusters/abcdefghijklmno1234567890/jobruns/0000000abcdef123456&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;virtualClusterId&#34;</span>: <span style="color:#e6db74">&#34;abcdefghijklmno1234567890&#34;</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h2 id="wrapup">Wrapup</h2>
<p>Take a look at my <a href="https://www.youtube.com/watch?v=2UMz72NRZss&amp;list=PLUe6KRx8LhLpJ8CyNHewFYukWm7sQyQrM">EMR on EKS playlists</a> for more examples.</p>
<p>And if you installed the EMR Studio stack, you can see your Studio URL in the <code>EMRStudio.EMRStudioURL</code> variable, or you can list your Studios and the access URL via the <code>aws emr list-studios</code> command.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>aws emr list-studios
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;Studios&#34;</span>: [
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;StudioId&#34;</span>: <span style="color:#e6db74">&#34;es-01234567890ABCDEFGHIJKLMN&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;Name&#34;</span>: <span style="color:#e6db74">&#34;big-data-studio&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;VpcId&#34;</span>: <span style="color:#e6db74">&#34;vpc-abcdef012345678901&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;Url&#34;</span>: <span style="color:#e6db74">&#34;https://es-01234567890ABCDEFGHIJKLMN.emrstudio-prod.us-east-1.amazonaws.com&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;CreationTime&#34;</span>: <span style="color:#e6db74">&#34;2021-06-02T16:20:56.548000-07:00&#34;</span>
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>]]></content:encoded></item></channel></rss>