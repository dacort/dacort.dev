<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>airflow on Damon Cortesi</title><link>https://dacort.dev/tags/airflow/</link><description>Recent content in airflow on Damon Cortesi</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 09 Jul 2021 11:30:00 -0700</lastBuildDate><atom:link href="https://dacort.dev/tags/airflow/index.xml" rel="self" type="application/rss+xml"/><item><title>Building and Testing a new Apache Airflow Plugin</title><link>https://dacort.dev/posts/building-and-testing-a-new-apache-airflow-plugin/</link><pubDate>Fri, 09 Jul 2021 11:30:00 -0700</pubDate><guid>https://dacort.dev/posts/building-and-testing-a-new-apache-airflow-plugin/</guid><description>Recently, I had the opportunity to add a new EMR on EKS plugin to Apache Airflow. While I&amp;rsquo;ve been a consumer of Airflow over the years, I&amp;rsquo;ve never contributed directly to the project. And weighing in at over half a million lines of code, Airflow is a pretty complex project to wade into. So here&amp;rsquo;s a guide on how I made a new operator in the AWS provider package.
Overview Before you get started, it&amp;rsquo;s good to have an understanding of the different components of an Airflow task.</description><content:encoded><![CDATA[<p>Recently, I had the opportunity to add a new EMR on EKS plugin to Apache Airflow. While I&rsquo;ve been a consumer of Airflow over the years, I&rsquo;ve never contributed directly to the project. And weighing in at over half a million lines of code, Airflow is a pretty complex project to wade into. So here&rsquo;s a guide on how I made a new operator in the AWS provider package.</p>
<p><img loading="lazy" src="cloc.png" alt=""  />
</p>
<h2 id="overview">Overview</h2>
<p>Before you get started, it&rsquo;s good to have an understanding of the different components of an Airflow task. The <a href="https://airflow.apache.org/docs/apache-airflow/stable/concepts/tasks.html">Airflow Tasks documentation</a> covers two of the important aspects:</p>
<ul>
<li>Operators, predefined task templates to build DAGs</li>
<li>Sensors, a subclass of Operators that wait on external services</li>
</ul>
<p><a href="https://airflow.apache.org/docs/apache-airflow/stable/concepts/connections.html#hooks">Hooks</a> are also important in that they are the main interface to external services and often the building blocks that Operators are built out of.</p>
<p>All that said, in Airflow 1.0, <a href="https://airflow.apache.org/docs/apache-airflow/stable/plugins.html">Plugins</a> were the primary way to integrate external features. That&rsquo;s changed in 2.0, and now there are sets of <a href="https://airflow.apache.org/docs/apache-airflow/stable/extra-packages-ref.html#providers-extras">Provider Packages</a> that provide pip-installable packages for integrating with different providers. This includes cloud providers like AWS and GCP, as well as different APIs like Discord, Salesforce, and Slack. The <a href="https://airflow.apache.org/docs/apache-airflow/stable/howto/custom-operator.html">custom operators</a> documentation is helpful, but it only discusses creating the operator - not how to test it, add documentation, update a provider package.</p>
<p>So&hellip;üòÖ once you have an understanding of how to add a new provider package and how it integrates, let&rsquo;s go over the steps we need to take to add a new plugin.</p>
<ol>
<li>Add the Plugin Operator/Hook/Sensor/etc</li>
<li>Add tests(!!) for your new code</li>
<li>Create an example DAG</li>
<li>Add documentation in on how to use the Operator</li>
<li>Update the various sections of the provider.yaml</li>
<li>linting, checks, and linting again</li>
</ol>
<p><em>The official Airflow docs on <a href="http://airflow.apache.org/docs/apache-airflow-providers/howto/create-update-providers.html">Community Providers</a> are also very helpful.</em></p>
<h2 id="creating-your-new-operator">Creating your new operator</h2>
<p>All provider packages live in the <a href="https://github.com/apache/airflow/tree/main/airflow/providers"><code>airflow/providers</code></a> subtree of the git repository.</p>
<p><img loading="lazy" src="provider_listing.png" alt="List of providers"  />
</p>
<p>If you look in each provider directory, you&rsquo;ll see various directories including <code>hooks</code>, <code>operators</code>, and <code>sensors</code>. These provide some good examples of how to create your Operator.</p>
<p>For now, I&rsquo;m going to create a new <code>emr_containers.py</code> file in each of the <code>hooks</code>, <code>operators</code>, and <code>sensors</code> directories. We&rsquo;ll be creating a new Hook for connecting to the <a href="https://docs.aws.amazon.com/emr-on-eks/latest/APIReference/Welcome.html">EMR on EKS API</a>, a new Sensor for waiting on jobs to complete, and an Operator that can be used to trigger your EMR on EKS jobs.</p>
<p>I won&rsquo;t go over the implementation details here, but you can take a look at each file in the Airflow repository.</p>
<p>One thing that was confusing to me during this process is that all three of those files have the same name&hellip;so at a glance, it was tough for me to know which component I was editing. But if you can keep this diagram in your head, it&rsquo;s pretty helpful.</p>
<p><img loading="lazy" src="emr_containers_workflow.png" alt=""  />
</p>
<p>Note that there is no <code>EMRContainerSensor</code> in this workflow - that&rsquo;s because the default operator handles polling/waiting for the job to complete itself.</p>
<h2 id="testing">Testing</h2>
<p>Similar to the provider packages, tests for the provider packages live in the <a href="https://github.com/apache/airflow/tree/main/tests/providers"><code>tests/providers</code></a> subtree.</p>
<p>With the AWS packages, many plugins use the <a href="https://github.com/spulec/moto">moto</a> library for testing, an AWS service mocking library. EMR on EKS is a fairly recent addition, so it&rsquo;s unfortunately not part of the mocking library. Instead, I used the standard <code>mock</code> library to return sample values from the API.</p>
<p>The tests are fairly standard unit tests, but what gets challenging is figuring how to actually <em>RUN</em> these tests. Airflow is a monorepo - there are many benefits and challenges to this approach, but what it means for us is we have to figure out how to run the whole smorgosboard of this project.</p>
<p>Luckily(!) Airflow has a cool CI environment known as <a href="https://github.com/apache/airflow/blob/main/BREEZE.rst">Breeze</a> that can pretty much do whatever you need to make sure your new plugin is working well!</p>
<h3 id="up-and-running-with-breeze">Up and running with Breeze</h3>
<p>The main challenge I had with Breeze was resource consumption. üôÅ</p>
<p><a href="https://github.com/apache/airflow/blob/main/BREEZE.rst#resources-required">Breeze requires</a> a minimum of 4GB RAM for Docker and 40GB of free disk space. I had to tweak both of those settings on my mac, but even so I think Breeze is named for the winds that are kicked up by laptop fans everywhere when it starts. üòÜ</p>
<p>In any case, you <em>should</em> be able to just type <code>./breeze</code> and be dropped into an Airflow shell after it builds a local version of the necessary Docker images.</p>
<h3 id="unit-tests">Unit tests</h3>
<p>Once in the Airflow shell, you should be able to run any of your unit tests.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">python -m pytest tests/providers/amazon/aws/operators/test_emr_containers.py
</code></pre></div><p>If you want, you can also run your tests with the <code>./breeze</code> CLI.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">./breeze tests tests/providers/amazon/aws/sensors/test_emr_containers.py tests/providers/amazon/aws/operators/test_emr_containers.py tests/providers/amazon/aws/hooks/test_emr_containers.py
</code></pre></div><p>Now, let&rsquo;s make sure we have an example DAG to run.</p>
<h2 id="integration-testing">Integration Testing</h2>
<h3 id="example-dags">Example DAGs</h3>
<p>It&rsquo;s crucial to provide an example DAG to show folks how to use your new Operator.</p>
<p>It&rsquo;s <em>also</em> crucial to have an example DAG so you can make sure your Operator works!</p>
<p>For AWS, example DAGs live in <code>airflow/providers/amazon/aws/example_dags</code>. For testing, however, you&rsquo;ll need to copy or link your DAG into <code>files/dags</code>. When you run <code>breeze</code>, it will mount that directory and the DAG will be available to Airflow.</p>
<h3 id="build-your-provider-package">Build your provider package</h3>
<p>First, we need to build a local version of our provider package with the <code>prepare-provider-packages</code> command.</p>
<p><em>You may receive an error like <code>The tag providers-amazon/2.0.0 exists. Not preparing the package.</code> - if you do, you&rsquo;ll need to provide a suffix to the <code>prepare-provider-packages</code> command with the <code>-S</code> flag.</em></p>
<p>Then you can hop into Airflow</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">./breeze prepare-provider-packages amazon -S dev
./breeze start-airflow --use-airflow-version wheel --use-packages-from-dist
</code></pre></div><p>If you run <code>breeze</code> without the <code>start-airflow</code>, you&rsquo;ll just get dropped into a bash prompt and need to start the webserver and scheduler manually. I recommend just letting Breeze take care of that.</p>
<p><img loading="lazy" src="breeze_shell.png" alt=""  />
</p>
<p>Once in the shell, you may need to create a user to be able to login to the Airflow UI.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">airflow users create --role Admin --username admin --password admin --email admin@example.com --firstname foo --lastname bar
</code></pre></div><p>By default, port 28080 gets forward to Airflow so you should be able to browse to <a href="http://localhost:28080">http://localhost:28080</a> and login as <code>admin</code>/<code>admin</code>.</p>
<p><img loading="lazy" src="airflow_dag_ui.png" alt=""  />
</p>
<h3 id="run-your-example-dag">Run your example DAG</h3>
<p>In theory, you can Unpause your DAG in the UI and run it, but it&rsquo;s likely you need some environment variables.</p>
<p>With AWS, you&rsquo;ll need to define access credentials and region. You can create a new Connection to do this, or just do so in your Airflow shell.</p>
<p>Since we&rsquo;re working localy, we&rsquo;ll use temporary access keys, but in a production environment your Airflow workers should use <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html">IAM roles</a>.</p>
<p><em>The following commands will all be executed in the <code>breeze</code> shell.</em></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">export AWS_ACCESS_KEY_ID<span style="color:#f92672">=</span>AKIAIOSFODNN7EXAMPLE
export AWS_SECRET_ACCESS_KEY<span style="color:#f92672">=</span>wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
export AWS_DEFAULT_REGION<span style="color:#f92672">=</span>us-east-1
</code></pre></div><p>Next, we&rsquo;ll try to run our example DAG using <code>airflow dags test</code>!</p>
<p>Our script requires the EMR on EKS virtual cluster ID and job role ARN, so we&rsquo;ll supply those as environment variables.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">VIRTUAL_CLUSTER_ID<span style="color:#f92672">=</span>abcdefghijklmno0123456789 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>JOB_ROLE_ARN<span style="color:#f92672">=</span>arn:aws:iam::111122223333:role/emr_eks_default_role <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>airflow dags test emr_eks_pi_job <span style="color:#66d9ef">$(</span>date -Is<span style="color:#66d9ef">)</span>
</code></pre></div><p>You should see the job spin up and display logs in your console.</p>
<p><img loading="lazy" src="emr_eks_dag_run.png" alt=""  />
</p>
<p>And if you refresh the Airflow UI you should see a successful run! If not&hellip;it&rsquo;s time to debug.</p>
<h2 id="documentation">Documentation</h2>
<p>OK! So&hellip;you&rsquo;ve built your new Operator. You&rsquo;ve written (and run) all your tests. Now it&rsquo;s time to help other folks use it!</p>
<p>The documentation was also a little bit tricky for me as it&rsquo;s in <a href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html">reStructuredText</a> format. I typically write in Markdown, so reST was a little foreign.</p>
<p>Fortunately, you can use <code>./breeze build-docs -- --package-filter apache-airflow-providers-amazon</code> to build the docs for a specific package.</p>
<p>Once you do that, the docs will be available in <code>docs/_build/docs/apache-airflow-providers-amazon/latest/index.html</code>. The links may not always work if you just open the file locally, but you should be able to make sure everything looks OK.</p>
<p>One awesome feature of reST is the ability to import snippets from other files and the Airflow docs make extensive use of this. For example, in my example DAG I have the following code:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># [START howto_operator_emr_eks_env_variables]</span>
VIRTUAL_CLUSTER_ID <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#34;VIRTUAL_CLUSTER_ID&#34;</span>, <span style="color:#e6db74">&#34;test-cluster&#34;</span>)
JOB_ROLE_ARN <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#34;JOB_ROLE_ARN&#34;</span>, <span style="color:#e6db74">&#34;arn:aws:iam::012345678912:role/emr_eks_default_role&#34;</span>)
<span style="color:#75715e"># [END howto_operator_emr_eks_env_variables]</span>
</code></pre></div><p>Note the <code>START</code> and <code>END</code> blocks. With those in there, I can include that snippet in my docs like so:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rest" data-lang="rest">.. <span style="color:#f92672">exampleinclude</span>:: /../../airflow/providers/amazon/aws/example_dags/example_emr_eks_job.py<span style="color:#960050;background-color:#1e0010">
</span><span style="color:#960050;background-color:#1e0010"></span>    <span style="color:#a6e22e">:language:</span> <span style="color:#a6e22e">python</span><span style="color:#960050;background-color:#1e0010">
</span><span style="color:#960050;background-color:#1e0010"></span>    <span style="color:#a6e22e">:start-after:</span> <span style="color:#a6e22e">[START howto_operator_emr_eks_env_variables]</span><span style="color:#960050;background-color:#1e0010">
</span><span style="color:#960050;background-color:#1e0010"></span>    <span style="color:#a6e22e">:end-before:</span> <span style="color:#a6e22e">[END howto_operator_emr_eks_env_variables]</span><span style="color:#960050;background-color:#1e0010">
</span></code></pre></div><p>And then it&rsquo;ll show up in the docs like this - pretty sweet!</p>
<p><img loading="lazy" src="docs_import.png" alt=""  />
</p>
<h3 id="provideryaml"><code>provider.yaml</code></h3>
<p>If you want your new Operator linked in the official provider docs, make sure to also update <code>provider.yaml</code> in the relevant provider.</p>
<h2 id="merging">Merging</h2>
<p>Now the <del>hard</del> easy part&hellip;getting your contribution merged in!</p>
<p>The official Airflow docs have a great section on the <a href="https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#contribution-workflow">contribution workflow</a>.</p>
<p>I think the main thing I struggled with were all the PR checks that happen automatically.</p>
<p>I couldn&rsquo;t figure out how to run them locally (and didn&rsquo;t learn about <a href="https://github.com/apache/airflow/blob/main/STATIC_CODE_CHECKS.rst#id2"><code>pre-commit</code></a> until after this process), so a lot of my workflow went like:</p>
<ul>
<li>commit locally</li>
<li>git push</li>
<li>wait for checks to fail</li>
<li>dive into GitHub Actions output to see what failed</li>
<li>fix locally</li>
<li>GOTO start</li>
</ul>
<p>I learned that you can use <code>breeze static-checks</code> to run a subset of the static checks locally. This is pretty helpful too if you want to avoid too many <code>git push</code>es.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">./breeze static-check all -- --files airflow/providers/amazon/aws/*/emr_containers.py 
</code></pre></div><p>That said, I&rsquo;m <em>very</em> happy there are so many checks in this project. There are a lot of things I didn&rsquo;t know about until the checks ran (like the <code>spelling_wordlist.txt</code> file) and it&rsquo;s great to have such a high level of automation to help contributors maintain the quality of their code.</p>
<h2 id="wrapup">Wrapup</h2>
<p>I want to send a quick shoutout to the folks in the <a href="https://apache-airflow-slack.herokuapp.com/">Airflow Slack community</a> - they&rsquo;re all super nice and welcoming. And especially the folks that reviewed my PR, who were kind enough to not make <em>too</em> much of my <code>BEEP BOOP</code> error message I had committed with my initial revision. ü§ñ üòÜ</p>
]]></content:encoded></item></channel></rss>