<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Building and Testing a new Apache Airflow Plugin | Damon Cortesi</title><meta name=keywords content="airflow,emr,eks"><meta name=description content="Recently, I had the opportunity to add a new EMR on EKS plugin to Apache Airflow. While I&rsquo;ve been a consumer of Airflow over the years, I&rsquo;ve never contributed directly to the project. And weighing in at over half a million lines of code, Airflow is a pretty complex project to wade into. So here&rsquo;s a guide on how I made a new operator in the AWS provider package.
Overview Before you get started, it&rsquo;s good to have an understanding of the different components of an Airflow task."><meta name=author content><link rel=canonical href=https://dacort.dev/posts/building-and-testing-a-new-apache-airflow-plugin/><link crossorigin=anonymous href=/assets/css/stylesheet.min.640768aec184f5d7657b498b44c1649aeeac390be208b1077640933543054b77.css integrity="sha256-ZAdorsGE9ddle0mLRMFkmu6sOQviCLEHdkCTNUMFS3c=" rel="preload stylesheet" as=style><link rel=preload href=/images/dacort.jpeg as=image><script defer crossorigin=anonymous src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://dacort.dev/images/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://dacort.dev/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://dacort.dev/favicon-32x32.png><link rel=apple-touch-icon href=https://dacort.dev/apple-touch-icon.png><link rel=mask-icon href=https://dacort.dev/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.83.1"><meta property="og:title" content="Building and Testing a new Apache Airflow Plugin"><meta property="og:description" content="Recently, I had the opportunity to add a new EMR on EKS plugin to Apache Airflow. While I&rsquo;ve been a consumer of Airflow over the years, I&rsquo;ve never contributed directly to the project. And weighing in at over half a million lines of code, Airflow is a pretty complex project to wade into. So here&rsquo;s a guide on how I made a new operator in the AWS provider package.
Overview Before you get started, it&rsquo;s good to have an understanding of the different components of an Airflow task."><meta property="og:type" content="article"><meta property="og:url" content="https://dacort.dev/posts/building-and-testing-a-new-apache-airflow-plugin/"><meta property="og:image" content="https://dacort.dev/posts/building-and-testing-a-new-apache-airflow-plugin/cover.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-07-09T11:30:00-07:00"><meta property="article:modified_time" content="2021-07-09T11:30:00-07:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dacort.dev/posts/building-and-testing-a-new-apache-airflow-plugin/cover.jpg"><meta name=twitter:title content="Building and Testing a new Apache Airflow Plugin"><meta name=twitter:description content="Recently, I had the opportunity to add a new EMR on EKS plugin to Apache Airflow. While I&rsquo;ve been a consumer of Airflow over the years, I&rsquo;ve never contributed directly to the project. And weighing in at over half a million lines of code, Airflow is a pretty complex project to wade into. So here&rsquo;s a guide on how I made a new operator in the AWS provider package.
Overview Before you get started, it&rsquo;s good to have an understanding of the different components of an Airflow task."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://dacort.dev/posts/"},{"@type":"ListItem","position":2,"name":"Building and Testing a new Apache Airflow Plugin","item":"https://dacort.dev/posts/building-and-testing-a-new-apache-airflow-plugin/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Building and Testing a new Apache Airflow Plugin","name":"Building and Testing a new Apache Airflow Plugin","description":"Recently, I had the opportunity to add a new EMR on EKS plugin to Apache Airflow. While I\u0026rsquo;ve been a consumer of Airflow over the years, I\u0026rsquo;ve never contributed directly to the project. And weighing in at over half a million lines of code, Airflow is a pretty complex project to wade into. So here\u0026rsquo;s a guide on how I made a new operator in the AWS provider package.\nOverview Before you get started, it\u0026rsquo;s good to have an understanding of the different components of an Airflow task.","keywords":["airflow","emr","eks"],"articleBody":"Recently, I had the opportunity to add a new EMR on EKS plugin to Apache Airflow. While I‚Äôve been a consumer of Airflow over the years, I‚Äôve never contributed directly to the project. And weighing in at over half a million lines of code, Airflow is a pretty complex project to wade into. So here‚Äôs a guide on how I made a new operator in the AWS provider package.\nOverview Before you get started, it‚Äôs good to have an understanding of the different components of an Airflow task. The Airflow Tasks documentation covers two of the important aspects:\n Operators, predefined task templates to build DAGs Sensors, a subclass of Operators that wait on external services  Hooks are also important in that they are the main interface to external services and often the building blocks that Operators are built out of.\nAll that said, in Airflow 1.0, Plugins were the primary way to integrate external features. That‚Äôs changed in 2.0, and now there are sets of Provider Packages that provide pip-installable packages for integrating with different providers. This includes cloud providers like AWS and GCP, as well as different APIs like Discord, Salesforce, and Slack. The custom operators documentation is helpful, but it only discusses creating the operator - not how to test it, add documentation, update a provider package.\nSo‚Ä¶üòÖ once you have an understanding of how to add a new provider package and how it integrates, let‚Äôs go over the steps we need to take to add a new plugin.\n Add the Plugin Operator/Hook/Sensor/etc Add tests(!!) for your new code Create an example DAG Add documentation in on how to use the Operator Update the various sections of the provider.yaml linting, checks, and linting again  The official Airflow docs on Community Providers are also very helpful.\nCreating your new operator All provider packages live in the airflow/providers subtree of the git repository.\nIf you look in each provider directory, you‚Äôll see various directories including hooks, operators, and sensors. These provide some good examples of how to create your Operator.\nFor now, I‚Äôm going to create a new emr_containers.py file in each of the hooks, operators, and sensors directories. We‚Äôll be creating a new Hook for connecting to the EMR on EKS API, a new Sensor for waiting on jobs to complete, and an Operator that can be used to trigger your EMR on EKS jobs.\nI won‚Äôt go over the implementation details here, but you can take a look at each file in the Airflow repository.\nOne thing that was confusing to me during this process is that all three of those files have the same name‚Ä¶so at a glance, it was tough for me to know which component I was editing. But if you can keep this diagram in your head, it‚Äôs pretty helpful.\nNote that there is no EMRContainerSensor in this workflow - that‚Äôs because the default operator handles polling/waiting for the job to complete itself.\nTesting Similar to the provider packages, tests for the provider packages live in the tests/providers subtree.\nWith the AWS packages, many plugins use the moto library for testing, an AWS service mocking library. EMR on EKS is a fairly recent addition, so it‚Äôs unfortunately not part of the mocking library. Instead, I used the standard mock library to return sample values from the API.\nThe tests are fairly standard unit tests, but what gets challenging is figuring how to actually RUN these tests. Airflow is a monorepo - there are many benefits and challenges to this approach, but what it means for us is we have to figure out how to run the whole smorgosboard of this project.\nLuckily(!) Airflow has a cool CI environment known as Breeze that can pretty much do whatever you need to make sure your new plugin is working well!\nUp and running with Breeze The main challenge I had with Breeze was resource consumption. üôÅ\nBreeze requires a minimum of 4GB RAM for Docker and 40GB of free disk space. I had to tweak both of those settings on my mac, but even so I think Breeze is named for the winds that are kicked up by laptop fans everywhere when it starts. üòÜ\nIn any case, you should be able to just type ./breeze and be dropped into an Airflow shell after it builds a local version of the necessary Docker images.\nUnit tests Once in the Airflow shell, you should be able to run any of your unit tests.\npython -m pytest tests/providers/amazon/aws/operators/test_emr_containers.py If you want, you can also run your tests with the ./breeze CLI.\n./breeze tests tests/providers/amazon/aws/sensors/test_emr_containers.py tests/providers/amazon/aws/operators/test_emr_containers.py tests/providers/amazon/aws/hooks/test_emr_containers.py Now, let‚Äôs make sure we have an example DAG to run.\nIntegration Testing Example DAGs It‚Äôs crucial to provide an example DAG to show folks how to use your new Operator.\nIt‚Äôs also crucial to have an example DAG so you can make sure your Operator works!\nFor AWS, example DAGs live in airflow/providers/amazon/aws/example_dags. For testing, however, you‚Äôll need to copy or link your DAG into files/dags. When you run breeze, it will mount that directory and the DAG will be available to Airflow.\nBuild your provider package First, we need to build a local version of our provider package with the prepare-provider-packages command.\nYou may receive an error like The tag providers-amazon/2.0.0 exists. Not preparing the package. - if you do, you‚Äôll need to provide a suffix to the prepare-provider-packages command with the -S flag.\nThen you can hop into Airflow\n./breeze prepare-provider-packages amazon -S dev ./breeze start-airflow --use-airflow-version wheel --use-packages-from-dist If you run breeze without the start-airflow, you‚Äôll just get dropped into a bash prompt and need to start the webserver and scheduler manually. I recommend just letting Breeze take care of that.\nOnce in the shell, you may need to create a user to be able to login to the Airflow UI.\nairflow users create --role Admin --username admin --password admin --email admin@example.com --firstname foo --lastname bar By default, port 28080 gets forward to Airflow so you should be able to browse to http://localhost:28080 and login as admin/admin.\nRun your example DAG In theory, you can Unpause your DAG in the UI and run it, but it‚Äôs likely you need some environment variables.\nWith AWS, you‚Äôll need to define access credentials and region. You can create a new Connection to do this, or just do so in your Airflow shell.\nSince we‚Äôre working localy, we‚Äôll use temporary access keys, but in a production environment your Airflow workers should use IAM roles.\nThe following commands will all be executed in the breeze shell.\nexport AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE export AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY export AWS_DEFAULT_REGION=us-east-1 Next, we‚Äôll try to run our example DAG using airflow dags test!\nOur script requires the EMR on EKS virtual cluster ID and job role ARN, so we‚Äôll supply those as environment variables.\nVIRTUAL_CLUSTER_ID=abcdefghijklmno0123456789 \\ JOB_ROLE_ARN=arn:aws:iam::111122223333:role/emr_eks_default_role \\ airflow dags test emr_eks_pi_job $(date -Is) You should see the job spin up and display logs in your console.\nAnd if you refresh the Airflow UI you should see a successful run! If not‚Ä¶it‚Äôs time to debug.\nDocumentation OK! So‚Ä¶you‚Äôve built your new Operator. You‚Äôve written (and run) all your tests. Now it‚Äôs time to help other folks use it!\nThe documentation was also a little bit tricky for me as it‚Äôs in reStructuredText format. I typically write in Markdown, so reST was a little foreign.\nFortunately, you can use ./breeze build-docs -- --package-filter apache-airflow-providers-amazon to build the docs for a specific package.\nOnce you do that, the docs will be available in docs/_build/docs/apache-airflow-providers-amazon/latest/index.html. The links may not always work if you just open the file locally, but you should be able to make sure everything looks OK.\nOne awesome feature of reST is the ability to import snippets from other files and the Airflow docs make extensive use of this. For example, in my example DAG I have the following code:\n# [START howto_operator_emr_eks_env_variables] VIRTUAL_CLUSTER_ID = os.getenv(\"VIRTUAL_CLUSTER_ID\", \"test-cluster\") JOB_ROLE_ARN = os.getenv(\"JOB_ROLE_ARN\", \"arn:aws:iam::012345678912:role/emr_eks_default_role\") # [END howto_operator_emr_eks_env_variables] Note the START and END blocks. With those in there, I can include that snippet in my docs like so:\n.. exampleinclude:: /../../airflow/providers/amazon/aws/example_dags/example_emr_eks_job.py :language: python :start-after: [START howto_operator_emr_eks_env_variables] :end-before: [END howto_operator_emr_eks_env_variables]And then it‚Äôll show up in the docs like this - pretty sweet!\nprovider.yaml If you want your new Operator linked in the official provider docs, make sure to also update provider.yaml in the relevant provider.\nMerging Now the hard easy part‚Ä¶getting your contribution merged in!\nThe official Airflow docs have a great section on the contribution workflow.\nI think the main thing I struggled with were all the PR checks that happen automatically.\nI couldn‚Äôt figure out how to run them locally (and didn‚Äôt learn about pre-commit until after this process), so a lot of my workflow went like:\n commit locally git push wait for checks to fail dive into GitHub Actions output to see what failed fix locally GOTO start  I learned that you can use breeze static-checks to run a subset of the static checks locally. This is pretty helpful too if you want to avoid too many git pushes.\n./breeze static-check all -- --files airflow/providers/amazon/aws/*/emr_containers.py That said, I‚Äôm very happy there are so many checks in this project. There are a lot of things I didn‚Äôt know about until the checks ran (like the spelling_wordlist.txt file) and it‚Äôs great to have such a high level of automation to help contributors maintain the quality of their code.\nWrapup I want to send a quick shoutout to the folks in the Airflow Slack community - they‚Äôre all super nice and welcoming. And especially the folks that reviewed my PR, who were kind enough to not make too much of my BEEP BOOP error message I had committed with my initial revision. ü§ñ üòÜ\n","wordCount":"1617","inLanguage":"en","image":"https://dacort.dev/posts/building-and-testing-a-new-apache-airflow-plugin/cover.jpg","datePublished":"2021-07-09T11:30:00-07:00","dateModified":"2021-07-09T11:30:00-07:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://dacort.dev/posts/building-and-testing-a-new-apache-airflow-plugin/"},"publisher":{"@type":"Organization","name":"Damon Cortesi","logo":{"@type":"ImageObject","url":"https://dacort.dev/images/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:#1d1e20;--entry:#2e2e33;--primary:rgba(255, 255, 255, 0.84);--secondary:rgba(255, 255, 255, 0.56);--tertiary:rgba(255, 255, 255, 0.16);--content:rgba(255, 255, 255, 0.74);--hljs-bg:#2e2e33;--code-bg:#37383e;--border:#333}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://dacort.dev/ accesskey=h title="Damon Cortesi (Alt + H)">Damon Cortesi</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://dacort.dev/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://dacort.dev/posts/ title=Blog><span>Blog</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://dacort.dev/>Home</a>&nbsp;¬ª&nbsp;<a href=https://dacort.dev/posts/>Posts</a></div><h1 class=post-title>Building and Testing a new Apache Airflow Plugin</h1><div class=post-meta>July 9, 2021&nbsp;¬∑&nbsp;8 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><div class=details>Table of Contents</div></summary><div class=inner><ul><li><a href=#overview aria-label=Overview>Overview</a></li><li><a href=#creating-your-new-operator aria-label="Creating your new operator">Creating your new operator</a></li><li><a href=#testing aria-label=Testing>Testing</a><ul><li><a href=#up-and-running-with-breeze aria-label="Up and running with Breeze">Up and running with Breeze</a></li><li><a href=#unit-tests aria-label="Unit tests">Unit tests</a></li></ul></li><li><a href=#integration-testing aria-label="Integration Testing">Integration Testing</a><ul><li><a href=#example-dags aria-label="Example DAGs">Example DAGs</a></li><li><a href=#build-your-provider-package aria-label="Build your provider package">Build your provider package</a></li><li><a href=#run-your-example-dag aria-label="Run your example DAG">Run your example DAG</a></li></ul></li><li><a href=#documentation aria-label=Documentation>Documentation</a><ul><li><a href=#provideryaml aria-label=provider.yaml><code>provider.yaml</code></a></li></ul></li><li><a href=#merging aria-label=Merging>Merging</a></li><li><a href=#wrapup aria-label=Wrapup>Wrapup</a></li></ul></div></details></div><div class=post-content><p>Recently, I had the opportunity to add a new EMR on EKS plugin to Apache Airflow. While I&rsquo;ve been a consumer of Airflow over the years, I&rsquo;ve never contributed directly to the project. And weighing in at over half a million lines of code, Airflow is a pretty complex project to wade into. So here&rsquo;s a guide on how I made a new operator in the AWS provider package.</p><p><img loading=lazy src=cloc.png alt></p><h2 id=overview>Overview<a hidden class=anchor aria-hidden=true href=#overview>#</a></h2><p>Before you get started, it&rsquo;s good to have an understanding of the different components of an Airflow task. The <a href=https://airflow.apache.org/docs/apache-airflow/stable/concepts/tasks.html>Airflow Tasks documentation</a> covers two of the important aspects:</p><ul><li>Operators, predefined task templates to build DAGs</li><li>Sensors, a subclass of Operators that wait on external services</li></ul><p><a href=https://airflow.apache.org/docs/apache-airflow/stable/concepts/connections.html#hooks>Hooks</a> are also important in that they are the main interface to external services and often the building blocks that Operators are built out of.</p><p>All that said, in Airflow 1.0, <a href=https://airflow.apache.org/docs/apache-airflow/stable/plugins.html>Plugins</a> were the primary way to integrate external features. That&rsquo;s changed in 2.0, and now there are sets of <a href=https://airflow.apache.org/docs/apache-airflow/stable/extra-packages-ref.html#providers-extras>Provider Packages</a> that provide pip-installable packages for integrating with different providers. This includes cloud providers like AWS and GCP, as well as different APIs like Discord, Salesforce, and Slack. The <a href=https://airflow.apache.org/docs/apache-airflow/stable/howto/custom-operator.html>custom operators</a> documentation is helpful, but it only discusses creating the operator - not how to test it, add documentation, update a provider package.</p><p>So&mldr;üòÖ once you have an understanding of how to add a new provider package and how it integrates, let&rsquo;s go over the steps we need to take to add a new plugin.</p><ol><li>Add the Plugin Operator/Hook/Sensor/etc</li><li>Add tests(!!) for your new code</li><li>Create an example DAG</li><li>Add documentation in on how to use the Operator</li><li>Update the various sections of the provider.yaml</li><li>linting, checks, and linting again</li></ol><p><em>The official Airflow docs on <a href=http://airflow.apache.org/docs/apache-airflow-providers/howto/create-update-providers.html>Community Providers</a> are also very helpful.</em></p><h2 id=creating-your-new-operator>Creating your new operator<a hidden class=anchor aria-hidden=true href=#creating-your-new-operator>#</a></h2><p>All provider packages live in the <a href=https://github.com/apache/airflow/tree/main/airflow/providers><code>airflow/providers</code></a> subtree of the git repository.</p><p><img loading=lazy src=provider_listing.png alt="List of providers"></p><p>If you look in each provider directory, you&rsquo;ll see various directories including <code>hooks</code>, <code>operators</code>, and <code>sensors</code>. These provide some good examples of how to create your Operator.</p><p>For now, I&rsquo;m going to create a new <code>emr_containers.py</code> file in each of the <code>hooks</code>, <code>operators</code>, and <code>sensors</code> directories. We&rsquo;ll be creating a new Hook for connecting to the <a href=https://docs.aws.amazon.com/emr-on-eks/latest/APIReference/Welcome.html>EMR on EKS API</a>, a new Sensor for waiting on jobs to complete, and an Operator that can be used to trigger your EMR on EKS jobs.</p><p>I won&rsquo;t go over the implementation details here, but you can take a look at each file in the Airflow repository.</p><p>One thing that was confusing to me during this process is that all three of those files have the same name&mldr;so at a glance, it was tough for me to know which component I was editing. But if you can keep this diagram in your head, it&rsquo;s pretty helpful.</p><p><img loading=lazy src=emr_containers_workflow.png alt></p><p>Note that there is no <code>EMRContainerSensor</code> in this workflow - that&rsquo;s because the default operator handles polling/waiting for the job to complete itself.</p><h2 id=testing>Testing<a hidden class=anchor aria-hidden=true href=#testing>#</a></h2><p>Similar to the provider packages, tests for the provider packages live in the <a href=https://github.com/apache/airflow/tree/main/tests/providers><code>tests/providers</code></a> subtree.</p><p>With the AWS packages, many plugins use the <a href=https://github.com/spulec/moto>moto</a> library for testing, an AWS service mocking library. EMR on EKS is a fairly recent addition, so it&rsquo;s unfortunately not part of the mocking library. Instead, I used the standard <code>mock</code> library to return sample values from the API.</p><p>The tests are fairly standard unit tests, but what gets challenging is figuring how to actually <em>RUN</em> these tests. Airflow is a monorepo - there are many benefits and challenges to this approach, but what it means for us is we have to figure out how to run the whole smorgosboard of this project.</p><p>Luckily(!) Airflow has a cool CI environment known as <a href=https://github.com/apache/airflow/blob/main/BREEZE.rst>Breeze</a> that can pretty much do whatever you need to make sure your new plugin is working well!</p><h3 id=up-and-running-with-breeze>Up and running with Breeze<a hidden class=anchor aria-hidden=true href=#up-and-running-with-breeze>#</a></h3><p>The main challenge I had with Breeze was resource consumption. üôÅ</p><p><a href=https://github.com/apache/airflow/blob/main/BREEZE.rst#resources-required>Breeze requires</a> a minimum of 4GB RAM for Docker and 40GB of free disk space. I had to tweak both of those settings on my mac, but even so I think Breeze is named for the winds that are kicked up by laptop fans everywhere when it starts. üòÜ</p><p>In any case, you <em>should</em> be able to just type <code>./breeze</code> and be dropped into an Airflow shell after it builds a local version of the necessary Docker images.</p><h3 id=unit-tests>Unit tests<a hidden class=anchor aria-hidden=true href=#unit-tests>#</a></h3><p>Once in the Airflow shell, you should be able to run any of your unit tests.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>python -m pytest tests/providers/amazon/aws/operators/test_emr_containers.py
</code></pre></div><p>If you want, you can also run your tests with the <code>./breeze</code> CLI.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>./breeze tests tests/providers/amazon/aws/sensors/test_emr_containers.py tests/providers/amazon/aws/operators/test_emr_containers.py tests/providers/amazon/aws/hooks/test_emr_containers.py
</code></pre></div><p>Now, let&rsquo;s make sure we have an example DAG to run.</p><h2 id=integration-testing>Integration Testing<a hidden class=anchor aria-hidden=true href=#integration-testing>#</a></h2><h3 id=example-dags>Example DAGs<a hidden class=anchor aria-hidden=true href=#example-dags>#</a></h3><p>It&rsquo;s crucial to provide an example DAG to show folks how to use your new Operator.</p><p>It&rsquo;s <em>also</em> crucial to have an example DAG so you can make sure your Operator works!</p><p>For AWS, example DAGs live in <code>airflow/providers/amazon/aws/example_dags</code>. For testing, however, you&rsquo;ll need to copy or link your DAG into <code>files/dags</code>. When you run <code>breeze</code>, it will mount that directory and the DAG will be available to Airflow.</p><h3 id=build-your-provider-package>Build your provider package<a hidden class=anchor aria-hidden=true href=#build-your-provider-package>#</a></h3><p>First, we need to build a local version of our provider package with the <code>prepare-provider-packages</code> command.</p><p><em>You may receive an error like <code>The tag providers-amazon/2.0.0 exists. Not preparing the package.</code> - if you do, you&rsquo;ll need to provide a suffix to the <code>prepare-provider-packages</code> command with the <code>-S</code> flag.</em></p><p>Then you can hop into Airflow</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>./breeze prepare-provider-packages amazon -S dev
./breeze start-airflow --use-airflow-version wheel --use-packages-from-dist
</code></pre></div><p>If you run <code>breeze</code> without the <code>start-airflow</code>, you&rsquo;ll just get dropped into a bash prompt and need to start the webserver and scheduler manually. I recommend just letting Breeze take care of that.</p><p><img loading=lazy src=breeze_shell.png alt></p><p>Once in the shell, you may need to create a user to be able to login to the Airflow UI.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>airflow users create --role Admin --username admin --password admin --email admin@example.com --firstname foo --lastname bar
</code></pre></div><p>By default, port 28080 gets forward to Airflow so you should be able to browse to <a href=http://localhost:28080>http://localhost:28080</a> and login as <code>admin</code>/<code>admin</code>.</p><p><img loading=lazy src=airflow_dag_ui.png alt></p><h3 id=run-your-example-dag>Run your example DAG<a hidden class=anchor aria-hidden=true href=#run-your-example-dag>#</a></h3><p>In theory, you can Unpause your DAG in the UI and run it, but it&rsquo;s likely you need some environment variables.</p><p>With AWS, you&rsquo;ll need to define access credentials and region. You can create a new Connection to do this, or just do so in your Airflow shell.</p><p>Since we&rsquo;re working localy, we&rsquo;ll use temporary access keys, but in a production environment your Airflow workers should use <a href=https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html>IAM roles</a>.</p><p><em>The following commands will all be executed in the <code>breeze</code> shell.</em></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>export AWS_ACCESS_KEY_ID<span style=color:#f92672>=</span>AKIAIOSFODNN7EXAMPLE
export AWS_SECRET_ACCESS_KEY<span style=color:#f92672>=</span>wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
export AWS_DEFAULT_REGION<span style=color:#f92672>=</span>us-east-1
</code></pre></div><p>Next, we&rsquo;ll try to run our example DAG using <code>airflow dags test</code>!</p><p>Our script requires the EMR on EKS virtual cluster ID and job role ARN, so we&rsquo;ll supply those as environment variables.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>VIRTUAL_CLUSTER_ID<span style=color:#f92672>=</span>abcdefghijklmno0123456789 <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>JOB_ROLE_ARN<span style=color:#f92672>=</span>arn:aws:iam::111122223333:role/emr_eks_default_role <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>airflow dags test emr_eks_pi_job <span style=color:#66d9ef>$(</span>date -Is<span style=color:#66d9ef>)</span>
</code></pre></div><p>You should see the job spin up and display logs in your console.</p><p><img loading=lazy src=emr_eks_dag_run.png alt></p><p>And if you refresh the Airflow UI you should see a successful run! If not&mldr;it&rsquo;s time to debug.</p><h2 id=documentation>Documentation<a hidden class=anchor aria-hidden=true href=#documentation>#</a></h2><p>OK! So&mldr;you&rsquo;ve built your new Operator. You&rsquo;ve written (and run) all your tests. Now it&rsquo;s time to help other folks use it!</p><p>The documentation was also a little bit tricky for me as it&rsquo;s in <a href=https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html>reStructuredText</a> format. I typically write in Markdown, so reST was a little foreign.</p><p>Fortunately, you can use <code>./breeze build-docs -- --package-filter apache-airflow-providers-amazon</code> to build the docs for a specific package.</p><p>Once you do that, the docs will be available in <code>docs/_build/docs/apache-airflow-providers-amazon/latest/index.html</code>. The links may not always work if you just open the file locally, but you should be able to make sure everything looks OK.</p><p>One awesome feature of reST is the ability to import snippets from other files and the Airflow docs make extensive use of this. For example, in my example DAG I have the following code:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># [START howto_operator_emr_eks_env_variables]</span>
VIRTUAL_CLUSTER_ID <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;VIRTUAL_CLUSTER_ID&#34;</span>, <span style=color:#e6db74>&#34;test-cluster&#34;</span>)
JOB_ROLE_ARN <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;JOB_ROLE_ARN&#34;</span>, <span style=color:#e6db74>&#34;arn:aws:iam::012345678912:role/emr_eks_default_role&#34;</span>)
<span style=color:#75715e># [END howto_operator_emr_eks_env_variables]</span>
</code></pre></div><p>Note the <code>START</code> and <code>END</code> blocks. With those in there, I can include that snippet in my docs like so:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-rest data-lang=rest>.. <span style=color:#f92672>exampleinclude</span>:: /../../airflow/providers/amazon/aws/example_dags/example_emr_eks_job.py<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>    <span style=color:#a6e22e>:language:</span> <span style=color:#a6e22e>python</span><span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>    <span style=color:#a6e22e>:start-after:</span> <span style=color:#a6e22e>[START howto_operator_emr_eks_env_variables]</span><span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>    <span style=color:#a6e22e>:end-before:</span> <span style=color:#a6e22e>[END howto_operator_emr_eks_env_variables]</span><span style=color:#960050;background-color:#1e0010>
</span></code></pre></div><p>And then it&rsquo;ll show up in the docs like this - pretty sweet!</p><p><img loading=lazy src=docs_import.png alt></p><h3 id=provideryaml><code>provider.yaml</code><a hidden class=anchor aria-hidden=true href=#provideryaml>#</a></h3><p>If you want your new Operator linked in the official provider docs, make sure to also update <code>provider.yaml</code> in the relevant provider.</p><h2 id=merging>Merging<a hidden class=anchor aria-hidden=true href=#merging>#</a></h2><p>Now the <del>hard</del> easy part&mldr;getting your contribution merged in!</p><p>The official Airflow docs have a great section on the <a href=https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#contribution-workflow>contribution workflow</a>.</p><p>I think the main thing I struggled with were all the PR checks that happen automatically.</p><p>I couldn&rsquo;t figure out how to run them locally (and didn&rsquo;t learn about <a href=https://github.com/apache/airflow/blob/main/STATIC_CODE_CHECKS.rst#id2><code>pre-commit</code></a> until after this process), so a lot of my workflow went like:</p><ul><li>commit locally</li><li>git push</li><li>wait for checks to fail</li><li>dive into GitHub Actions output to see what failed</li><li>fix locally</li><li>GOTO start</li></ul><p>I learned that you can use <code>breeze static-checks</code> to run a subset of the static checks locally. This is pretty helpful too if you want to avoid too many <code>git push</code>es.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>./breeze static-check all -- --files airflow/providers/amazon/aws/*/emr_containers.py 
</code></pre></div><p>That said, I&rsquo;m <em>very</em> happy there are so many checks in this project. There are a lot of things I didn&rsquo;t know about until the checks ran (like the <code>spelling_wordlist.txt</code> file) and it&rsquo;s great to have such a high level of automation to help contributors maintain the quality of their code.</p><h2 id=wrapup>Wrapup<a hidden class=anchor aria-hidden=true href=#wrapup>#</a></h2><p>I want to send a quick shoutout to the folks in the <a href=https://apache-airflow-slack.herokuapp.com/>Airflow Slack community</a> - they&rsquo;re all super nice and welcoming. And especially the folks that reviewed my PR, who were kind enough to not make <em>too</em> much of my <code>BEEP BOOP</code> error message I had committed with my initial revision. ü§ñ üòÜ</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://dacort.dev/tags/airflow/>airflow</a></li><li><a href=https://dacort.dev/tags/emr/>emr</a></li><li><a href=https://dacort.dev/tags/eks/>eks</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Building and Testing a new Apache Airflow Plugin on twitter" href="https://twitter.com/intent/tweet/?text=Building%20and%20Testing%20a%20new%20Apache%20Airflow%20Plugin&url=https%3a%2f%2fdacort.dev%2fposts%2fbuilding-and-testing-a-new-apache-airflow-plugin%2f&hashtags=airflow%2cemr%2ceks"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Building and Testing a new Apache Airflow Plugin on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fdacort.dev%2fposts%2fbuilding-and-testing-a-new-apache-airflow-plugin%2f&title=Building%20and%20Testing%20a%20new%20Apache%20Airflow%20Plugin&summary=Building%20and%20Testing%20a%20new%20Apache%20Airflow%20Plugin&source=https%3a%2f%2fdacort.dev%2fposts%2fbuilding-and-testing-a-new-apache-airflow-plugin%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Building and Testing a new Apache Airflow Plugin on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fdacort.dev%2fposts%2fbuilding-and-testing-a-new-apache-airflow-plugin%2f&title=Building%20and%20Testing%20a%20new%20Apache%20Airflow%20Plugin"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Building and Testing a new Apache Airflow Plugin on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdacort.dev%2fposts%2fbuilding-and-testing-a-new-apache-airflow-plugin%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Building and Testing a new Apache Airflow Plugin on whatsapp" href="https://api.whatsapp.com/send?text=Building%20and%20Testing%20a%20new%20Apache%20Airflow%20Plugin%20-%20https%3a%2f%2fdacort.dev%2fposts%2fbuilding-and-testing-a-new-apache-airflow-plugin%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Building and Testing a new Apache Airflow Plugin on telegram" href="https://telegram.me/share/url?text=Building%20and%20Testing%20a%20new%20Apache%20Airflow%20Plugin&url=https%3a%2f%2fdacort.dev%2fposts%2fbuilding-and-testing-a-new-apache-airflow-plugin%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://dacort.dev/>Damon Cortesi</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script><script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script></body></html>