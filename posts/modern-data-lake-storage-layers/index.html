<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>An Introduction to Modern Data Lake Storage Layers | Damon Cortesi</title><meta name=keywords content="aws,apache,hudi,iceberg,deltalake"><meta name=description content="In recent years we&rsquo;ve seen a rise in new storage layers for data lakes. In 2017, Uber announced Hudi - an incremental processing framework for data pipelines. In 2018, Netflix introduced Iceberg - a new table format for managing extremely large cloud datasets. And in 2019, Databricks open-sourced Delta Lake - originally intended to bring ACID transactions to data lakes.

üìπ If you&rsquo;d like to watch a video that discusses the content of this post, I&rsquo;ve also recorded an overview here. Each relevant section below will also link to individual timestamps."><meta name=author content><link rel=canonical href=https://dacort.xyz/posts/modern-data-lake-storage-layers/><link crossorigin=anonymous href=/assets/css/stylesheet.23c6104d1a47983bd689d7323faf77e563c7aa24a54cf000dcd19fadce7b2348.css integrity="sha256-I8YQTRpHmDvWidcyP6935WPHqiSlTPAA3NGfrc57I0g=" rel="preload stylesheet" as=style><link rel=icon href=https://dacort.xyz/images/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://dacort.xyz/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://dacort.xyz/favicon-32x32.png><link rel=apple-touch-icon href=https://dacort.xyz/apple-touch-icon.png><link rel=mask-icon href=https://dacort.xyz/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://dacort.xyz/posts/modern-data-lake-storage-layers/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://dacort.xyz/posts/modern-data-lake-storage-layers/"><meta property="og:site_name" content="Damon Cortesi"><meta property="og:title" content="An Introduction to Modern Data Lake Storage Layers"><meta property="og:description" content="In recent years we‚Äôve seen a rise in new storage layers for data lakes. In 2017, Uber announced Hudi - an incremental processing framework for data pipelines. In 2018, Netflix introduced Iceberg - a new table format for managing extremely large cloud datasets. And in 2019, Databricks open-sourced Delta Lake - originally intended to bring ACID transactions to data lakes.
üìπ If you‚Äôd like to watch a video that discusses the content of this post, I‚Äôve also recorded an overview here. Each relevant section below will also link to individual timestamps."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-02-02T14:22:22-08:00"><meta property="article:modified_time" content="2022-02-02T14:22:22-08:00"><meta property="article:tag" content="Aws"><meta property="article:tag" content="Apache"><meta property="article:tag" content="Hudi"><meta property="article:tag" content="Iceberg"><meta property="article:tag" content="Deltalake"><meta property="og:image" content="https://dacort.xyz/posts/modern-data-lake-storage-layers/lake.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dacort.xyz/posts/modern-data-lake-storage-layers/lake.jpg"><meta name=twitter:title content="An Introduction to Modern Data Lake Storage Layers"><meta name=twitter:description content="In recent years we&rsquo;ve seen a rise in new storage layers for data lakes. In 2017, Uber announced Hudi - an incremental processing framework for data pipelines. In 2018, Netflix introduced Iceberg - a new table format for managing extremely large cloud datasets. And in 2019, Databricks open-sourced Delta Lake - originally intended to bring ACID transactions to data lakes.

üìπ If you&rsquo;d like to watch a video that discusses the content of this post, I&rsquo;ve also recorded an overview here. Each relevant section below will also link to individual timestamps."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://dacort.xyz/posts/"},{"@type":"ListItem","position":2,"name":"An Introduction to Modern Data Lake Storage Layers","item":"https://dacort.xyz/posts/modern-data-lake-storage-layers/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"An Introduction to Modern Data Lake Storage Layers","name":"An Introduction to Modern Data Lake Storage Layers","description":"In recent years we\u0026rsquo;ve seen a rise in new storage layers for data lakes. In 2017, Uber announced Hudi - an incremental processing framework for data pipelines. In 2018, Netflix introduced Iceberg - a new table format for managing extremely large cloud datasets. And in 2019, Databricks open-sourced Delta Lake - originally intended to bring ACID transactions to data lakes.\nüìπ If you\u0026rsquo;d like to watch a video that discusses the content of this post, I\u0026rsquo;ve also recorded an overview here. Each relevant section below will also link to individual timestamps.\n","keywords":["aws","apache","hudi","iceberg","deltalake"],"articleBody":"In recent years we‚Äôve seen a rise in new storage layers for data lakes. In 2017, Uber announced Hudi - an incremental processing framework for data pipelines. In 2018, Netflix introduced Iceberg - a new table format for managing extremely large cloud datasets. And in 2019, Databricks open-sourced Delta Lake - originally intended to bring ACID transactions to data lakes.\nüìπ If you‚Äôd like to watch a video that discusses the content of this post, I‚Äôve also recorded an overview here. Each relevant section below will also link to individual timestamps.\nThis post aims to introduce each of these engines and give some insight into how they function under the hood and some of the differences in each. While I‚Äôll summarize the findings here, you can also view my Jupyter notebooks for each in my modern-data-lake-storage-layers repository. We begin with basic operations of writing and updating datasets.\nOne thing to note about all of these frameworks is that each began with a different challenge they were solving for, but over time they have begun to converge on a common set of functionality. I should also note that I am learning about these frameworks as well - the comments here are neither authoritative or comprehensive. ü§ó\nApache Hudi üìπ Intro to Apache Hudi video\nApache Hudi (Hadoop Upsert Delete and Incremental) was originally designed as an incremental stream processing framework and was built to combine the benefits of stream and batch processing. Hudi can be used with Spark, Flink, Presto, Trino and Hive, but much of the original work was focused around Spark and that‚Äôs what I use for these examples. One of the other huge benefits of Hudi is the concept of a self-managed data layer. For example, Hudi can automatically perform asynchronous compaction to optimize data lakes and also supports multi-writer gaurantees. Hudi also offers flexibility in storage formats depending on read/write requirements and data size.\nFor Hudi, we create a simple Spark DataFrame partitioned by creation_date and write that to S3.\n# Create a DataFrame inputDF = spark.createDataFrame( [ (\"100\", \"2015-01-01\", \"2015-01-01T13:51:39.340396Z\"), (\"101\", \"2015-01-01\", \"2015-01-01T12:14:58.597216Z\"), (\"102\", \"2015-01-01\", \"2015-01-01T13:51:40.417052Z\"), (\"103\", \"2015-01-01\", \"2015-01-01T13:51:40.519832Z\"), (\"104\", \"2015-01-02\", \"2015-01-01T12:15:00.512679Z\"), (\"105\", \"2015-01-02\", \"2015-01-01T13:51:42.248818Z\"), ], [\"id\", \"creation_date\", \"last_update_time\"], ) # Specify common DataSourceWriteOptions in the single hudiOptions variable hudiOptions = { \"hoodie.table.name\": \"my_hudi_table\", \"hoodie.datasource.write.recordkey.field\": \"id\", \"hoodie.datasource.write.partitionpath.field\": \"creation_date\", \"hoodie.datasource.write.precombine.field\": \"last_update_time\", \"hoodie.datasource.hive_sync.enable\": \"true\", \"hoodie.datasource.hive_sync.table\": \"my_hudi_table\", \"hoodie.datasource.hive_sync.partition_fields\": \"creation_date\", \"hoodie.datasource.hive_sync.partition_extractor_class\": \"org.apache.hudi.hive.MultiPartKeysValueExtractor\", \"hoodie.index.type\": \"GLOBAL_BLOOM\", # This is required if we want to ensure we upsert a record, even if the partition changes \"hoodie.bloom.index.update.partition.path\": \"true\", # This is required to write the data into the new partition (defaults to false in 0.8.0, true in 0.9.0) } # Write a DataFrame as a Hudi dataset inputDF.write.format(\"org.apache.hudi\").option( \"hoodie.datasource.write.operation\", \"insert\" ).options(**hudiOptions).mode(\"overwrite\").save(f\"s3://{S3_BUCKET_NAME}/tmp/hudi/\") When we look at the file structure on S3, we see a few things:\nA hoodie.properties file 2022-01-14 00:33:46 503 tmp/hudi/.hoodie/hoodie.properties This file contains certain metadata about the Hudi dataset:\n#Properties saved on Fri Jan 14 00:33:45 UTC 2022 #Fri Jan 14 00:33:45 UTC 2022 hoodie.table.precombine.field=last_update_time hoodie.table.partition.fields=creation_date hoodie.table.type=COPY_ON_WRITE hoodie.archivelog.folder=archived hoodie.populate.meta.fields=true hoodie.timeline.layout.version=1 hoodie.table.version=2 hoodie.table.recordkey.fields=id hoodie.table.base.file.format=PARQUET hoodie.table.keygenerator.class=org.apache.hudi.keygen.SimpleKeyGenerator hoodie.table.name=my_hudi_table A set of commit-related files 2022-01-14 00:33:57 2706 tmp/hudi/.hoodie/20220114003341.commit 2022-01-14 00:33:48 0 tmp/hudi/.hoodie/20220114003341.commit.requested 2022-01-14 00:33:52 1842 tmp/hudi/.hoodie/20220114003341.inflight The actual .parquet data files and associated metadata organized into date-based partitions. 2022-01-14 00:33:54 93 tmp/hudi/2015-01-01/.hoodie_partition_metadata 2022-01-14 00:33:54 434974 tmp/hudi/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet 2022-01-14 00:33:55 93 tmp/hudi/2015-01-02/.hoodie_partition_metadata 2022-01-14 00:33:55 434943 tmp/hudi/2015-01-02/43051d12-87e7-4dfb-8201-6ce293cf0df7-0_1-6-99_20220114003341.parquet We then update the creation_date of one row in this dataset.\nfrom pyspark.sql.functions import lit # Create a new DataFrame from the first row of inputDF with a different creation_date value updateDF = inputDF.where(\"id = 100\").withColumn(\"creation_date\", lit(\"2022-01-11\")) updateDF.show() # Update by using the \"upsert\" operation updateDF.write.format(\"org.apache.hudi\").option( \"hoodie.datasource.write.operation\", \"upsert\" ).options(**hudiOptions).mode(\"append\").save(f\"s3://{S3_BUCKET_NAME}/tmp/hudi/\") One thing to note here is that since we‚Äôre updating a partition value (DANGER!), we had to set the hoodie.index.type to GLOBAL_BLOOM as well as setting hoodie.bloom.index.update.partition.path to true. This can have a large impact on performance so normally we would try not to change a partition value in a production environment, but it‚Äôs useful here to see the impact it has. You can mind more details in the Hudi FAQ about Hudi indexing.\nAfter this write, we have a new set of commit-related files on S3:\n2022-01-14 00:34:15 2706 tmp/hudi/.hoodie/20220114003401.commit 2022-01-14 00:34:03 0 tmp/hudi/.hoodie/20220114003401.commit.requested 2022-01-14 00:34:08 2560 tmp/hudi/.hoodie/20220114003401.inflight And we actually have 2 new .parquet files:\n2022-01-14 00:34:12 434925 tmp/hudi/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-37-13680_20220114003401.parquet ... 2022-01-14 00:34:13 93 tmp/hudi/2022-01-11/.hoodie_partition_metadata 2022-01-14 00:34:14 434979 tmp/hudi/2022-01-11/0c210872-484e-428b-a9ca-90a26e42125c-0_1-43-13681_20220114003401.parquet So what happened with the update is that the old partition (2015-01-01) had its data overwritten and the new partition (2022-01-11) also had data written to it. You can now see why the global bloom index could have such a large impact on write performance as there is significant potential for write amplication.\nIf we query the data and add the source filename for each row, we can also see that data for the old partition now comes from the new parquet file (notice the commit ID 20220114003401 shows up in the filename):\nfrom pyspark.sql.functions import input_file_name snapshotQueryDF = spark.read \\ .format('org.apache.hudi') \\ .load(f\"s3://{S3_BUCKET_NAME}/tmp/hudi/\") \\ .select('id', 'creation_date') \\ .withColumn(\"filename\", input_file_name()) snapshotQueryDF.show(truncate=False) +---+-------------+------------------------------------------------------------------------------------------------------------------------------+ |id |creation_date|filename | +---+-------------+------------------------------------------------------------------------------------------------------------------------------+ |100|2022-01-11 |/hudi/2022-01-11/0c210872-484e-428b-a9ca-90a26e42125c-0_1-43-13681_20220114003401.parquet | |105|2015-01-02 |/hudi/2015-01-02/43051d12-87e7-4dfb-8201-6ce293cf0df7-0_1-6-99_20220114003341.parquet | |104|2015-01-02 |/hudi/2015-01-02/43051d12-87e7-4dfb-8201-6ce293cf0df7-0_1-6-99_20220114003341.parquet | |102|2015-01-01 |/hudi/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-37-13680_20220114003401.parquet | |103|2015-01-01 |/hudi/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-37-13680_20220114003401.parquet | |101|2015-01-01 |/hudi/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-37-13680_20220114003401.parquet | +---+-------------+------------------------------------------------------------------------------------------------------------------------------+ One other thing to note is that Hudi adds quite a bit of metadata to your Parquet files. This data helps enable record-level change streams - more detail can be found in this comprehensive blog post about the Hudi platform. If we use native Spark to read one of the Parquet files and show it, we see that there‚Äôs various _hoodie-prefixed keys.\nfrom pyspark.sql.functions import split rawDF = ( spark.read.parquet(f\"s3://{S3_BUCKET_NAME}/tmp/hudi/*/*.parquet\") .withColumn(\"filename\", split(input_file_name(), \"tmp/hudi\").getItem(1)) .sort(\"_hoodie_commit_time\", \"_hoodie_commit_seqno\") ) rawDF.show(truncate=False) +-------------------+--------------------+------------------+----------------------+------------------------------------------------------------------------+---+-------------+---------------------------+------------------------------------------------------------------------------------+ |_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|_hoodie_file_name |id |creation_date|last_update_time |filename | +-------------------+--------------------+------------------+----------------------+------------------------------------------------------------------------+---+-------------+---------------------------+------------------------------------------------------------------------------------+ |20220114003341 |20220114003341_0_1 |100 |2015-01-01 |57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet |100|2015-01-01 |2015-01-01T13:51:39.340396Z|/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet | |20220114003341 |20220114003341_0_2 |102 |2015-01-01 |57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet |102|2015-01-01 |2015-01-01T13:51:40.417052Z|/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-37-13680_20220114003401.parquet| |20220114003341 |20220114003341_0_2 |102 |2015-01-01 |57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet |102|2015-01-01 |2015-01-01T13:51:40.417052Z|/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet | |20220114003341 |20220114003341_0_3 |103 |2015-01-01 |57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet |103|2015-01-01 |2015-01-01T13:51:40.519832Z|/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-37-13680_20220114003401.parquet| |20220114003341 |20220114003341_0_3 |103 |2015-01-01 |57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet |103|2015-01-01 |2015-01-01T13:51:40.519832Z|/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet | |20220114003341 |20220114003341_0_4 |101 |2015-01-01 |57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet |101|2015-01-01 |2015-01-01T12:14:58.597216Z|/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-37-13680_20220114003401.parquet| |20220114003341 |20220114003341_0_4 |101 |2015-01-01 |57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet |101|2015-01-01 |2015-01-01T12:14:58.597216Z|/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet | |20220114003341 |20220114003341_1_5 |105 |2015-01-02 |43051d12-87e7-4dfb-8201-6ce293cf0df7-0_1-6-99_20220114003341.parquet |105|2015-01-02 |2015-01-01T13:51:42.248818Z|/2015-01-02/43051d12-87e7-4dfb-8201-6ce293cf0df7-0_1-6-99_20220114003341.parquet | |20220114003341 |20220114003341_1_6 |104 |2015-01-02 |43051d12-87e7-4dfb-8201-6ce293cf0df7-0_1-6-99_20220114003341.parquet |104|2015-01-02 |2015-01-01T12:15:00.512679Z|/2015-01-02/43051d12-87e7-4dfb-8201-6ce293cf0df7-0_1-6-99_20220114003341.parquet | |20220114003401 |20220114003401_1_1 |100 |2022-01-11 |0c210872-484e-428b-a9ca-90a26e42125c-0_1-43-13681_20220114003401.parquet|100|2022-01-11 |2015-01-01T13:51:39.340396Z|/2022-01-11/0c210872-484e-428b-a9ca-90a26e42125c-0_1-43-13681_20220114003401.parquet| +-------------------+--------------------+------------------+----------------------+------------------------------------------------------------------------+---+-------------+---------------------------+------------------------------------------------------------------------------------+ In the background, Hudi figures out which commits and values to show based on the commit files and metadata in the parquet files.\nApache Iceberg üìπ Intro to Apache Iceberg video\nWhen I first heard about Iceberg, the phrase ‚Äútable format for storing large, slow-moving tabular data‚Äù didn‚Äôt really make sense to me. But after working with data lakes at scale, it became quite clear. Apache Hive is a popular data warehouse project that provides a SQL-like interface to large datasets. Built on top of Hadoop, it originally used HDFS as its data store. With cloud migrations, object stores like Amazon S3 enabled the ability to store even more data particularly without the operational concerns of a large Hadoop cluster, but with some limitations when compared to HDFS. Specifically, directory listings are slower (simple physics here, network calls are slower), renames are not atomic (by design), and results were previously eventually consistent.\nSo imagine you are Netflix, you have hundreds of petabytes of data stored on S3, and you need a way for your organization to efficiently query this. You need a data storage layer that reduces or removes directory listings, you want atomic changes, and you want to ensure that when you‚Äôre reading your data you get consistent results. There is more to Iceberg, but I‚Äôm simplifying because this helped me understand. :)\nThese were some of the original goals for Iceberg, so let‚Äôs dive in and see how it works. Similar to Hudi, we‚Äôll create a simple Spark DataFrame and write that to S3 in Iceberg format.\nI should note that much of Iceberg is focused around Spark SQL, so I will switch to that below for certain operations.\n# Create a DataFrame inputDF = spark.createDataFrame( [ (\"100\", \"2015-01-01\", \"2015-01-01T13:51:39.340396Z\"), (\"101\", \"2015-01-01\", \"2015-01-01T12:14:58.597216Z\"), (\"102\", \"2015-01-01\", \"2015-01-01T13:51:40.417052Z\"), (\"103\", \"2015-01-01\", \"2015-01-01T13:51:40.519832Z\"), (\"104\", \"2015-01-02\", \"2015-01-01T12:15:00.512679Z\"), (\"105\", \"2015-01-02\", \"2015-01-01T13:51:42.248818Z\"), ], [\"id\", \"creation_date\", \"last_update_time\"], ) # Write a DataFrame as an Iceberg dataset inputDF.write.format(\"iceberg\").mode(\"overwrite\").partitionBy(\"creation_date\").option( \"path\", f\"s3://{S3_BUCKET_NAME}/tmp/iceberg/\" ).saveAsTable(ICEBERG_TABLE_NAME) There are two main differences here - there is not as much ‚Äúconfiguration‚Äù as we had to do with Hudi and we also explicitly use saveAsTable. With Iceberg, much of the metadata is stored in a data catalog so creating the table is necessary. Let‚Äôs see what happened on S3.\nFirst, we have a metadata.json file 2022-01-28 06:03:50 2457 tmp/iceberg/metadata/00000-bb1d38a9-af77-42c4-a7b7-69416fe36d9c.metadata.json Then a snapshot manifest list file 2022-01-28 06:03:50 3785 tmp/iceberg/metadata/snap-7934053180928033536-1-e79c79ba-c7f0-45ad-8f2e-fd1bc349db55.avro And a manifest file 2022-01-28 06:03:50 6244 tmp/iceberg/metadata/e79c79ba-c7f0-45ad-8f2e-fd1bc349db55-m0.avro And finally, we‚Äôve got our Parquet data files 2022-01-28 06:03:49 1197 tmp/iceberg/data/creation_date=2015-01-01/00000-4-fa9a18fd-abc4-4e04-91b4-e2ac4c9531be-00001.parquet 2022-01-28 06:03:49 1171 tmp/iceberg/data/creation_date=2015-01-01/00001-5-eab30115-a1d6-4918-abb4-a198ac12b262-00001.parquet 2022-01-28 06:03:50 1182 tmp/iceberg/data/creation_date=2015-01-02/00001-5-eab30115-a1d6-4918-abb4-a198ac12b262-00002.parquet There are a lot of moving pieces here, but the image from the Iceberg spec illustrates it quite well.\nSimilar to Hudi, our data is written to Parquet files in each partition, although Hive-style partitioning is used by default. Hudi can also do this by setting the hoodie.datasource.write.hive_style_partitioning parameter.\nDifferent from Hudi, though, is the default usage of the data catalog to identify the current metadata file to use. That metadata file contains references to a list of manifest files to use to determine which data files compose the dataset for that particular version, also known as snapshots. As of Hudi 0.7.0, it also supports a metadata table to reduce the performance impact of file listings. The snapshot data also has quite a bit of additional information. Let‚Äôs update our dataset then take a look at S3 again and the snapshot portion of the metadata file.\nspark.sql(f\"UPDATE {ICEBERG_TABLE_NAME} SET creation_date = '2022-01-11' WHERE id = 100\") We can see that we have:\n2 new .parquet data files 2022-01-28 06:07:07 1180 tmp/iceberg/data/creation_date=2015-01-01/00000-16-033354bd-7b02-44f4-95e2-7045e10706fc-00001.parquet 2022-01-28 06:07:08 1171 tmp/iceberg/data/creation_date=2022-01-11/00000-16-033354bd-7b02-44f4-95e2-7045e10706fc-00002.parquet As well as:\n1 new metadata.json file 2 new .avro metadata listings 1 new snap-*.avro snapshot file Let‚Äôs look at the snapshot portion of the metadata.json file.\n\"snapshots\": [ { \"manifest-list\": \"s3:///tmp/iceberg/metadata/snap-7934053180928033536-1-e79c79ba-c7f0-45ad-8f2e-fd1bc349db55.avro\", \"schema-id\": 0, \"snapshot-id\": 7934053180928033536, \"summary\": { \"added-data-files\": \"3\", \"added-files-size\": \"3550\", \"added-records\": \"6\", \"changed-partition-count\": \"2\", \"operation\": \"append\", \"spark.app.id\": \"application_1643153254969_0029\", \"total-data-files\": \"3\", \"total-delete-files\": \"0\", \"total-equality-deletes\": \"0\", \"total-files-size\": \"3550\", \"total-position-deletes\": \"0\", \"total-records\": \"6\" }, \"timestamp-ms\": 1643349829278 }, { \"manifest-list\": \"s3:///tmp/iceberg/metadata/snap-5441092870212826638-1-605de48f-8ccf-450c-935e-bbd4194ee8cc.avro\", \"parent-snapshot-id\": 7934053180928033536, \"schema-id\": 0, \"snapshot-id\": 5441092870212826638, \"summary\": { \"added-data-files\": \"2\", \"added-files-size\": \"2351\", \"added-records\": \"3\", \"changed-partition-count\": \"2\", \"deleted-data-files\": \"1\", \"deleted-records\": \"3\", \"operation\": \"overwrite\", \"removed-files-size\": \"1197\", \"spark.app.id\": \"application_1643153254969_0029\", \"total-data-files\": \"4\", \"total-delete-files\": \"0\", \"total-equality-deletes\": \"0\", \"total-files-size\": \"4704\", \"total-position-deletes\": \"0\", \"total-records\": \"6\" }, \"timestamp-ms\": 1643350027635 } ] This is pretty amazing - we see how many files and records were added or deleted, what the file sizes were, and even what the Spark app_id was! ü§Ø Some of this data is in the manifest-list files as well, but you can begin to see just how much you could potentially optimize your queries using this data.\nDelta Lake üìπ Intro to Delta Lake video\nDelta Lake was also introduced by Databricks as a way to address many of the challenges of Data Lakes. Similar to Hudi and Iceberg its goals include unifying batch and stream processing, ACID transactions, and scalable metadata handling among others.\nAgain, we‚Äôll create a simple Spark DataFrame and write it to S3 in Delta format.\n# Create a DataFrame inputDF = spark.createDataFrame( [ (\"100\", \"2015-01-01\", \"2015-01-01T13:51:39.340396Z\"), (\"101\", \"2015-01-01\", \"2015-01-01T12:14:58.597216Z\"), (\"102\", \"2015-01-01\", \"2015-01-01T13:51:40.417052Z\"), (\"103\", \"2015-01-01\", \"2015-01-01T13:51:40.519832Z\"), (\"104\", \"2015-01-02\", \"2015-01-01T12:15:00.512679Z\"), (\"105\", \"2015-01-02\", \"2015-01-01T13:51:42.248818Z\"), ], [\"id\", \"creation_date\", \"last_update_time\"], ) # Write a DataFrame as a Delta dataset inputDF.write.format(\"delta\").mode(\"overwrite\").option( \"overwriteSchema\", \"true\" ).partitionBy(\"creation_date\").save(f\"s3://{S3_BUCKET_NAME}/tmp/delta/\") On S3, we now see the following files:\na 00000000000000000000.json file 2022-01-24 22:57:54 2120 tmp/delta/_delta_log/00000000000000000000.json Several .snappy.parquet files 2022-01-24 22:57:52 875 tmp/delta/creation_date=2015-01-01/part-00005-2e09dbe4-469e-40dc-9b36-833480f6d375.c000.snappy.parquet 2022-01-24 22:57:52 875 tmp/delta/creation_date=2015-01-01/part-00010-848c69e1-71fb-4f8f-a19a-dd74e0ef1b8a.c000.snappy.parquet 2022-01-24 22:57:53 875 tmp/delta/creation_date=2015-01-01/part-00015-937d1837-0f03-4306-9b4e-4366207e688d.c000.snappy.parquet 2022-01-24 22:57:54 875 tmp/delta/creation_date=2015-01-01/part-00021-978a808e-4c36-4646-b7b1-ef5a21e706d8.c000.snappy.parquet 2022-01-24 22:57:54 875 tmp/delta/creation_date=2015-01-02/part-00026-538e1ac6-055e-4e72-9177-63daaaae1f98.c000.snappy.parquet 2022-01-24 22:57:52 875 tmp/delta/creation_date=2015-01-02/part-00031-8a03451a-0297-4c43-b64d-56db25807d02.c000.snappy.parquet OK, so what‚Äôs in that _delta_log file? Similar to Iceberg, quite a bit of information about this initial write to S3 including the number of files written, the schema of the dataset, and even the individual add operations for each file.\n{ \"commitInfo\": { \"timestamp\": 1643065073634, \"operation\": \"WRITE\", \"operationParameters\": { \"mode\": \"Overwrite\", \"partitionBy\": \"[\\\"creation_date\\\"]\" }, \"isBlindAppend\": false, \"operationMetrics\": { \"numFiles\": \"6\", \"numOutputBytes\": \"5250\", \"numOutputRows\": \"6\" } } } { \"protocol\": { \"minReaderVersion\": 1, \"minWriterVersion\": 2 } } { \"metaData\": { \"id\": \"a7f4b1d1-09f6-4475-894a-0eec90d1aab5\", \"format\": { \"provider\": \"parquet\", \"options\": {} }, \"schemaString\": \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"creation_date\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"last_update_time\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\", \"partitionColumns\": [ \"creation_date\" ], \"configuration\": {}, \"createdTime\": 1643065064066 } } { \"add\": { \"path\": \"creation_date=2015-01-01/part-00005-2e09dbe4-469e-40dc-9b36-833480f6d375.c000.snappy.parquet\", \"partitionValues\": { \"creation_date\": \"2015-01-01\" }, \"size\": 875, \"modificationTime\": 1643065072000, \"dataChange\": true } } { \"add\": { \"path\": \"creation_date=2015-01-01/part-00010-848c69e1-71fb-4f8f-a19a-dd74e0ef1b8a.c000.snappy.parquet\", \"partitionValues\": { \"creation_date\": \"2015-01-01\" }, \"size\": 875, \"modificationTime\": 1643065072000, \"dataChange\": true } } { \"add\": { \"path\": \"creation_date=2015-01-01/part-00015-937d1837-0f03-4306-9b4e-4366207e688d.c000.snappy.parquet\", \"partitionValues\": { \"creation_date\": \"2015-01-01\" }, \"size\": 875, \"modificationTime\": 1643065073000, \"dataChange\": true } } { \"add\": { \"path\": \"creation_date=2015-01-01/part-00021-978a808e-4c36-4646-b7b1-ef5a21e706d8.c000.snappy.parquet\", \"partitionValues\": { \"creation_date\": \"2015-01-01\" }, \"size\": 875, \"modificationTime\": 1643065074000, \"dataChange\": true } } { \"add\": { \"path\": \"creation_date=2015-01-02/part-00026-538e1ac6-055e-4e72-9177-63daaaae1f98.c000.snappy.parquet\", \"partitionValues\": { \"creation_date\": \"2015-01-02\" }, \"size\": 875, \"modificationTime\": 1643065074000, \"dataChange\": true } } { \"add\": { \"path\": \"creation_date=2015-01-02/part-00031-8a03451a-0297-4c43-b64d-56db25807d02.c000.snappy.parquet\", \"partitionValues\": { \"creation_date\": \"2015-01-02\" }, \"size\": 875, \"modificationTime\": 1643065072000, \"dataChange\": true } } Alright, let‚Äôs go ahead and update one of our rows. Delta Lake provides a merge operation that we can use. We‚Äôll use the syntax from the docs that‚Äôs slightly different from native Spark as it creates a DeltaTable object.\nfrom pyspark.sql.functions import lit # Create a new DataFrame from the first row of inputDF with a different creation_date value updateDF = inputDF.where(\"id = 100\").withColumn(\"creation_date\", lit(\"2022-01-11\")) from delta.tables import * from pyspark.sql.functions import * deltaTable = DeltaTable.forPath(spark, f\"s3://{S3_BUCKET_NAME}/tmp/delta/\") deltaTable.alias(\"oldData\") \\ .merge( updateDF.alias(\"newData\"), \"oldData.id = newData.id\") \\ .whenMatchedUpdate(set = { \"creation_date\": col(\"newData.creation_date\") }) \\ .execute() Interestingly, now when we look at S3 we see 1 new json file and only 1 new parquet file (Remember Hudi and Iceberg both had 2 new parquet files).\n2022-01-24 23:05:46 1018 tmp/delta/_delta_log/00000000000000000001.json 2022-01-24 23:05:46 875 tmp/delta/creation_date=2022-01-11/part-00000-3f3fd83a-b876-4b6f-8f64-d8a4189392ae.c000.snappy.parquet If we look at that new JSON file we see something really interesting:\n{ \"commitInfo\": { \"timestamp\": 1643065545396, \"operation\": \"MERGE\", \"operationParameters\": { \"predicate\": \"(oldData.`id` = newData.`id`)\", \"matchedPredicates\": \"[{\\\"actionType\\\":\\\"update\\\"}]\", \"notMatchedPredicates\": \"[]\" }, \"readVersion\": 0, \"isBlindAppend\": false, \"operationMetrics\": { \"numTargetRowsCopied\": \"0\", \"numTargetRowsDeleted\": \"0\", \"numTargetFilesAdded\": \"1\", \"executionTimeMs\": \"4705\", \"numTargetRowsInserted\": \"0\", \"scanTimeMs\": \"3399\", \"numTargetRowsUpdated\": \"1\", \"numOutputRows\": \"1\", \"numSourceRows\": \"1\", \"numTargetFilesRemoved\": \"1\", \"rewriteTimeMs\": \"1265\" } } } { \"remove\": { \"path\": \"creation_date=2015-01-01/part-00005-2e09dbe4-469e-40dc-9b36-833480f6d375.c000.snappy.parquet\", \"deletionTimestamp\": 1643065545378, \"dataChange\": true, \"extendedFileMetadata\": true, \"partitionValues\": { \"creation_date\": \"2015-01-01\" }, \"size\": 875 } } { \"add\": { \"path\": \"creation_date=2022-01-11/part-00000-3f3fd83a-b876-4b6f-8f64-d8a4189392ae.c000.snappy.parquet\", \"partitionValues\": { \"creation_date\": \"2022-01-11\" }, \"size\": 875, \"modificationTime\": 1643065546000, \"dataChange\": true } } In addition to the operationMetrics that gives us insight into how the data changed on ‚Äúdisk‚Äù, we also now see both a remove and add operation. In Delta Lake (and I‚Äôm not quite sure why this happened yet‚Ä¶), each row was written to an individual .parquet file! So for this second version of the data, the fact that that row was updated simply lives in the metadata because it was the only row stored in that Parquet file. I‚Äôm guessing this is simply because my dataset is so small the default number of partitions in Spark/Delta Lake resulted in this write configuration.\nSnapshots So now we‚Äôve got a good idea of the semantics of each of these storage layers. Let‚Äôs take one more look at an important component of all of them and that‚Äôs snapshots!\nHudi Hudi has a concept of ‚Äúpoint-in-time‚Äù queries where you provide it a range of two commit timestamps and it will show you what the data looked like at that point in time.\n# Query data from the first version of the table readOptions = { 'hoodie.datasource.query.type': 'incremental', 'hoodie.datasource.read.begin.instanttime': '0', 'hoodie.datasource.read.end.instanttime': '20220114003341', } incQueryDF = spark.read \\ .format('org.apache.hudi') \\ .options(**readOptions) \\ .load(f\"s3://{S3_BUCKET_NAME}/tmp/hudi\") incQueryDF.show() Iceberg Iceberg supports a similar mechanism called time travel and you can use either a snapshot-id or as-of-timestamp similar to Hudi.\n# time travel to 2022-01-27 22:04:00 -0800 df = spark.read \\ .option(\"as-of-timestamp\", \"1643349840000\") \\ .format(\"iceberg\") \\ .load(ICEBERG_TABLE_NAME) df.show() Delta Lake And, of course, Delta Lake supports this as well using either Spark SQL or DataFrames. And similar to Iceberg you can use versionAsOf or timestampAsOf.\n# time travel to 2022-01-24 23:00 df1 = ( spark.read.format(\"delta\") .option(\"timestampAsOf\", \"2022-01-24 23:00\") .load(f\"s3://{S3_BUCKET_NAME}/tmp/delta/\") ) Deletes I bet you‚Äôre surprised I haven‚Äôt mentioned deletes or GDPR yet. Don‚Äôt worry‚Ä¶I will. üòÄ But first I just wanted to understand exactly how these different systems work.\nWrapup In this post, we reviewed the basics of Apache Hudi, Apache Iceberg, and Delta Lake - modern data lake storage layers. All these frameworks enable a set of functionality that optimize working with data in cloud-based object stores, albeit with slightly different approaches.\n","wordCount":"2718","inLanguage":"en","image":"https://dacort.xyz/posts/modern-data-lake-storage-layers/lake.jpg","datePublished":"2022-02-02T14:22:22-08:00","dateModified":"2022-02-02T14:22:22-08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://dacort.xyz/posts/modern-data-lake-storage-layers/"},"publisher":{"@type":"Organization","name":"Damon Cortesi","logo":{"@type":"ImageObject","url":"https://dacort.xyz/images/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://dacort.xyz/ accesskey=h title="Damon Cortesi (Alt + H)">Damon Cortesi</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://dacort.xyz/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://dacort.xyz/deep-thoughts-by-ai/ title="Deep Thoughts"><span>Deep Thoughts</span></a></li><li><a href=https://dacort.xyz/posts/ title=Blog><span>Blog</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://dacort.xyz/>Home</a>&nbsp;¬ª&nbsp;<a href=https://dacort.xyz/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">An Introduction to Modern Data Lake Storage Layers</h1><div class=post-meta><span title='2022-02-02 14:22:22 -0800 -0800'>February 2, 2022</span>&nbsp;¬∑&nbsp;<span>13 min</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#apache-hudi aria-label="Apache Hudi">Apache Hudi</a></li><li><a href=#apache-iceberg aria-label="Apache Iceberg">Apache Iceberg</a></li><li><a href=#delta-lake aria-label="Delta Lake">Delta Lake</a></li><li><a href=#snapshots aria-label=Snapshots>Snapshots</a><ul><li><a href=#hudi aria-label=Hudi>Hudi</a></li><li><a href=#iceberg aria-label=Iceberg>Iceberg</a></li><li><a href=#delta-lake-1 aria-label="Delta Lake">Delta Lake</a></li></ul></li><li><a href=#deletes aria-label=Deletes>Deletes</a></li><li><a href=#wrapup aria-label=Wrapup>Wrapup</a></li></ul></div></details></div><div class=post-content><p>In recent years we&rsquo;ve seen a rise in new storage layers for data lakes. In 2017, <a href=https://eng.uber.com/hoodie/>Uber announced Hudi</a> - an incremental processing framework for data pipelines. In 2018, <a href=https://conferences.oreilly.com/strata/strata-ny-2018/public/schedule/detail/69503.html>Netflix introduced Iceberg</a> - a new table format for managing extremely large cloud datasets. And in 2019, <a href=https://techcrunch.com/2019/04/24/databricks-open-sources-delta-lake-to-make-data-lakes-more-reliable/>Databricks open-sourced Delta Lake</a> - originally intended to bring ACID transactions to data lakes.</p><blockquote><p>üìπ <em>If you&rsquo;d like to watch a video that discusses the content of this post, I&rsquo;ve also recorded <a href="https://www.youtube.com/watch?v=fryfx0Zg7KA">an overview here</a>. Each relevant section below will also link to individual timestamps.</em></p></blockquote><p>This post aims to introduce each of these engines and give some insight into how they function under the hood and some of the differences in each. While I&rsquo;ll summarize the findings here, you can also view my Jupyter notebooks for each in my <a href=https://github.com/dacort/modern-data-lake-storage-layers/tree/main/notebooks>modern-data-lake-storage-layers</a> repository. We begin with basic operations of writing and updating datasets.</p><p>One thing to note about all of these frameworks is that each began with a different challenge they were solving for, but over time they have begun to converge on a common set of functionality. I should <strong>also</strong> note that I am learning about these frameworks as well - the comments here are neither authoritative or comprehensive. ü§ó</p><h2 id=apache-hudi>Apache Hudi<a hidden class=anchor aria-hidden=true href=#apache-hudi>#</a></h2><blockquote><p>üìπ <a href="https://www.youtube.com/watch?v=fryfx0Zg7KA&amp;t=323s">Intro to Apache Hudi video</a></p></blockquote><p>Apache Hudi (Hadoop Upsert Delete and Incremental) was originally designed as an incremental stream processing framework and was built to combine the benefits of stream and batch processing. Hudi can be used with Spark, Flink, Presto, Trino and Hive, but much of the original work was focused around Spark and that&rsquo;s what I use for these examples. One of the other huge benefits of Hudi is the concept of a self-managed data layer. For example, Hudi can automatically perform <a href=https://hudi.apache.org/docs/compaction>asynchronous compaction</a> to optimize data lakes and also supports <a href=https://hudi.apache.org/docs/concurrency_control>multi-writer gaurantees</a>. Hudi also offers flexibility in storage formats depending on read/write requirements and data size.</p><p>For Hudi, we create a simple Spark DataFrame partitioned by <code>creation_date</code> and write that to S3.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Create a DataFrame</span>
</span></span><span style=display:flex><span>inputDF <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>createDataFrame(
</span></span><span style=display:flex><span>    [
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;100&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01T13:51:39.340396Z&#34;</span>),
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;101&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01T12:14:58.597216Z&#34;</span>),
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;102&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01T13:51:40.417052Z&#34;</span>),
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;103&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01T13:51:40.519832Z&#34;</span>),
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;104&#34;</span>, <span style=color:#e6db74>&#34;2015-01-02&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01T12:15:00.512679Z&#34;</span>),
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;105&#34;</span>, <span style=color:#e6db74>&#34;2015-01-02&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01T13:51:42.248818Z&#34;</span>),
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    [<span style=color:#e6db74>&#34;id&#34;</span>, <span style=color:#e6db74>&#34;creation_date&#34;</span>, <span style=color:#e6db74>&#34;last_update_time&#34;</span>],
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Specify common DataSourceWriteOptions in the single hudiOptions variable</span>
</span></span><span style=display:flex><span>hudiOptions <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;hoodie.table.name&#34;</span>: <span style=color:#e6db74>&#34;my_hudi_table&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;hoodie.datasource.write.recordkey.field&#34;</span>: <span style=color:#e6db74>&#34;id&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;hoodie.datasource.write.partitionpath.field&#34;</span>: <span style=color:#e6db74>&#34;creation_date&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;hoodie.datasource.write.precombine.field&#34;</span>: <span style=color:#e6db74>&#34;last_update_time&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;hoodie.datasource.hive_sync.enable&#34;</span>: <span style=color:#e6db74>&#34;true&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;hoodie.datasource.hive_sync.table&#34;</span>: <span style=color:#e6db74>&#34;my_hudi_table&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;hoodie.datasource.hive_sync.partition_fields&#34;</span>: <span style=color:#e6db74>&#34;creation_date&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;hoodie.datasource.hive_sync.partition_extractor_class&#34;</span>: <span style=color:#e6db74>&#34;org.apache.hudi.hive.MultiPartKeysValueExtractor&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;hoodie.index.type&#34;</span>: <span style=color:#e6db74>&#34;GLOBAL_BLOOM&#34;</span>,  <span style=color:#75715e># This is required if we want to ensure we upsert a record, even if the partition changes</span>
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;hoodie.bloom.index.update.partition.path&#34;</span>: <span style=color:#e6db74>&#34;true&#34;</span>,  <span style=color:#75715e># This is required to write the data into the new partition (defaults to false in 0.8.0, true in 0.9.0)</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Write a DataFrame as a Hudi dataset</span>
</span></span><span style=display:flex><span>inputDF<span style=color:#f92672>.</span>write<span style=color:#f92672>.</span>format(<span style=color:#e6db74>&#34;org.apache.hudi&#34;</span>)<span style=color:#f92672>.</span>option(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;hoodie.datasource.write.operation&#34;</span>, <span style=color:#e6db74>&#34;insert&#34;</span>
</span></span><span style=display:flex><span>)<span style=color:#f92672>.</span>options(<span style=color:#f92672>**</span>hudiOptions)<span style=color:#f92672>.</span>mode(<span style=color:#e6db74>&#34;overwrite&#34;</span>)<span style=color:#f92672>.</span>save(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;s3://</span><span style=color:#e6db74>{</span>S3_BUCKET_NAME<span style=color:#e6db74>}</span><span style=color:#e6db74>/tmp/hudi/&#34;</span>)
</span></span></code></pre></div><p>When we look at the file structure on S3, we see a few things:</p><ol><li>A <code>hoodie.properties</code> file</li></ol><pre tabindex=0><code>2022-01-14 00:33:46        503 tmp/hudi/.hoodie/hoodie.properties
</code></pre><p>This file contains certain metadata about the Hudi dataset:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ini data-lang=ini><span style=display:flex><span><span style=color:#75715e>#Properties saved on Fri Jan 14 00:33:45 UTC 2022</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#Fri Jan 14 00:33:45 UTC 2022</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>hoodie.table.precombine.field</span><span style=color:#f92672>=</span><span style=color:#e6db74>last_update_time</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>hoodie.table.partition.fields</span><span style=color:#f92672>=</span><span style=color:#e6db74>creation_date</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>hoodie.table.type</span><span style=color:#f92672>=</span><span style=color:#e6db74>COPY_ON_WRITE</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>hoodie.archivelog.folder</span><span style=color:#f92672>=</span><span style=color:#e6db74>archived</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>hoodie.populate.meta.fields</span><span style=color:#f92672>=</span><span style=color:#e6db74>true</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>hoodie.timeline.layout.version</span><span style=color:#f92672>=</span><span style=color:#e6db74>1</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>hoodie.table.version</span><span style=color:#f92672>=</span><span style=color:#e6db74>2</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>hoodie.table.recordkey.fields</span><span style=color:#f92672>=</span><span style=color:#e6db74>id</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>hoodie.table.base.file.format</span><span style=color:#f92672>=</span><span style=color:#e6db74>PARQUET</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>hoodie.table.keygenerator.class</span><span style=color:#f92672>=</span><span style=color:#e6db74>org.apache.hudi.keygen.SimpleKeyGenerator</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>hoodie.table.name</span><span style=color:#f92672>=</span><span style=color:#e6db74>my_hudi_table</span>
</span></span></code></pre></div><ol start=2><li>A set of commit-related files</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>2022-01-14 00:33:57       2706 tmp/hudi/.hoodie/20220114003341.commit
</span></span><span style=display:flex><span>2022-01-14 00:33:48          0 tmp/hudi/.hoodie/20220114003341.commit.requested
</span></span><span style=display:flex><span>2022-01-14 00:33:52       1842 tmp/hudi/.hoodie/20220114003341.inflight
</span></span></code></pre></div><ol start=3><li>The actual <code>.parquet</code> data files and associated metadata organized into date-based partitions.</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>2022-01-14 00:33:54         93 tmp/hudi/2015-01-01/.hoodie_partition_metadata
</span></span><span style=display:flex><span>2022-01-14 00:33:54     434974 tmp/hudi/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet
</span></span><span style=display:flex><span>2022-01-14 00:33:55         93 tmp/hudi/2015-01-02/.hoodie_partition_metadata
</span></span><span style=display:flex><span>2022-01-14 00:33:55     434943 tmp/hudi/2015-01-02/43051d12-87e7-4dfb-8201-6ce293cf0df7-0_1-6-99_20220114003341.parquet
</span></span></code></pre></div><p>We then update the <code>creation_date</code> of one row in this dataset.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql.functions <span style=color:#f92672>import</span> lit
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Create a new DataFrame from the first row of inputDF with a different creation_date value</span>
</span></span><span style=display:flex><span>updateDF <span style=color:#f92672>=</span> inputDF<span style=color:#f92672>.</span>where(<span style=color:#e6db74>&#34;id = 100&#34;</span>)<span style=color:#f92672>.</span>withColumn(<span style=color:#e6db74>&#34;creation_date&#34;</span>, lit(<span style=color:#e6db74>&#34;2022-01-11&#34;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>updateDF<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Update by using the &#34;upsert&#34; operation</span>
</span></span><span style=display:flex><span>updateDF<span style=color:#f92672>.</span>write<span style=color:#f92672>.</span>format(<span style=color:#e6db74>&#34;org.apache.hudi&#34;</span>)<span style=color:#f92672>.</span>option(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;hoodie.datasource.write.operation&#34;</span>, <span style=color:#e6db74>&#34;upsert&#34;</span>
</span></span><span style=display:flex><span>)<span style=color:#f92672>.</span>options(<span style=color:#f92672>**</span>hudiOptions)<span style=color:#f92672>.</span>mode(<span style=color:#e6db74>&#34;append&#34;</span>)<span style=color:#f92672>.</span>save(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;s3://</span><span style=color:#e6db74>{</span>S3_BUCKET_NAME<span style=color:#e6db74>}</span><span style=color:#e6db74>/tmp/hudi/&#34;</span>)
</span></span></code></pre></div><p>One thing to note here is that since we&rsquo;re updating a partition value (<strong>DANGER!</strong>), we had to set the <code>hoodie.index.type</code> to <code>GLOBAL_BLOOM</code> as well as setting <code>hoodie.bloom.index.update.partition.path</code> to <code>true</code>. This can have a large impact on performance so normally we would try not to change a partition value in a production environment, but it&rsquo;s useful here to see the impact it has. You can mind more details in the Hudi FAQ about <a href=https://hudi.apache.org/learn/faq/#how-does-the-hudi-indexing-work--what-are-its-benefits>Hudi indexing</a>.</p><p>After this write, we have a new set of commit-related files on S3:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>2022-01-14 00:34:15       2706 tmp/hudi/.hoodie/20220114003401.commit
</span></span><span style=display:flex><span>2022-01-14 00:34:03          0 tmp/hudi/.hoodie/20220114003401.commit.requested
</span></span><span style=display:flex><span>2022-01-14 00:34:08       2560 tmp/hudi/.hoodie/20220114003401.inflight
</span></span></code></pre></div><p>And we actually have <strong>2</strong> new <code>.parquet</code> files:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>2022-01-14 00:34:12     434925 tmp/hudi/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-37-13680_20220114003401.parquet
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>2022-01-14 00:34:13         93 tmp/hudi/2022-01-11/.hoodie_partition_metadata
</span></span><span style=display:flex><span>2022-01-14 00:34:14     434979 tmp/hudi/2022-01-11/0c210872-484e-428b-a9ca-90a26e42125c-0_1-43-13681_20220114003401.parquet
</span></span></code></pre></div><p>So what happened with the update is that the old partition (<code>2015-01-01</code>) had its data overwritten and the new partition (<code>2022-01-11</code>) <em>also</em> had data written to it. You can now see why the global bloom index could have such a large impact on write performance as there is significant potential for write amplication.</p><p>If we query the data and add the source filename for each row, we can also see that data for the old partition now comes from the new parquet file (notice the commit ID <code>20220114003401</code> shows up in the filename):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span>  pyspark.sql.functions <span style=color:#f92672>import</span> input_file_name
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>snapshotQueryDF <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read \
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>format(<span style=color:#e6db74>&#39;org.apache.hudi&#39;</span>) \
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>load(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;s3://</span><span style=color:#e6db74>{</span>S3_BUCKET_NAME<span style=color:#e6db74>}</span><span style=color:#e6db74>/tmp/hudi/&#34;</span>) \
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>select(<span style=color:#e6db74>&#39;id&#39;</span>, <span style=color:#e6db74>&#39;creation_date&#39;</span>) \
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>withColumn(<span style=color:#e6db74>&#34;filename&#34;</span>, input_file_name())
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>snapshotQueryDF<span style=color:#f92672>.</span>show(truncate<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>+---+-------------+------------------------------------------------------------------------------------------------------------------------------+
</span></span><span style=display:flex><span>|id |creation_date|filename                                                                                                                      |
</span></span><span style=display:flex><span>+---+-------------+------------------------------------------------------------------------------------------------------------------------------+
</span></span><span style=display:flex><span>|100|2022-01-11   |/hudi/2022-01-11/0c210872-484e-428b-a9ca-90a26e42125c-0_1-43-13681_20220114003401.parquet                                     |
</span></span><span style=display:flex><span>|105|2015-01-02   |/hudi/2015-01-02/43051d12-87e7-4dfb-8201-6ce293cf0df7-0_1-6-99_20220114003341.parquet                                         |
</span></span><span style=display:flex><span>|104|2015-01-02   |/hudi/2015-01-02/43051d12-87e7-4dfb-8201-6ce293cf0df7-0_1-6-99_20220114003341.parquet                                         |
</span></span><span style=display:flex><span>|102|2015-01-01   |/hudi/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-37-13680_20220114003401.parquet                                     |
</span></span><span style=display:flex><span>|103|2015-01-01   |/hudi/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-37-13680_20220114003401.parquet                                     |
</span></span><span style=display:flex><span>|101|2015-01-01   |/hudi/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-37-13680_20220114003401.parquet                                     |
</span></span><span style=display:flex><span>+---+-------------+------------------------------------------------------------------------------------------------------------------------------+
</span></span></code></pre></div><p>One other thing to note is that Hudi adds quite a bit of metadata to your Parquet files. This data helps enable record-level change streams - more detail can be found in this <a href=https://hudi.apache.org/blog/2021/07/21/streaming-data-lake-platform/#writers>comprehensive blog post about the Hudi platform</a>. If we use native Spark to read one of the Parquet files and show it, we see that there&rsquo;s various <code>_hoodie</code>-prefixed keys.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql.functions <span style=color:#f92672>import</span> split
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>rawDF <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>    spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>parquet(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;s3://</span><span style=color:#e6db74>{</span>S3_BUCKET_NAME<span style=color:#e6db74>}</span><span style=color:#e6db74>/tmp/hudi/*/*.parquet&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>withColumn(<span style=color:#e6db74>&#34;filename&#34;</span>, split(input_file_name(), <span style=color:#e6db74>&#34;tmp/hudi&#34;</span>)<span style=color:#f92672>.</span>getItem(<span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>sort(<span style=color:#e6db74>&#34;_hoodie_commit_time&#34;</span>, <span style=color:#e6db74>&#34;_hoodie_commit_seqno&#34;</span>)
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>rawDF<span style=color:#f92672>.</span>show(truncate<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>+-------------------+--------------------+------------------+----------------------+------------------------------------------------------------------------+---+-------------+---------------------------+------------------------------------------------------------------------------------+
</span></span><span style=display:flex><span>|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|_hoodie_file_name                                                       |id |creation_date|last_update_time           |filename                                                                            |
</span></span><span style=display:flex><span>+-------------------+--------------------+------------------+----------------------+------------------------------------------------------------------------+---+-------------+---------------------------+------------------------------------------------------------------------------------+
</span></span><span style=display:flex><span>|20220114003341     |20220114003341_0_1  |100               |2015-01-01            |57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet    |100|2015-01-01   |2015-01-01T13:51:39.340396Z|/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet    |
</span></span><span style=display:flex><span>|20220114003341     |20220114003341_0_2  |102               |2015-01-01            |57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet    |102|2015-01-01   |2015-01-01T13:51:40.417052Z|/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-37-13680_20220114003401.parquet|
</span></span><span style=display:flex><span>|20220114003341     |20220114003341_0_2  |102               |2015-01-01            |57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet    |102|2015-01-01   |2015-01-01T13:51:40.417052Z|/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet    |
</span></span><span style=display:flex><span>|20220114003341     |20220114003341_0_3  |103               |2015-01-01            |57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet    |103|2015-01-01   |2015-01-01T13:51:40.519832Z|/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-37-13680_20220114003401.parquet|
</span></span><span style=display:flex><span>|20220114003341     |20220114003341_0_3  |103               |2015-01-01            |57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet    |103|2015-01-01   |2015-01-01T13:51:40.519832Z|/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet    |
</span></span><span style=display:flex><span>|20220114003341     |20220114003341_0_4  |101               |2015-01-01            |57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet    |101|2015-01-01   |2015-01-01T12:14:58.597216Z|/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-37-13680_20220114003401.parquet|
</span></span><span style=display:flex><span>|20220114003341     |20220114003341_0_4  |101               |2015-01-01            |57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet    |101|2015-01-01   |2015-01-01T12:14:58.597216Z|/2015-01-01/57f66198-5303-4922-9323-91737ec40d25-0_0-4-98_20220114003341.parquet    |
</span></span><span style=display:flex><span>|20220114003341     |20220114003341_1_5  |105               |2015-01-02            |43051d12-87e7-4dfb-8201-6ce293cf0df7-0_1-6-99_20220114003341.parquet    |105|2015-01-02   |2015-01-01T13:51:42.248818Z|/2015-01-02/43051d12-87e7-4dfb-8201-6ce293cf0df7-0_1-6-99_20220114003341.parquet    |
</span></span><span style=display:flex><span>|20220114003341     |20220114003341_1_6  |104               |2015-01-02            |43051d12-87e7-4dfb-8201-6ce293cf0df7-0_1-6-99_20220114003341.parquet    |104|2015-01-02   |2015-01-01T12:15:00.512679Z|/2015-01-02/43051d12-87e7-4dfb-8201-6ce293cf0df7-0_1-6-99_20220114003341.parquet    |
</span></span><span style=display:flex><span>|20220114003401     |20220114003401_1_1  |100               |2022-01-11            |0c210872-484e-428b-a9ca-90a26e42125c-0_1-43-13681_20220114003401.parquet|100|2022-01-11   |2015-01-01T13:51:39.340396Z|/2022-01-11/0c210872-484e-428b-a9ca-90a26e42125c-0_1-43-13681_20220114003401.parquet|
</span></span><span style=display:flex><span>+-------------------+--------------------+------------------+----------------------+------------------------------------------------------------------------+---+-------------+---------------------------+------------------------------------------------------------------------------------+
</span></span></code></pre></div><p>In the background, Hudi figures out which commits and values to show based on the commit files and metadata in the parquet files.</p><h2 id=apache-iceberg>Apache Iceberg<a hidden class=anchor aria-hidden=true href=#apache-iceberg>#</a></h2><blockquote><p>üìπ <a href="https://www.youtube.com/watch?v=fryfx0Zg7KA&amp;t=1039s">Intro to Apache Iceberg video</a></p></blockquote><p>When I first heard about Iceberg, the phrase &ldquo;table format for storing large, slow-moving tabular data&rdquo; didn&rsquo;t really make sense to me. But after working with data lakes at scale, it became quite clear. Apache Hive is a popular data warehouse project that provides a SQL-like interface to large datasets. Built on top of Hadoop, it originally used HDFS as its data store. With cloud migrations, object stores like Amazon S3 enabled the ability to store even more data particularly without the operational concerns of a large Hadoop cluster, but with some limitations when compared to HDFS. Specifically, directory listings are slower (simple physics here, network calls are slower), renames are not atomic (by design), and results were previously eventually consistent.</p><p>So imagine you are Netflix, you have <a href=https://netflixtechblog.com/optimizing-data-warehouse-storage-7b94a48fdcbe>hundreds of petabytes of data</a> stored on S3, and you need a way for your organization to efficiently query this. You need a data storage layer that reduces or removes directory listings, you want atomic changes, and you want to ensure that when you&rsquo;re reading your data you get consistent results. <em>There is more to Iceberg, but I&rsquo;m simplifying because this helped me understand. :)</em></p><p>These were some of the original goals for Iceberg, so let&rsquo;s dive in and see how it works. Similar to Hudi, we&rsquo;ll create a simple Spark DataFrame and write that to S3 in Iceberg format.</p><p>I should note that much of Iceberg is focused around Spark SQL, so I will switch to that below for certain operations.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Create a DataFrame</span>
</span></span><span style=display:flex><span>inputDF <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>createDataFrame(
</span></span><span style=display:flex><span>    [
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;100&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01T13:51:39.340396Z&#34;</span>),
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;101&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01T12:14:58.597216Z&#34;</span>),
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;102&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01T13:51:40.417052Z&#34;</span>),
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;103&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01T13:51:40.519832Z&#34;</span>),
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;104&#34;</span>, <span style=color:#e6db74>&#34;2015-01-02&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01T12:15:00.512679Z&#34;</span>),
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;105&#34;</span>, <span style=color:#e6db74>&#34;2015-01-02&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01T13:51:42.248818Z&#34;</span>),
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    [<span style=color:#e6db74>&#34;id&#34;</span>, <span style=color:#e6db74>&#34;creation_date&#34;</span>, <span style=color:#e6db74>&#34;last_update_time&#34;</span>],
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Write a DataFrame as an Iceberg dataset</span>
</span></span><span style=display:flex><span>inputDF<span style=color:#f92672>.</span>write<span style=color:#f92672>.</span>format(<span style=color:#e6db74>&#34;iceberg&#34;</span>)<span style=color:#f92672>.</span>mode(<span style=color:#e6db74>&#34;overwrite&#34;</span>)<span style=color:#f92672>.</span>partitionBy(<span style=color:#e6db74>&#34;creation_date&#34;</span>)<span style=color:#f92672>.</span>option(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;path&#34;</span>, <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;s3://</span><span style=color:#e6db74>{</span>S3_BUCKET_NAME<span style=color:#e6db74>}</span><span style=color:#e6db74>/tmp/iceberg/&#34;</span>
</span></span><span style=display:flex><span>)<span style=color:#f92672>.</span>saveAsTable(ICEBERG_TABLE_NAME)
</span></span></code></pre></div><p>There are two main differences here - there is not as much &ldquo;configuration&rdquo; as we had to do with Hudi and we also explicitly use <code>saveAsTable</code>. With Iceberg, much of the metadata is stored in a data catalog so creating the table is necessary. Let&rsquo;s see what happened on S3.</p><ol><li>First, we have a <code>metadata.json</code> file</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>2022-01-28 06:03:50       2457 tmp/iceberg/metadata/00000-bb1d38a9-af77-42c4-a7b7-69416fe36d9c.metadata.json
</span></span></code></pre></div><ol start=2><li>Then a snapshot <strong>manifest list</strong> file</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>2022-01-28 06:03:50       3785 tmp/iceberg/metadata/snap-7934053180928033536-1-e79c79ba-c7f0-45ad-8f2e-fd1bc349db55.avro
</span></span></code></pre></div><ol start=3><li>And a manifest file</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>2022-01-28 06:03:50       6244 tmp/iceberg/metadata/e79c79ba-c7f0-45ad-8f2e-fd1bc349db55-m0.avro
</span></span></code></pre></div><ol start=4><li>And finally, we&rsquo;ve got our Parquet data files</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>2022-01-28 06:03:49       1197 tmp/iceberg/data/creation_date=2015-01-01/00000-4-fa9a18fd-abc4-4e04-91b4-e2ac4c9531be-00001.parquet
</span></span><span style=display:flex><span>2022-01-28 06:03:49       1171 tmp/iceberg/data/creation_date=2015-01-01/00001-5-eab30115-a1d6-4918-abb4-a198ac12b262-00001.parquet
</span></span><span style=display:flex><span>2022-01-28 06:03:50       1182 tmp/iceberg/data/creation_date=2015-01-02/00001-5-eab30115-a1d6-4918-abb4-a198ac12b262-00002.parquet
</span></span></code></pre></div><p>There are a lot of moving pieces here, but the image from the Iceberg spec illustrates it quite well.</p><p><img alt="Iceberg Metadata Diagram" loading=lazy src=/posts/modern-data-lake-storage-layers/iceberg-metadata.png></p><p>Similar to Hudi, our data is written to Parquet files in each partition, although Hive-style partitioning is used by default. Hudi can also do this by setting the <a href=https://hudi.apache.org/docs/configurations/#hoodiedatasourcewritehive_style_partitioning><code>hoodie.datasource.write.hive_style_partitioning </code></a>parameter.</p><p>Different from Hudi, though, is the default usage of the data catalog to identify the current metadata file to use. That metadata file contains references to a list of manifest files to use to determine which data files compose the dataset for that particular version, also known as snapshots. As of <a href=https://hudi.apache.org/releases/release-0.7.0/#metadata-table>Hudi 0.7.0</a>, it also supports a metadata table to reduce the performance impact of file listings. The snapshot data also has quite a bit of additional information. Let&rsquo;s update our dataset then take a look at S3 again and the snapshot portion of the metadata file.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>spark<span style=color:#f92672>.</span>sql(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;UPDATE </span><span style=color:#e6db74>{</span>ICEBERG_TABLE_NAME<span style=color:#e6db74>}</span><span style=color:#e6db74> SET creation_date = &#39;2022-01-11&#39; WHERE id = 100&#34;</span>)
</span></span></code></pre></div><p>We can see that we have:</p><ul><li>2 new .parquet data files</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>2022-01-28 06:07:07       1180 tmp/iceberg/data/creation_date=2015-01-01/00000-16-033354bd-7b02-44f4-95e2-7045e10706fc-00001.parquet
</span></span><span style=display:flex><span>2022-01-28 06:07:08       1171 tmp/iceberg/data/creation_date=2022-01-11/00000-16-033354bd-7b02-44f4-95e2-7045e10706fc-00002.parquet
</span></span></code></pre></div><p>As well as:</p><ul><li>1 new metadata.json file</li><li>2 new .avro metadata listings</li><li>1 new snap-*.avro snapshot file</li></ul><p>Let&rsquo;s look at the snapshot portion of the <code>metadata.json</code> file.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#e6db74>&#34;snapshots&#34;</span><span style=color:#960050;background-color:#1e0010>:</span> [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;manifest-list&#34;</span>: <span style=color:#e6db74>&#34;s3://&lt;BUCKET&gt;/tmp/iceberg/metadata/snap-7934053180928033536-1-e79c79ba-c7f0-45ad-8f2e-fd1bc349db55.avro&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;schema-id&#34;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;snapshot-id&#34;</span>: <span style=color:#ae81ff>7934053180928033536</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;summary&#34;</span>: {
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;added-data-files&#34;</span>: <span style=color:#e6db74>&#34;3&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;added-files-size&#34;</span>: <span style=color:#e6db74>&#34;3550&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;added-records&#34;</span>: <span style=color:#e6db74>&#34;6&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;changed-partition-count&#34;</span>: <span style=color:#e6db74>&#34;2&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;operation&#34;</span>: <span style=color:#e6db74>&#34;append&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;spark.app.id&#34;</span>: <span style=color:#e6db74>&#34;application_1643153254969_0029&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;total-data-files&#34;</span>: <span style=color:#e6db74>&#34;3&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;total-delete-files&#34;</span>: <span style=color:#e6db74>&#34;0&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;total-equality-deletes&#34;</span>: <span style=color:#e6db74>&#34;0&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;total-files-size&#34;</span>: <span style=color:#e6db74>&#34;3550&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;total-position-deletes&#34;</span>: <span style=color:#e6db74>&#34;0&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;total-records&#34;</span>: <span style=color:#e6db74>&#34;6&#34;</span>
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;timestamp-ms&#34;</span>: <span style=color:#ae81ff>1643349829278</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;manifest-list&#34;</span>: <span style=color:#e6db74>&#34;s3://&lt;BUCKET&gt;/tmp/iceberg/metadata/snap-5441092870212826638-1-605de48f-8ccf-450c-935e-bbd4194ee8cc.avro&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;parent-snapshot-id&#34;</span>: <span style=color:#ae81ff>7934053180928033536</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;schema-id&#34;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;snapshot-id&#34;</span>: <span style=color:#ae81ff>5441092870212826638</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;summary&#34;</span>: {
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;added-data-files&#34;</span>: <span style=color:#e6db74>&#34;2&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;added-files-size&#34;</span>: <span style=color:#e6db74>&#34;2351&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;added-records&#34;</span>: <span style=color:#e6db74>&#34;3&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;changed-partition-count&#34;</span>: <span style=color:#e6db74>&#34;2&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;deleted-data-files&#34;</span>: <span style=color:#e6db74>&#34;1&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;deleted-records&#34;</span>: <span style=color:#e6db74>&#34;3&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;operation&#34;</span>: <span style=color:#e6db74>&#34;overwrite&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;removed-files-size&#34;</span>: <span style=color:#e6db74>&#34;1197&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;spark.app.id&#34;</span>: <span style=color:#e6db74>&#34;application_1643153254969_0029&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;total-data-files&#34;</span>: <span style=color:#e6db74>&#34;4&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;total-delete-files&#34;</span>: <span style=color:#e6db74>&#34;0&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;total-equality-deletes&#34;</span>: <span style=color:#e6db74>&#34;0&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;total-files-size&#34;</span>: <span style=color:#e6db74>&#34;4704&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;total-position-deletes&#34;</span>: <span style=color:#e6db74>&#34;0&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;total-records&#34;</span>: <span style=color:#e6db74>&#34;6&#34;</span>
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;timestamp-ms&#34;</span>: <span style=color:#ae81ff>1643350027635</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>]
</span></span></code></pre></div><p>This is pretty amazing - we see how many files <strong>and records</strong> were added or deleted, what the file sizes were, and even what the Spark <code>app_id</code> was! ü§Ø Some of this data is in the <code>manifest-list</code> files as well, but you can begin to see <em>just</em> how much you could potentially optimize your queries using this data.</p><h2 id=delta-lake>Delta Lake<a hidden class=anchor aria-hidden=true href=#delta-lake>#</a></h2><blockquote><p>üìπ <a href="https://www.youtube.com/watch?v=fryfx0Zg7KA&amp;t=1814s">Intro to Delta Lake video</a></p></blockquote><p>Delta Lake was also introduced by Databricks as a way to address many of the challenges of Data Lakes. Similar to Hudi and Iceberg its goals include unifying batch and stream processing, ACID transactions, and scalable metadata handling among others.</p><p>Again, we&rsquo;ll create a simple Spark DataFrame and write it to S3 in Delta format.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Create a DataFrame</span>
</span></span><span style=display:flex><span>inputDF <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>createDataFrame(
</span></span><span style=display:flex><span>    [
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;100&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01T13:51:39.340396Z&#34;</span>),
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;101&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01T12:14:58.597216Z&#34;</span>),
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;102&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01T13:51:40.417052Z&#34;</span>),
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;103&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01T13:51:40.519832Z&#34;</span>),
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;104&#34;</span>, <span style=color:#e6db74>&#34;2015-01-02&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01T12:15:00.512679Z&#34;</span>),
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;105&#34;</span>, <span style=color:#e6db74>&#34;2015-01-02&#34;</span>, <span style=color:#e6db74>&#34;2015-01-01T13:51:42.248818Z&#34;</span>),
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    [<span style=color:#e6db74>&#34;id&#34;</span>, <span style=color:#e6db74>&#34;creation_date&#34;</span>, <span style=color:#e6db74>&#34;last_update_time&#34;</span>],
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Write a DataFrame as a Delta dataset</span>
</span></span><span style=display:flex><span>inputDF<span style=color:#f92672>.</span>write<span style=color:#f92672>.</span>format(<span style=color:#e6db74>&#34;delta&#34;</span>)<span style=color:#f92672>.</span>mode(<span style=color:#e6db74>&#34;overwrite&#34;</span>)<span style=color:#f92672>.</span>option(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;overwriteSchema&#34;</span>, <span style=color:#e6db74>&#34;true&#34;</span>
</span></span><span style=display:flex><span>)<span style=color:#f92672>.</span>partitionBy(<span style=color:#e6db74>&#34;creation_date&#34;</span>)<span style=color:#f92672>.</span>save(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;s3://</span><span style=color:#e6db74>{</span>S3_BUCKET_NAME<span style=color:#e6db74>}</span><span style=color:#e6db74>/tmp/delta/&#34;</span>)
</span></span></code></pre></div><p>On S3, we now see the following files:</p><ol><li>a <code>00000000000000000000.json</code> file</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>2022-01-24 22:57:54       2120 tmp/delta/_delta_log/00000000000000000000.json
</span></span></code></pre></div><ol start=2><li>Several <code>.snappy.parquet</code> files</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>2022-01-24 22:57:52        875 tmp/delta/creation_date=2015-01-01/part-00005-2e09dbe4-469e-40dc-9b36-833480f6d375.c000.snappy.parquet
</span></span><span style=display:flex><span>2022-01-24 22:57:52        875 tmp/delta/creation_date=2015-01-01/part-00010-848c69e1-71fb-4f8f-a19a-dd74e0ef1b8a.c000.snappy.parquet
</span></span><span style=display:flex><span>2022-01-24 22:57:53        875 tmp/delta/creation_date=2015-01-01/part-00015-937d1837-0f03-4306-9b4e-4366207e688d.c000.snappy.parquet
</span></span><span style=display:flex><span>2022-01-24 22:57:54        875 tmp/delta/creation_date=2015-01-01/part-00021-978a808e-4c36-4646-b7b1-ef5a21e706d8.c000.snappy.parquet
</span></span><span style=display:flex><span>2022-01-24 22:57:54        875 tmp/delta/creation_date=2015-01-02/part-00026-538e1ac6-055e-4e72-9177-63daaaae1f98.c000.snappy.parquet
</span></span><span style=display:flex><span>2022-01-24 22:57:52        875 tmp/delta/creation_date=2015-01-02/part-00031-8a03451a-0297-4c43-b64d-56db25807d02.c000.snappy.parquet
</span></span></code></pre></div><p>OK, so what&rsquo;s in that <code>_delta_log</code> file? Similar to Iceberg, quite a bit of information about this initial write to S3 including the number of files written, the schema of the dataset, and even the individual <code>add</code> operations for each file.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;commitInfo&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;timestamp&#34;</span>: <span style=color:#ae81ff>1643065073634</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;operation&#34;</span>: <span style=color:#e6db74>&#34;WRITE&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;operationParameters&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;mode&#34;</span>: <span style=color:#e6db74>&#34;Overwrite&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;partitionBy&#34;</span>: <span style=color:#e6db74>&#34;[\&#34;creation_date\&#34;]&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;isBlindAppend&#34;</span>: <span style=color:#66d9ef>false</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;operationMetrics&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;numFiles&#34;</span>: <span style=color:#e6db74>&#34;6&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;numOutputBytes&#34;</span>: <span style=color:#e6db74>&#34;5250&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;numOutputRows&#34;</span>: <span style=color:#e6db74>&#34;6&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;protocol&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;minReaderVersion&#34;</span>: <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;minWriterVersion&#34;</span>: <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;metaData&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;id&#34;</span>: <span style=color:#e6db74>&#34;a7f4b1d1-09f6-4475-894a-0eec90d1aab5&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;format&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;provider&#34;</span>: <span style=color:#e6db74>&#34;parquet&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;options&#34;</span>: {}
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;schemaString&#34;</span>: <span style=color:#e6db74>&#34;{\&#34;type\&#34;:\&#34;struct\&#34;,\&#34;fields\&#34;:[{\&#34;name\&#34;:\&#34;id\&#34;,\&#34;type\&#34;:\&#34;string\&#34;,\&#34;nullable\&#34;:true,\&#34;metadata\&#34;:{}},{\&#34;name\&#34;:\&#34;creation_date\&#34;,\&#34;type\&#34;:\&#34;string\&#34;,\&#34;nullable\&#34;:true,\&#34;metadata\&#34;:{}},{\&#34;name\&#34;:\&#34;last_update_time\&#34;,\&#34;type\&#34;:\&#34;string\&#34;,\&#34;nullable\&#34;:true,\&#34;metadata\&#34;:{}}]}&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;partitionColumns&#34;</span>: [
</span></span><span style=display:flex><span>      <span style=color:#e6db74>&#34;creation_date&#34;</span>
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;configuration&#34;</span>: {},
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;createdTime&#34;</span>: <span style=color:#ae81ff>1643065064066</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;add&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;path&#34;</span>: <span style=color:#e6db74>&#34;creation_date=2015-01-01/part-00005-2e09dbe4-469e-40dc-9b36-833480f6d375.c000.snappy.parquet&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;partitionValues&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;creation_date&#34;</span>: <span style=color:#e6db74>&#34;2015-01-01&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;size&#34;</span>: <span style=color:#ae81ff>875</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;modificationTime&#34;</span>: <span style=color:#ae81ff>1643065072000</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;dataChange&#34;</span>: <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;add&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;path&#34;</span>: <span style=color:#e6db74>&#34;creation_date=2015-01-01/part-00010-848c69e1-71fb-4f8f-a19a-dd74e0ef1b8a.c000.snappy.parquet&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;partitionValues&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;creation_date&#34;</span>: <span style=color:#e6db74>&#34;2015-01-01&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;size&#34;</span>: <span style=color:#ae81ff>875</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;modificationTime&#34;</span>: <span style=color:#ae81ff>1643065072000</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;dataChange&#34;</span>: <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;add&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;path&#34;</span>: <span style=color:#e6db74>&#34;creation_date=2015-01-01/part-00015-937d1837-0f03-4306-9b4e-4366207e688d.c000.snappy.parquet&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;partitionValues&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;creation_date&#34;</span>: <span style=color:#e6db74>&#34;2015-01-01&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;size&#34;</span>: <span style=color:#ae81ff>875</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;modificationTime&#34;</span>: <span style=color:#ae81ff>1643065073000</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;dataChange&#34;</span>: <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;add&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;path&#34;</span>: <span style=color:#e6db74>&#34;creation_date=2015-01-01/part-00021-978a808e-4c36-4646-b7b1-ef5a21e706d8.c000.snappy.parquet&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;partitionValues&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;creation_date&#34;</span>: <span style=color:#e6db74>&#34;2015-01-01&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;size&#34;</span>: <span style=color:#ae81ff>875</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;modificationTime&#34;</span>: <span style=color:#ae81ff>1643065074000</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;dataChange&#34;</span>: <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;add&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;path&#34;</span>: <span style=color:#e6db74>&#34;creation_date=2015-01-02/part-00026-538e1ac6-055e-4e72-9177-63daaaae1f98.c000.snappy.parquet&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;partitionValues&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;creation_date&#34;</span>: <span style=color:#e6db74>&#34;2015-01-02&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;size&#34;</span>: <span style=color:#ae81ff>875</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;modificationTime&#34;</span>: <span style=color:#ae81ff>1643065074000</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;dataChange&#34;</span>: <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;add&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;path&#34;</span>: <span style=color:#e6db74>&#34;creation_date=2015-01-02/part-00031-8a03451a-0297-4c43-b64d-56db25807d02.c000.snappy.parquet&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;partitionValues&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;creation_date&#34;</span>: <span style=color:#e6db74>&#34;2015-01-02&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;size&#34;</span>: <span style=color:#ae81ff>875</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;modificationTime&#34;</span>: <span style=color:#ae81ff>1643065072000</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;dataChange&#34;</span>: <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Alright, let&rsquo;s go ahead and update one of our rows. Delta Lake provides a merge operation that we can use. We&rsquo;ll use the syntax <a href=https://docs.delta.io/latest/quick-start.html#update-table-data>from the docs</a> that&rsquo;s slightly different from native Spark as it creates a DeltaTable object.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql.functions <span style=color:#f92672>import</span> lit
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Create a new DataFrame from the first row of inputDF with a different creation_date value</span>
</span></span><span style=display:flex><span>updateDF <span style=color:#f92672>=</span> inputDF<span style=color:#f92672>.</span>where(<span style=color:#e6db74>&#34;id = 100&#34;</span>)<span style=color:#f92672>.</span>withColumn(<span style=color:#e6db74>&#34;creation_date&#34;</span>, lit(<span style=color:#e6db74>&#34;2022-01-11&#34;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> delta.tables <span style=color:#f92672>import</span> <span style=color:#f92672>*</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql.functions <span style=color:#f92672>import</span> <span style=color:#f92672>*</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>deltaTable <span style=color:#f92672>=</span> DeltaTable<span style=color:#f92672>.</span>forPath(spark, <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;s3://</span><span style=color:#e6db74>{</span>S3_BUCKET_NAME<span style=color:#e6db74>}</span><span style=color:#e6db74>/tmp/delta/&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>deltaTable<span style=color:#f92672>.</span>alias(<span style=color:#e6db74>&#34;oldData&#34;</span>) \
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>merge(
</span></span><span style=display:flex><span>    updateDF<span style=color:#f92672>.</span>alias(<span style=color:#e6db74>&#34;newData&#34;</span>),
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;oldData.id = newData.id&#34;</span>) \
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>whenMatchedUpdate(set <span style=color:#f92672>=</span> { <span style=color:#e6db74>&#34;creation_date&#34;</span>: col(<span style=color:#e6db74>&#34;newData.creation_date&#34;</span>) }) \
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>execute()
</span></span></code></pre></div><p>Interestingly, now when we look at S3 we see 1 new <code>json</code> file and only 1 new <code>parquet</code> file (Remember Hudi and Iceberg both had 2 new <code>parquet</code> files).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>2022-01-24 23:05:46       1018 tmp/delta/_delta_log/00000000000000000001.json
</span></span><span style=display:flex><span>2022-01-24 23:05:46        875 tmp/delta/creation_date=2022-01-11/part-00000-3f3fd83a-b876-4b6f-8f64-d8a4189392ae.c000.snappy.parquet
</span></span></code></pre></div><p>If we look at that new JSON file we see something really interesting:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;commitInfo&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;timestamp&#34;</span>: <span style=color:#ae81ff>1643065545396</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;operation&#34;</span>: <span style=color:#e6db74>&#34;MERGE&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;operationParameters&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;predicate&#34;</span>: <span style=color:#e6db74>&#34;(oldData.`id` = newData.`id`)&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;matchedPredicates&#34;</span>: <span style=color:#e6db74>&#34;[{\&#34;actionType\&#34;:\&#34;update\&#34;}]&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;notMatchedPredicates&#34;</span>: <span style=color:#e6db74>&#34;[]&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;readVersion&#34;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;isBlindAppend&#34;</span>: <span style=color:#66d9ef>false</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;operationMetrics&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;numTargetRowsCopied&#34;</span>: <span style=color:#e6db74>&#34;0&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;numTargetRowsDeleted&#34;</span>: <span style=color:#e6db74>&#34;0&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;numTargetFilesAdded&#34;</span>: <span style=color:#e6db74>&#34;1&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;executionTimeMs&#34;</span>: <span style=color:#e6db74>&#34;4705&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;numTargetRowsInserted&#34;</span>: <span style=color:#e6db74>&#34;0&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;scanTimeMs&#34;</span>: <span style=color:#e6db74>&#34;3399&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;numTargetRowsUpdated&#34;</span>: <span style=color:#e6db74>&#34;1&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;numOutputRows&#34;</span>: <span style=color:#e6db74>&#34;1&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;numSourceRows&#34;</span>: <span style=color:#e6db74>&#34;1&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;numTargetFilesRemoved&#34;</span>: <span style=color:#e6db74>&#34;1&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;rewriteTimeMs&#34;</span>: <span style=color:#e6db74>&#34;1265&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;remove&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;path&#34;</span>: <span style=color:#e6db74>&#34;creation_date=2015-01-01/part-00005-2e09dbe4-469e-40dc-9b36-833480f6d375.c000.snappy.parquet&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;deletionTimestamp&#34;</span>: <span style=color:#ae81ff>1643065545378</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;dataChange&#34;</span>: <span style=color:#66d9ef>true</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;extendedFileMetadata&#34;</span>: <span style=color:#66d9ef>true</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;partitionValues&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;creation_date&#34;</span>: <span style=color:#e6db74>&#34;2015-01-01&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;size&#34;</span>: <span style=color:#ae81ff>875</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;add&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;path&#34;</span>: <span style=color:#e6db74>&#34;creation_date=2022-01-11/part-00000-3f3fd83a-b876-4b6f-8f64-d8a4189392ae.c000.snappy.parquet&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;partitionValues&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;creation_date&#34;</span>: <span style=color:#e6db74>&#34;2022-01-11&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;size&#34;</span>: <span style=color:#ae81ff>875</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;modificationTime&#34;</span>: <span style=color:#ae81ff>1643065546000</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;dataChange&#34;</span>: <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>In addition to the <code>operationMetrics</code> that gives us insight into how the data changed on &ldquo;disk&rdquo;, we also now see both a <code>remove</code> and <code>add</code> operation. In Delta Lake (and I&rsquo;m not quite sure why this happened yet&mldr;), each row was written to an individual <code>.parquet</code> file! So for this second version of the data, the fact that that row was updated simply lives in the metadata because it was the only row stored in that Parquet file. I&rsquo;m guessing this is simply because my dataset is so small the default number of partitions in Spark/Delta Lake resulted in this write configuration.</p><h2 id=snapshots>Snapshots<a hidden class=anchor aria-hidden=true href=#snapshots>#</a></h2><p>So now we&rsquo;ve got a good idea of the semantics of each of these storage layers. Let&rsquo;s take one more look at an important component of all of them and that&rsquo;s snapshots!</p><h3 id=hudi>Hudi<a hidden class=anchor aria-hidden=true href=#hudi>#</a></h3><p>Hudi has a concept of &ldquo;point-in-time&rdquo; queries where you provide it a range of two commit timestamps and it will show you what the data looked like at that point in time.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Query data from the first version of the table</span>
</span></span><span style=display:flex><span>readOptions <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;hoodie.datasource.query.type&#39;</span>: <span style=color:#e6db74>&#39;incremental&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;hoodie.datasource.read.begin.instanttime&#39;</span>: <span style=color:#e6db74>&#39;0&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;hoodie.datasource.read.end.instanttime&#39;</span>: <span style=color:#e6db74>&#39;20220114003341&#39;</span>,
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>incQueryDF <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read \
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>format(<span style=color:#e6db74>&#39;org.apache.hudi&#39;</span>) \
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>options(<span style=color:#f92672>**</span>readOptions) \
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>load(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;s3://</span><span style=color:#e6db74>{</span>S3_BUCKET_NAME<span style=color:#e6db74>}</span><span style=color:#e6db74>/tmp/hudi&#34;</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>incQueryDF<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><h3 id=iceberg>Iceberg<a hidden class=anchor aria-hidden=true href=#iceberg>#</a></h3><p>Iceberg supports a similar mechanism called time travel and you can use either a <code>snapshot-id</code> or <code>as-of-timestamp</code> similar to Hudi.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># time travel to 2022-01-27 22:04:00 -0800</span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read \
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;as-of-timestamp&#34;</span>, <span style=color:#e6db74>&#34;1643349840000&#34;</span>) \
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>format(<span style=color:#e6db74>&#34;iceberg&#34;</span>) \
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>load(ICEBERG_TABLE_NAME)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><h3 id=delta-lake-1>Delta Lake<a hidden class=anchor aria-hidden=true href=#delta-lake-1>#</a></h3><p>And, of course, Delta Lake supports this as well using either <a href=https://docs.delta.io/latest/delta-batch.html#-deltatimetravel>Spark SQL or DataFrames</a>. And similar to Iceberg you can use <code>versionAsOf</code> or <code>timestampAsOf</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># time travel to 2022-01-24 23:00</span>
</span></span><span style=display:flex><span>df1 <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>    spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>format(<span style=color:#e6db74>&#34;delta&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;timestampAsOf&#34;</span>, <span style=color:#e6db74>&#34;2022-01-24 23:00&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>load(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;s3://</span><span style=color:#e6db74>{</span>S3_BUCKET_NAME<span style=color:#e6db74>}</span><span style=color:#e6db74>/tmp/delta/&#34;</span>)
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><h2 id=deletes>Deletes<a hidden class=anchor aria-hidden=true href=#deletes>#</a></h2><p>I bet you&rsquo;re surprised I haven&rsquo;t mentioned deletes or GDPR yet. Don&rsquo;t worry&mldr;I will. üòÄ But first I just wanted to understand exactly how these different systems work.</p><h2 id=wrapup>Wrapup<a hidden class=anchor aria-hidden=true href=#wrapup>#</a></h2><p>In this post, we reviewed the basics of Apache Hudi, Apache Iceberg, and Delta Lake - modern data lake storage layers. All these frameworks enable a set of functionality that optimize working with data in cloud-based object stores, albeit with slightly different approaches.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://dacort.xyz/tags/aws/>Aws</a></li><li><a href=https://dacort.xyz/tags/apache/>Apache</a></li><li><a href=https://dacort.xyz/tags/hudi/>Hudi</a></li><li><a href=https://dacort.xyz/tags/iceberg/>Iceberg</a></li><li><a href=https://dacort.xyz/tags/deltalake/>Deltalake</a></li></ul><nav class=paginav><a class=prev href=https://dacort.xyz/posts/serverless-analytics-of-twitter-data/><span class=title>¬´ Prev</span><br><span>"Serverless" Analytics of Twitter Data with MSK Connect and Athena</span>
</a><a class=next href=https://dacort.xyz/posts/ssh-to-ec2-instances-with-session-manager/><span class=title>Next ¬ª</span><br><span>SSH to EC2 Instances with Session Manager</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share An Introduction to Modern Data Lake Storage Layers on x" href="https://x.com/intent/tweet/?text=An%20Introduction%20to%20Modern%20Data%20Lake%20Storage%20Layers&amp;url=https%3a%2f%2fdacort.xyz%2fposts%2fmodern-data-lake-storage-layers%2f&amp;hashtags=aws%2capache%2chudi%2ciceberg%2cdeltalake"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share An Introduction to Modern Data Lake Storage Layers on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdacort.xyz%2fposts%2fmodern-data-lake-storage-layers%2f&amp;title=An%20Introduction%20to%20Modern%20Data%20Lake%20Storage%20Layers&amp;summary=An%20Introduction%20to%20Modern%20Data%20Lake%20Storage%20Layers&amp;source=https%3a%2f%2fdacort.xyz%2fposts%2fmodern-data-lake-storage-layers%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share An Introduction to Modern Data Lake Storage Layers on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fdacort.xyz%2fposts%2fmodern-data-lake-storage-layers%2f&title=An%20Introduction%20to%20Modern%20Data%20Lake%20Storage%20Layers"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share An Introduction to Modern Data Lake Storage Layers on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdacort.xyz%2fposts%2fmodern-data-lake-storage-layers%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share An Introduction to Modern Data Lake Storage Layers on whatsapp" href="https://api.whatsapp.com/send?text=An%20Introduction%20to%20Modern%20Data%20Lake%20Storage%20Layers%20-%20https%3a%2f%2fdacort.xyz%2fposts%2fmodern-data-lake-storage-layers%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share An Introduction to Modern Data Lake Storage Layers on telegram" href="https://telegram.me/share/url?text=An%20Introduction%20to%20Modern%20Data%20Lake%20Storage%20Layers&amp;url=https%3a%2f%2fdacort.xyz%2fposts%2fmodern-data-lake-storage-layers%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share An Introduction to Modern Data Lake Storage Layers on ycombinator" href="https://news.ycombinator.com/submitlink?t=An%20Introduction%20to%20Modern%20Data%20Lake%20Storage%20Layers&u=https%3a%2f%2fdacort.xyz%2fposts%2fmodern-data-lake-storage-layers%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://dacort.xyz/>Damon Cortesi</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>